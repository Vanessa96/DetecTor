{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average error is 3.11ms\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "def check_sleep(amount):\n",
    "    start = datetime.now()\n",
    "    time.sleep(amount)\n",
    "    end = datetime.now()\n",
    "    delta = end-start\n",
    "    return delta.seconds + delta.microseconds/1000000.\n",
    "\n",
    "error = sum(abs(check_sleep(0.1)-0.1) for i in range(100))*10\n",
    "print(\"Average error is %0.2fms\" % error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-11-15 22:46:42.136237\n"
     ]
    }
   ],
   "source": [
    "print(datetime.fromtimestamp(1605498402.136237))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import autoreload\n",
    "# ?autoreload\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import torch\n",
    "from transformers import AutoModel\n",
    "from transformers import AutoConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aten_mul(node):\n",
    "    os = node.outputs[0].shape\n",
    "    return math.prod(os)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "globals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8e97873a8244bba8b3742ae48f5f778",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=684.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# config = AutoConfig.from_pretrained(\"prajjwal1/bert-tiny\")\n",
    "config = AutoConfig.from_pretrained(\"albert-base-v2\")\n",
    "config.hidden_act = 'gelu_fast'\n",
    "config.torchscript = True\n",
    "model = AutoModel.from_config(config)\n",
    "inputs = torch.randint(1000, size=(1, 100)).long()\n",
    "# model()\n",
    "# with torch.onnx.select_model_mode_for_export(model, False):\n",
    "  # trace, _ = torch.jit._get_trace_graph(model, args=(inputs,))\n",
    "#     trace = torch.jit.trace(model, (inputs, ))\n",
    "mo=model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;33mproperty:\u001b[0m\n",
       "    \u001b[0;36mT_destination\u001b[0m\u001b[1;30m, \u001b[0m\u001b[0;36m_backward_hooks\u001b[0m\u001b[1;30m, \u001b[0m\u001b[0;36m_buffers\u001b[0m\u001b[1;30m, \u001b[0m\u001b[0;36m_forward_hooks\u001b[0m\u001b[1;30m, \u001b[0m\u001b[0;36m_forward_pre_hooks\u001b[0m\u001b[1;30m, \u001b[0m\u001b[0;36m_load_state_dict_pre_hooks\u001b[0m\u001b[1;30m, \u001b[0m\u001b[0;36m_modules\u001b[0m\u001b[1;30m, \u001b[0m\u001b[0;36m_non_persistent_buffers_set\u001b[0m\u001b[1;30m, \u001b[0m\u001b[0;36m_parameters\u001b[0m\u001b[1;30m, \u001b[0m\u001b[0;36m_state_dict_hooks\u001b[0m\u001b[1;30m, \u001b[0m\u001b[0;36m_version\u001b[0m\u001b[1;30m, \u001b[0m\u001b[0;36mauthorized_missing_keys\u001b[0m\u001b[1;30m, \u001b[0m\u001b[0;36mauthorized_unexpected_keys\u001b[0m\u001b[1;30m, \u001b[0m\u001b[0;36mbase_model_prefix\u001b[0m\u001b[1;30m, \u001b[0m\u001b[0;36mconfig\u001b[0m\u001b[1;30m, \u001b[0m\u001b[0;36mdump_patches\u001b[0m\u001b[1;30m, \u001b[0m\u001b[0;36membeddings\u001b[0m\u001b[1;30m, \u001b[0m\u001b[0;36mencoder\u001b[0m\u001b[1;30m, \u001b[0m\u001b[0;36mkeys_to_never_save\u001b[0m\u001b[1;30m, \u001b[0m\u001b[0;36mname_or_path\u001b[0m\u001b[1;30m, \u001b[0m\u001b[0;36mpooler\u001b[0m\u001b[1;30m, \u001b[0m\u001b[0;36mpooler_activation\u001b[0m\u001b[1;30m, \u001b[0m\u001b[0;36mtraining\u001b[0m\n",
       "\u001b[0;33mspecial attribute:\u001b[0m\n",
       "    \u001b[0;36m__annotations__\u001b[0m\u001b[1;30m, \u001b[0m\u001b[0;36m__class__\u001b[0m\u001b[1;30m, \u001b[0m\u001b[0;36m__dict__\u001b[0m\u001b[1;30m, \u001b[0m\u001b[0;36m__doc__\u001b[0m\u001b[1;30m, \u001b[0m\u001b[0;36m__module__\u001b[0m\u001b[1;30m, \u001b[0m\u001b[0;36m__weakref__\u001b[0m\n",
       "\u001b[0;33mabstract class:\u001b[0m\n",
       "    \u001b[0;36m__subclasshook__\u001b[0m\n",
       "\u001b[0;33mobject customization:\u001b[0m\n",
       "    \u001b[0;36m__format__\u001b[0m\u001b[1;30m, \u001b[0m\u001b[0;36m__hash__\u001b[0m\u001b[1;30m, \u001b[0m\u001b[0;36m__init__\u001b[0m\u001b[1;30m, \u001b[0m\u001b[0;36m__new__\u001b[0m\u001b[1;30m, \u001b[0m\u001b[0;36m__repr__\u001b[0m\u001b[1;30m, \u001b[0m\u001b[0;36m__sizeof__\u001b[0m\u001b[1;30m, \u001b[0m\u001b[0;36m__str__\u001b[0m\n",
       "\u001b[0;33mrich comparison:\u001b[0m\n",
       "    \u001b[0;36m__eq__\u001b[0m\u001b[1;30m, \u001b[0m\u001b[0;36m__ge__\u001b[0m\u001b[1;30m, \u001b[0m\u001b[0;36m__gt__\u001b[0m\u001b[1;30m, \u001b[0m\u001b[0;36m__le__\u001b[0m\u001b[1;30m, \u001b[0m\u001b[0;36m__lt__\u001b[0m\u001b[1;30m, \u001b[0m\u001b[0;36m__ne__\u001b[0m\n",
       "\u001b[0;33mattribute access:\u001b[0m\n",
       "    \u001b[0;36m__delattr__\u001b[0m\u001b[1;30m, \u001b[0m\u001b[0;36m__dir__\u001b[0m\u001b[1;30m, \u001b[0m\u001b[0;36m__getattr__\u001b[0m\u001b[1;30m, \u001b[0m\u001b[0;36m__getattribute__\u001b[0m\u001b[1;30m, \u001b[0m\u001b[0;36m__setattr__\u001b[0m\n",
       "\u001b[0;33mclass customization:\u001b[0m\n",
       "    \u001b[0;36m__init_subclass__\u001b[0m\n",
       "\u001b[0;33mpickle:\u001b[0m\n",
       "    \u001b[0;36m__reduce__\u001b[0m\u001b[1;30m, \u001b[0m\u001b[0;36m__reduce_ex__\u001b[0m\u001b[1;30m, \u001b[0m\u001b[0;36m__setstate__\u001b[0m\n",
       "\u001b[0;33mdescriptor:\u001b[0m\n",
       "    \u001b[0;36mbase_model\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30m@property with getter, :obj:`torch.nn.Module`: The main body of the model.\u001b[0m\n",
       "    \u001b[0;36mdevice\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30m@property with getter, :obj:`torch.device`: The device on which the module is (assuming that all the module parameters are on the same\u001b[0m\n",
       "    \u001b[0;36mdtype\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30m@property with getter, :obj:`torch.dtype`: The dtype of the module (assuming that all the module parameters have the same dtype).\u001b[0m\n",
       "    \u001b[0;36mdummy_inputs\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30m@property with getter, :obj:`Dict[str, torch.Tensor]`: Dummy inputs to do a forward pass in the network.\u001b[0m\n",
       "    \u001b[0;36mfrom_pretrained\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30mclass classmethod with getter, classmethod(function) -> method\u001b[0m\n",
       "\u001b[0;33mstatic method:\u001b[0m\n",
       "    \u001b[0;36m_expand_inputs_for_generation\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30mstaticmethod(function) -> method\u001b[0m\n",
       "    \u001b[0;36m_hook_rss_memory_post_forward\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30mstaticmethod(function) -> method\u001b[0m\n",
       "    \u001b[0;36m_hook_rss_memory_pre_forward\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30mstaticmethod(function) -> method\u001b[0m\n",
       "    \u001b[0;36m_init_sequence_length_for_generation\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30mstaticmethod(function) -> method\u001b[0m\n",
       "    \u001b[0;36m_reorder_cache\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30mstaticmethod(function) -> method\u001b[0m\n",
       "    \u001b[0;36m_tie_encoder_decoder_weights\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30mstaticmethod(function) -> method\u001b[0m\n",
       "    \u001b[0;36m_update_model_kwargs_for_generation\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30mstaticmethod(function) -> method\u001b[0m\n",
       "    \u001b[0;36m_update_seq_length_for_generation\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30mstaticmethod(function) -> method\u001b[0m\n",
       "\u001b[0;33mclass:\u001b[0m\n",
       "    \u001b[0;36mconfig_class\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30mThis is the configuration class to store the configuration of a :class:`~transformers.AlbertModel` or a\u001b[0m\n",
       "\u001b[0;33mfunction:\u001b[0m\n",
       "    \u001b[0;36m_apply\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30m\u001b[0m\n",
       "    \u001b[0;36m_call_impl\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30m\u001b[0m\n",
       "    \u001b[0;36m_convert_head_mask_to_5d\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30m-> [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[0m\n",
       "    \u001b[0;36m_get_decoder_start_token_id\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30m\u001b[0m\n",
       "    \u001b[0;36m_get_logits_processor\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30mThis class returns a :obj:`~transformers.LogitsProcessorList` list object that contains all relevant\u001b[0m\n",
       "    \u001b[0;36m_get_logits_warper\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30mThis class returns a :obj:`~transformers.LogitsProcessorList` list object that contains all relevant\u001b[0m\n",
       "    \u001b[0;36m_get_name\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30m\u001b[0m\n",
       "    \u001b[0;36m_get_pad_token_id\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30m\u001b[0m\n",
       "    \u001b[0;36m_get_resized_embeddings\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30mBuild a resized Embedding Module from a provided token Embedding Module. Increasing the size will add newly\u001b[0m\n",
       "    \u001b[0;36m_init_weights\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30mInitialize the weights.\u001b[0m\n",
       "    \u001b[0;36m_load_from_state_dict\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30mCopies parameters and buffers from :attr:`state_dict` into only\u001b[0m\n",
       "    \u001b[0;36m_named_members\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30mHelper method for yielding various names + members of modules.\u001b[0m\n",
       "    \u001b[0;36m_prepare_attention_mask_for_generation\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30m\u001b[0m\n",
       "    \u001b[0;36m_prepare_decoder_input_ids_for_generation\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30m\u001b[0m\n",
       "    \u001b[0;36m_prepare_encoder_decoder_kwargs_for_generation\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30m\u001b[0m\n",
       "    \u001b[0;36m_prepare_input_ids_for_generation\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30m\u001b[0m\n",
       "    \u001b[0;36m_prune_heads\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30mPrunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} ALBERT has\u001b[0m\n",
       "    \u001b[0;36m_register_load_state_dict_pre_hook\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30mThese hooks will be called with arguments: `state_dict`, `prefix`,\u001b[0m\n",
       "    \u001b[0;36m_register_state_dict_hook\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30mThese hooks will be called with arguments: `self`, `state_dict`,\u001b[0m\n",
       "    \u001b[0;36m_replicate_for_data_parallel\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30m\u001b[0m\n",
       "    \u001b[0;36m_resize_token_embeddings\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30m\u001b[0m\n",
       "    \u001b[0;36m_save_to_state_dict\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30mSaves module state to `destination` dictionary, containing a state\u001b[0m\n",
       "    \u001b[0;36m_slow_forward\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30m\u001b[0m\n",
       "    \u001b[0;36m_tie_or_clone_weights\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30mTie or clone module weights depending of whether we are using TorchScript or not\u001b[0m\n",
       "    \u001b[0;36madd_memory_hooks\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30mAdd a memory hook before and after each sub-module forward pass to record increase in memory consumption.\u001b[0m\n",
       "    \u001b[0;36madd_module\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30mAdds a child module to the current module.\u001b[0m\n",
       "    \u001b[0;36madjust_logits_during_generation\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30mImplement in subclasses of :class:`~transformers.PreTrainedModel` for custom behavior to adjust the logits in\u001b[0m\n",
       "    \u001b[0;36mapply\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30mApplies ``fn`` recursively to every submodule (as returned by ``.children()``)\u001b[0m\n",
       "    \u001b[0;36mbeam_sample\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30mGenerates sequences for models with a language modeling head using beam search with multinomial sampling.\u001b[0m\n",
       "    \u001b[0;36mbeam_search\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30mGenerates sequences for models with a language modeling head using beam search decoding.\u001b[0m\n",
       "    \u001b[0;36mbfloat16\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30mCasts all floating point parameters and buffers to ``bfloat16`` datatype.\u001b[0m\n",
       "    \u001b[0;36mbuffers\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30mReturns an iterator over module buffers.\u001b[0m\n",
       "    \u001b[0;36mchildren\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30mReturns an iterator over immediate children modules.\u001b[0m\n",
       "    \u001b[0;36mcpu\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30mMoves all model parameters and buffers to the CPU.\u001b[0m\n",
       "    \u001b[0;36mcuda\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30mMoves all model parameters and buffers to the GPU.\u001b[0m\n",
       "    \u001b[0;36mdouble\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30mCasts all floating point parameters and buffers to ``double`` datatype.\u001b[0m\n",
       "    \u001b[0;36mestimate_tokens\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30mHelper function to estimate the total number of tokens from the model inputs.\u001b[0m\n",
       "    \u001b[0;36meval\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30mSets the module in evaluation mode.\u001b[0m\n",
       "    \u001b[0;36mextra_repr\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30mSet the extra representation of the module\u001b[0m\n",
       "    \u001b[0;36mfloat\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30mCasts all floating point parameters and buffers to float datatype.\u001b[0m\n",
       "    \u001b[0;36mfloating_point_ops\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30mGet number of (optionally, non-embeddings) floating-point operations for the forward and backward passes of a\u001b[0m\n",
       "    \u001b[0;36mforward\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30mThe :class:`~transformers.AlbertModel` forward method, overrides the :func:`__call__` special method.\u001b[0m\n",
       "    \u001b[0;36mgenerate\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30mGenerates sequences for models with a language modeling head. The method currently supports greedy decoding,\u001b[0m\n",
       "    \u001b[0;36mget_extended_attention_mask\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30mMakes broadcastable attention and causal masks so that future and masked tokens are ignored.\u001b[0m\n",
       "    \u001b[0;36mget_head_mask\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30mPrepare the head mask if needed.\u001b[0m\n",
       "    \u001b[0;36mget_input_embeddings\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30mReturns the model's input embeddings.\u001b[0m\n",
       "    \u001b[0;36mget_output_embeddings\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30mReturns the model's output embeddings.\u001b[0m\n",
       "    \u001b[0;36mgreedy_search\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30mGenerates sequences for models with a language modeling head using greedy decoding.\u001b[0m\n",
       "    \u001b[0;36mhalf\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30mCasts all floating point parameters and buffers to ``half`` datatype.\u001b[0m\n",
       "    \u001b[0;36minit_weights\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30mInitializes and prunes weights if needed.\u001b[0m\n",
       "    \u001b[0;36minvert_attention_mask\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30mInvert an attention mask (e.g., switches 0. and 1.).\u001b[0m\n",
       "    \u001b[0;36mload_state_dict\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30mCopies parameters and buffers from :attr:`state_dict` into\u001b[0m\n",
       "    \u001b[0;36mload_tf_weights\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30mLoad tf checkpoints in a pytorch model.\u001b[0m\n",
       "    \u001b[0;36mmodules\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30mReturns an iterator over all modules in the network.\u001b[0m\n",
       "    \u001b[0;36mnamed_buffers\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30mReturns an iterator over module buffers, yielding both the\u001b[0m\n",
       "    \u001b[0;36mnamed_children\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30mReturns an iterator over immediate children modules, yielding both\u001b[0m\n",
       "    \u001b[0;36mnamed_modules\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30mReturns an iterator over all modules in the network, yielding\u001b[0m\n",
       "    \u001b[0;36mnamed_parameters\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30mReturns an iterator over module parameters, yielding both the\u001b[0m\n",
       "    \u001b[0;36mnum_parameters\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30mGet number of (optionally, trainable or non-embeddings) parameters in the module.\u001b[0m\n",
       "    \u001b[0;36mparameters\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30mReturns an iterator over module parameters.\u001b[0m\n",
       "    \u001b[0;36mprepare_inputs_for_generation\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30mImplement in subclasses of :class:`~transformers.PreTrainedModel` for custom behavior to prepare inputs in the\u001b[0m\n",
       "    \u001b[0;36mprune_heads\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30mPrunes heads of the base model.\u001b[0m\n",
       "    \u001b[0;36mregister_backward_hook\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30mRegisters a backward hook on the module.\u001b[0m\n",
       "    \u001b[0;36mregister_buffer\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30mAdds a buffer to the module.\u001b[0m\n",
       "    \u001b[0;36mregister_forward_hook\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30mRegisters a forward hook on the module.\u001b[0m\n",
       "    \u001b[0;36mregister_forward_pre_hook\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30mRegisters a forward pre-hook on the module.\u001b[0m\n",
       "    \u001b[0;36mregister_parameter\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30mAdds a parameter to the module.\u001b[0m\n",
       "    \u001b[0;36mrequires_grad_\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30mChange if autograd should record operations on parameters in this\u001b[0m\n",
       "    \u001b[0;36mreset_memory_hooks_state\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30mReset the :obj:`mem_rss_diff` attribute of each module (see\u001b[0m\n",
       "    \u001b[0;36mresize_token_embeddings\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30mResizes input token embeddings matrix of the model if :obj:`new_num_tokens != config.vocab_size`.\u001b[0m\n",
       "    \u001b[0;36msample\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30mGenerates sequences for models with a language modeling head using multinomial sampling.\u001b[0m\n",
       "    \u001b[0;36msave_pretrained\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30mSave a model and its configuration file to a directory, so that it can be re-loaded using the\u001b[0m\n",
       "    \u001b[0;36mset_input_embeddings\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30mSet model's input embeddings.\u001b[0m\n",
       "    \u001b[0;36mshare_memory\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30m\u001b[0m\n",
       "    \u001b[0;36mstate_dict\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30mReturns a dictionary containing a whole state of the module.\u001b[0m\n",
       "    \u001b[0;36mtie_weights\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30mTie the weights between the input embeddings and the output embeddings.\u001b[0m\n",
       "    \u001b[0;36mto\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30mMoves and/or casts the parameters and buffers.\u001b[0m\n",
       "    \u001b[0;36mtrain\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30mSets the module in training mode.\u001b[0m\n",
       "    \u001b[0;36mtype\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30mCasts all parameters and buffers to :attr:`dst_type`.\u001b[0m\n",
       "    \u001b[0;36mzero_grad\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30mSets gradients of all model parameters to zero. See similar function\u001b[0m\n",
       "\u001b[0;33mmagic:\u001b[0m\n",
       "    \u001b[0;36m__call__\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30m\u001b[0m"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdir(mo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AlbertModel(\n",
       "  (embeddings): AlbertEmbeddings(\n",
       "    (word_embeddings): Embedding(30000, 128, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 128)\n",
       "    (token_type_embeddings): Embedding(2, 128)\n",
       "    (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0, inplace=False)\n",
       "  )\n",
       "  (encoder): AlbertTransformer(\n",
       "    (embedding_hidden_mapping_in): Linear(in_features=128, out_features=768, bias=True)\n",
       "    (albert_layer_groups): ModuleList(\n",
       "      (0): AlbertLayerGroup(\n",
       "        (albert_layers): ModuleList(\n",
       "          (0): AlbertLayer(\n",
       "            (full_layer_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (attention): AlbertAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (attention_dropout): Dropout(p=0, inplace=False)\n",
       "              (output_dropout): Dropout(p=0, inplace=False)\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            )\n",
       "            (ffn): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (ffn_output): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (pooler_activation): Tanh()\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=['aten::Int', 'aten::ScalarImplicit', 'aten::abs', 'aten::add', 'aten::add_', 'aten::addmm', 'aten::arange', 'aten::bmm', 'aten::cat', 'aten::clone', 'aten::constant_pad_nd', 'aten::contiguous', 'aten::copy_', 'aten::cumsum', 'aten::div', 'aten::dropout', 'aten::einsum', 'aten::embedding', 'aten::eq', 'aten::expand_as', 'aten::fill_', 'aten::floor_divide', 'aten::full_like', 'aten::gather', 'aten::gelu', 'aten::index', 'aten::layer_norm', 'aten::le', 'aten::log', 'aten::lt', 'aten::masked_fill', 'aten::masked_fill_', 'aten::matmul', 'aten::max', 'aten::mean', 'aten::min', 'aten::mul', 'aten::mul_', 'aten::ne', 'aten::neg', 'aten::ones', 'aten::permute', 'aten::pow', 'aten::relu', 'aten::repeat', 'aten::reshape', 'aten::rsub', 'aten::select', 'aten::size', 'aten::slice', 'aten::softmax', 'aten::split', 'aten::sqrt', 'aten::squeeze', 'aten::stack', 'aten::sub', 'aten::sum', 'aten::t', 'aten::tanh', 'aten::to', 'aten::transpose', 'aten::triu', 'aten::type_as', 'aten::unsqueeze', 'aten::view', 'aten::where', 'aten::zeros', 'aten::zeros_like', 'prim::Constant', 'prim::GetAttr', 'prim::ListConstruct', 'prim::ListUnpack', 'prim::NumToTensor', 'prim::TupleConstruct', 'prim::TupleUnpack']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'aten::Int': 'aten_Int',\n",
      "'aten::ScalarImplicit': 'aten_ScalarImplicit',\n",
      "'aten::abs': 'aten_abs',\n",
      "'aten::add': 'aten_add',\n",
      "'aten::add_': 'aten_add_',\n",
      "'aten::addmm': 'aten_addmm',\n",
      "'aten::arange': 'aten_arange',\n",
      "'aten::bmm': 'aten_bmm',\n",
      "'aten::cat': 'aten_cat',\n",
      "'aten::clone': 'aten_clone',\n",
      "'aten::constant_pad_nd': 'aten_constant_pad_nd',\n",
      "'aten::contiguous': 'aten_contiguous',\n",
      "'aten::copy_': 'aten_copy_',\n",
      "'aten::cumsum': 'aten_cumsum',\n",
      "'aten::div': 'aten_div',\n",
      "'aten::dropout': 'aten_dropout',\n",
      "'aten::einsum': 'aten_einsum',\n",
      "'aten::embedding': 'aten_embedding',\n",
      "'aten::eq': 'aten_eq',\n",
      "'aten::expand_as': 'aten_expand_as',\n",
      "'aten::fill_': 'aten_fill_',\n",
      "'aten::floor_divide': 'aten_floor_divide',\n",
      "'aten::full_like': 'aten_full_like',\n",
      "'aten::gather': 'aten_gather',\n",
      "'aten::gelu': 'aten_gelu',\n",
      "'aten::index': 'aten_index',\n",
      "'aten::layer_norm': 'aten_layer_norm',\n",
      "'aten::le': 'aten_le',\n",
      "'aten::log': 'aten_log',\n",
      "'aten::lt': 'aten_lt',\n",
      "'aten::masked_fill': 'aten_masked_fill',\n",
      "'aten::masked_fill_': 'aten_masked_fill_',\n",
      "'aten::matmul': 'aten_matmul',\n",
      "'aten::max': 'aten_max',\n",
      "'aten::mean': 'aten_mean',\n",
      "'aten::min': 'aten_min',\n",
      "'aten::mul': 'aten_mul',\n",
      "'aten::mul_': 'aten_mul_',\n",
      "'aten::ne': 'aten_ne',\n",
      "'aten::neg': 'aten_neg',\n",
      "'aten::ones': 'aten_ones',\n",
      "'aten::permute': 'aten_permute',\n",
      "'aten::pow': 'aten_pow',\n",
      "'aten::relu': 'aten_relu',\n",
      "'aten::repeat': 'aten_repeat',\n",
      "'aten::reshape': 'aten_reshape',\n",
      "'aten::rsub': 'aten_rsub',\n",
      "'aten::select': 'aten_select',\n",
      "'aten::size': 'aten_size',\n",
      "'aten::slice': 'aten_slice',\n",
      "'aten::softmax': 'aten_softmax',\n",
      "'aten::split': 'aten_split',\n",
      "'aten::sqrt': 'aten_sqrt',\n",
      "'aten::squeeze': 'aten_squeeze',\n",
      "'aten::stack': 'aten_stack',\n",
      "'aten::sub': 'aten_sub',\n",
      "'aten::sum': 'aten_sum',\n",
      "'aten::t': 'aten_t',\n",
      "'aten::tanh': 'aten_tanh',\n",
      "'aten::to': 'aten_to',\n",
      "'aten::transpose': 'aten_transpose',\n",
      "'aten::triu': 'aten_triu',\n",
      "'aten::type_as': 'aten_type_as',\n",
      "'aten::unsqueeze': 'aten_unsqueeze',\n",
      "'aten::view': 'aten_view',\n",
      "'aten::where': 'aten_where',\n",
      "'aten::zeros': 'aten_zeros',\n",
      "'aten::zeros_like': 'aten_zeros_like',\n",
      "'prim::Constant': 'prim_Constant',\n",
      "'prim::GetAttr': 'prim_GetAttr',\n",
      "'prim::ListConstruct': 'prim_ListConstruct',\n",
      "'prim::ListUnpack': 'prim_ListUnpack',\n",
      "'prim::NumToTensor': 'prim_NumToTensor',\n",
      "'prim::TupleConstruct': 'prim_TupleConstruct',\n",
      "'prim::TupleUnpack': 'prim_TupleUnpack',\n"
     ]
    }
   ],
   "source": [
    "for ai in a:\n",
    "    print(f\"'{ai}': '{ai.replace('::', '_')}',\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.zeros(100, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.getsizeof(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start_times = dict()\n",
    "end_times = dict()\n",
    "\n",
    "def log_start_builder(name):\n",
    "    def log_start(module, m_in):\n",
    "        start_times[f'{name}:{module.__class__.__name__}'] = time.perf_counter()\n",
    "    return log_start\n",
    "def log_end_builder(name):\n",
    "    def log_end(module, m_in, m_out):\n",
    "        end_times[f'{name}:{module.__class__.__name__}'] = time.perf_counter()\n",
    "#         print(name, module.__class__.__name__, 'end', time.perf_counter())\n",
    "    return log_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " AlbertModel\n",
      "embeddings AlbertEmbeddings\n",
      "embeddings.word_embeddings Embedding\n",
      "embeddings.position_embeddings Embedding\n",
      "embeddings.token_type_embeddings Embedding\n",
      "embeddings.LayerNorm LayerNorm\n",
      "embeddings.dropout Dropout\n",
      "encoder AlbertTransformer\n",
      "encoder.embedding_hidden_mapping_in Linear\n",
      "encoder.albert_layer_groups ModuleList\n",
      "encoder.albert_layer_groups.0 AlbertLayerGroup\n",
      "encoder.albert_layer_groups.0.albert_layers ModuleList\n",
      "encoder.albert_layer_groups.0.albert_layers.0 AlbertLayer\n",
      "encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm LayerNorm\n",
      "encoder.albert_layer_groups.0.albert_layers.0.attention AlbertAttention\n",
      "encoder.albert_layer_groups.0.albert_layers.0.attention.query Linear\n",
      "encoder.albert_layer_groups.0.albert_layers.0.attention.key Linear\n",
      "encoder.albert_layer_groups.0.albert_layers.0.attention.value Linear\n",
      "encoder.albert_layer_groups.0.albert_layers.0.attention.attention_dropout Dropout\n",
      "encoder.albert_layer_groups.0.albert_layers.0.attention.output_dropout Dropout\n",
      "encoder.albert_layer_groups.0.albert_layers.0.attention.dense Linear\n",
      "encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm LayerNorm\n",
      "encoder.albert_layer_groups.0.albert_layers.0.ffn Linear\n",
      "encoder.albert_layer_groups.0.albert_layers.0.ffn_output Linear\n",
      "encoder.albert_layer_groups.0.albert_layers.0.dropout Dropout\n",
      "pooler Linear\n",
      "pooler_activation Tanh\n"
     ]
    }
   ],
   "source": [
    "for name, module in mo.named_modules():\n",
    "    print(name, module.__class__.__name__)\n",
    "    module.register_forward_pre_hook(log_start_builder(name))\n",
    "    module.register_forward_hook(log_end_builder(name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:229: TracerWarning: Converting a tensor to a Python index might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  position_ids = self.position_ids[:, :seq_length]\n",
      "/Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_utils.py:1673: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  input_tensor.shape == tensor_shape for input_tensor in input_tensors\n"
     ]
    }
   ],
   "source": [
    "trace = torch.jit.trace(model, inputs) # 1605233162.4006221"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":AlbertModel, 485.596 ms, 98.592858562, 99.078454732\n",
      "embeddings:AlbertEmbeddings, 0.386 ms, 98.593064335, 98.593450238\n",
      "embeddings.word_embeddings:Embedding, 0.058 ms, 98.593096499, 98.593154752\n",
      "embeddings.position_embeddings:Embedding, 0.037 ms, 98.593165679, 98.593202809\n",
      "embeddings.token_type_embeddings:Embedding, 0.032 ms, 98.593211549, 98.593243077\n",
      "embeddings.LayerNorm:LayerNorm, 0.078 ms, 98.5933244, 98.593402786\n",
      "embeddings.dropout:Dropout, 0.023 ms, 98.593415764, 98.593438446\n",
      "encoder:AlbertTransformer, 484.573 ms, 98.593459438, 99.078032653\n",
      "encoder.embedding_hidden_mapping_in:Linear, 0.682 ms, 98.593469199, 98.594151621\n",
      "encoder.albert_layer_groups.0:AlbertLayerGroup, 39.481 ms, 99.03853302, 99.078014506\n",
      "encoder.albert_layer_groups.0.albert_layers.0:AlbertLayer, 39.456 ms, 99.038551111, 99.07800696\n",
      "encoder.albert_layer_groups.0.albert_layers.0.attention:AlbertAttention, 14.133 ms, 99.038580346, 99.052713264\n",
      "encoder.albert_layer_groups.0.albert_layers.0.attention.query:Linear, 2.921 ms, 99.03859484, 99.041515629\n",
      "encoder.albert_layer_groups.0.albert_layers.0.attention.key:Linear, 2.901 ms, 99.041537627, 99.044438786\n",
      "encoder.albert_layer_groups.0.albert_layers.0.attention.value:Linear, 3.040 ms, 99.044453661, 99.047494067\n",
      "encoder.albert_layer_groups.0.albert_layers.0.attention.attention_dropout:Dropout, 0.027 ms, 99.048781587, 99.04880908\n",
      "encoder.albert_layer_groups.0.albert_layers.0.attention.output_dropout:Dropout, 0.030 ms, 99.052437421, 99.052467603\n",
      "encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm:LayerNorm, 0.127 ms, 99.052555287, 99.052682087\n",
      "encoder.albert_layer_groups.0.albert_layers.0.ffn:Linear, 11.446 ms, 99.052821117, 99.064266891\n",
      "encoder.albert_layer_groups.0.albert_layers.0.ffn_output:Linear, 11.629 ms, 99.066083888, 99.077712816\n",
      "encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm:LayerNorm, 0.131 ms, 99.077854299, 99.077985603\n",
      "pooler:Linear, 0.297 ms, 99.078075593, 99.078372658\n",
      "pooler_activation:Tanh, 0.033 ms, 99.078382758, 99.078415853\n"
     ]
    }
   ],
   "source": [
    "for k, start in start_times.items():\n",
    "    print(f'{k}, {(end_times[k]-start)*1000:.3f} ms, {start}, {end_times[k]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = trace.inlined_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "graph(%self.1 : __torch__.transformers.modeling_albert.AlbertModel,\n",
       "      %input_ids : Long(1:100, 100:1, requires_grad=0, device=cpu)):\n",
       "  %2 : __torch__.torch.nn.modules.activation.Tanh = prim::GetAttr[name=\"pooler_activation\"](%self.1)\n",
       "  %3 : __torch__.torch.nn.modules.linear.___torch_mangle_14.Linear = prim::GetAttr[name=\"pooler\"](%self.1)\n",
       "  %4 : __torch__.transformers.modeling_albert.AlbertTransformer = prim::GetAttr[name=\"encoder\"](%self.1)\n",
       "  %5 : __torch__.transformers.modeling_albert.AlbertEmbeddings = prim::GetAttr[name=\"embeddings\"](%self.1)\n",
       "  %6 : int = prim::Constant[value=0]() # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:662:0\n",
       "  %7 : int = aten::size(%input_ids, %6) # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:662:0\n",
       "  %8 : Long(device=cpu) = prim::NumToTensor(%7)\n",
       "  %9 : int = aten::Int(%8)\n",
       "  %10 : int = aten::Int(%8)\n",
       "  %11 : int = prim::Constant[value=1]() # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:662:0\n",
       "  %12 : int = aten::size(%input_ids, %11) # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:662:0\n",
       "  %13 : Long(device=cpu) = prim::NumToTensor(%12)\n",
       "  %14 : int = aten::Int(%13)\n",
       "  %15 : int = aten::Int(%13)\n",
       "  %16 : int[] = prim::ListConstruct(%10, %15)\n",
       "  %17 : int = prim::Constant[value=6]() # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:671:0\n",
       "  %18 : None = prim::Constant()\n",
       "  %19 : Device = prim::Constant[value=\"cpu\"]() # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:671:0\n",
       "  %20 : bool = prim::Constant[value=0]() # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:671:0\n",
       "  %attention_mask.1 : Float(1:100, 100:1, requires_grad=0, device=cpu) = aten::ones(%16, %17, %18, %19, %20) # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:671:0\n",
       "  %22 : int[] = prim::ListConstruct(%9, %14)\n",
       "  %23 : int = prim::Constant[value=4]() # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:673:0\n",
       "  %24 : None = prim::Constant()\n",
       "  %25 : Device = prim::Constant[value=\"cpu\"]() # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:673:0\n",
       "  %26 : bool = prim::Constant[value=0]() # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:673:0\n",
       "  %input.2 : Long(1:100, 100:1, requires_grad=0, device=cpu) = aten::zeros(%22, %23, %24, %25, %26) # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:673:0\n",
       "  %28 : int = prim::Constant[value=1]() # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:675:0\n",
       "  %29 : Float(1:100, 1:100, 100:1, requires_grad=0, device=cpu) = aten::unsqueeze(%attention_mask.1, %28) # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:675:0\n",
       "  %30 : int = prim::Constant[value=2]() # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:675:0\n",
       "  %extended_attention_mask : Float(1:100, 1:100, 1:100, 100:1, requires_grad=0, device=cpu) = aten::unsqueeze(%29, %30) # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:675:0\n",
       "  %32 : int = prim::Constant[value=6]() # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:676:0\n",
       "  %33 : bool = prim::Constant[value=0]() # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:676:0\n",
       "  %34 : bool = prim::Constant[value=0]() # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:676:0\n",
       "  %35 : None = prim::Constant()\n",
       "  %36 : Float(1:100, 1:100, 1:100, 100:1, requires_grad=0, device=cpu) = aten::to(%extended_attention_mask, %32, %33, %34, %35) # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:676:0\n",
       "  %37 : float = prim::Constant[value=1.]() # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/tensor.py:511:0\n",
       "  %38 : int = prim::Constant[value=1]() # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/tensor.py:511:0\n",
       "  %39 : Float(1:100, 1:100, 1:100, 100:1, requires_grad=0, device=cpu) = aten::rsub(%36, %37, %38) # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/tensor.py:511:0\n",
       "  %40 : Double(requires_grad=0, device=cpu) = prim::Constant[value={-10000}]() # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:677:0\n",
       "  %attention_mask : Float(1:100, 1:100, 1:100, 100:1, requires_grad=0, device=cpu) = aten::mul(%39, %40) # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:677:0\n",
       "  %55 : float = prim::Constant[value=0.](), scope: __module.embeddings/__module.embeddings.dropout # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:983:0\n",
       "  %56 : int = prim::Constant[value=128](), scope: __module.embeddings/__module.embeddings.LayerNorm # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:2094:0\n",
       "  %57 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.embeddings/__module.embeddings.LayerNorm # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:2094:0\n",
       "  %58 : bool = prim::Constant[value=1](), scope: __module.embeddings/__module.embeddings.LayerNorm # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:2094:0\n",
       "  %59 : int = prim::Constant[value=-1](), scope: __module.embeddings/__module.embeddings.position_embeddings # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1852:0\n",
       "  %60 : bool = prim::Constant[value=0](), scope: __module.embeddings/__module.embeddings.word_embeddings # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1852:0\n",
       "  %61 : int = prim::Constant[value=9223372036854775807](), scope: __module.embeddings # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:229:0\n",
       "  %62 : int = prim::Constant[value=0](), scope: __module.embeddings # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:229:0\n",
       "  %63 : int = prim::Constant[value=1](), scope: __module.embeddings # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:222:0\n",
       "  %64 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name=\"LayerNorm\"](%5)\n",
       "  %65 : __torch__.torch.nn.modules.sparse.___torch_mangle_1.Embedding = prim::GetAttr[name=\"token_type_embeddings\"](%5)\n",
       "  %66 : __torch__.torch.nn.modules.sparse.___torch_mangle_0.Embedding = prim::GetAttr[name=\"position_embeddings\"](%5)\n",
       "  %67 : __torch__.torch.nn.modules.sparse.Embedding = prim::GetAttr[name=\"word_embeddings\"](%5)\n",
       "  %68 : Tensor = prim::GetAttr[name=\"position_ids\"](%5)\n",
       "  %69 : int = aten::size(%input_ids, %63), scope: __module.embeddings # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:222:0\n",
       "  %70 : Long(1:512, 512:1, requires_grad=0, device=cpu) = aten::slice(%68, %62, %62, %61, %63), scope: __module.embeddings # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:229:0\n",
       "  %input.1 : Long(1:512, 100:1, requires_grad=0, device=cpu) = aten::slice(%70, %63, %62, %69, %63), scope: __module.embeddings # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:229:0\n",
       "  %72 : Tensor = prim::GetAttr[name=\"weight\"](%67)\n",
       "  %inputs_embeds : Float(1:12800, 100:128, 128:1, requires_grad=1, device=cpu) = aten::embedding(%72, %input_ids, %62, %60, %60), scope: __module.embeddings/__module.embeddings.word_embeddings # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1852:0\n",
       "  %74 : Tensor = prim::GetAttr[name=\"weight\"](%66)\n",
       "  %position_embeddings : Float(1:12800, 100:128, 128:1, requires_grad=1, device=cpu) = aten::embedding(%74, %input.1, %59, %60, %60), scope: __module.embeddings/__module.embeddings.position_embeddings # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1852:0\n",
       "  %76 : Tensor = prim::GetAttr[name=\"weight\"](%65)\n",
       "  %token_type_embeddings : Float(1:12800, 100:128, 128:1, requires_grad=1, device=cpu) = aten::embedding(%76, %input.2, %59, %60, %60), scope: __module.embeddings/__module.embeddings.token_type_embeddings # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1852:0\n",
       "  %78 : Float(1:12800, 100:128, 128:1, requires_grad=1, device=cpu) = aten::add(%inputs_embeds, %position_embeddings, %63), scope: __module.embeddings # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:239:0\n",
       "  %input.3 : Float(1:12800, 100:128, 128:1, requires_grad=1, device=cpu) = aten::add(%78, %token_type_embeddings, %63), scope: __module.embeddings # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:239:0\n",
       "  %80 : Tensor = prim::GetAttr[name=\"bias\"](%64)\n",
       "  %81 : Tensor = prim::GetAttr[name=\"weight\"](%64)\n",
       "  %82 : int[] = prim::ListConstruct(%56), scope: __module.embeddings/__module.embeddings.LayerNorm\n",
       "  %input.4 : Float(1:12800, 100:128, 128:1, requires_grad=1, device=cpu) = aten::layer_norm(%input.3, %82, %81, %80, %57, %58), scope: __module.embeddings/__module.embeddings.LayerNorm # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:2094:0\n",
       "  %input.5 : Float(1:12800, 100:128, 128:1, requires_grad=1, device=cpu) = aten::dropout(%input.4, %55, %60), scope: __module.embeddings/__module.embeddings.dropout # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:983:0\n",
       "  %85 : Double(requires_grad=0, device=cpu) = prim::Constant[value={1}](), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/activations.py:38:0\n",
       "  %86 : Double(requires_grad=0, device=cpu) = prim::Constant[value={0.044715}](), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/activations.py:38:0\n",
       "  %87 : Double(requires_grad=0, device=cpu) = prim::Constant[value={0.797885}](), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/activations.py:38:0\n",
       "  %88 : Double(requires_grad=0, device=cpu) = prim::Constant[value={0.5}](), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/activations.py:38:0\n",
       "  %89 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:271:0\n",
       "  %90 : int = prim::Constant[value=12](), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:272:0\n",
       "  %91 : int = prim::Constant[value=64](), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:272:0\n",
       "  %92 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:273:0\n",
       "  %93 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:273:0\n",
       "  %94 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:303:0\n",
       "  %95 : int = prim::Constant[value=-2](), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:303:0\n",
       "  %96 : Double(requires_grad=0, device=cpu) = prim::Constant[value={8}](), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:304:0\n",
       "  %97 : None = prim::Constant(), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention\n",
       "  %98 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.attention_dropout # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:983:0\n",
       "  %99 : float = prim::Constant[value=0.](), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.attention_dropout # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:983:0\n",
       "  %100 : int = prim::Constant[value=768](), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:326:0\n",
       "  %101 : int = prim::Constant[value=6](), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:326:0\n",
       "  %102 : str = prim::Constant[value=\"bfnd,ndh->bfh\"](), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/functional.py:344:0\n",
       "  %103 : bool = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:2094:0\n",
       "  %104 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:2094:0\n",
       "  %105 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.embedding_hidden_mapping_in # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1694:0\n",
       "  %106 : __torch__.torch.nn.modules.container.___torch_mangle_13.ModuleList = prim::GetAttr[name=\"albert_layer_groups\"](%4)\n",
       "  %107 : __torch__.transformers.modeling_albert.AlbertLayerGroup = prim::GetAttr[name=\"0\"](%106)\n",
       "  %108 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name=\"embedding_hidden_mapping_in\"](%4)\n",
       "  %109 : Tensor = prim::GetAttr[name=\"bias\"](%108)\n",
       "  %110 : Tensor = prim::GetAttr[name=\"weight\"](%108)\n",
       "  %111 : Float(128:1, 768:128, requires_grad=1, device=cpu) = aten::t(%110), scope: __module.encoder/__module.encoder.embedding_hidden_mapping_in # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1692:0\n",
       "  %output.1 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::matmul(%input.5, %111), scope: __module.encoder/__module.encoder.embedding_hidden_mapping_in # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1692:0\n",
       "  %input.6 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::add_(%output.1, %109, %105), scope: __module.encoder/__module.encoder.embedding_hidden_mapping_in # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1694:0\n",
       "  %114 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name=\"albert_layers\"](%107)\n",
       "  %115 : __torch__.transformers.modeling_albert.AlbertLayer = prim::GetAttr[name=\"0\"](%114)\n",
       "  %116 : __torch__.torch.nn.modules.normalization.___torch_mangle_2.LayerNorm = prim::GetAttr[name=\"full_layer_layer_norm\"](%115)\n",
       "  %117 : __torch__.torch.nn.modules.linear.___torch_mangle_11.Linear = prim::GetAttr[name=\"ffn_output\"](%115)\n",
       "  %118 : __torch__.torch.nn.modules.linear.___torch_mangle_10.Linear = prim::GetAttr[name=\"ffn\"](%115)\n",
       "  %119 : __torch__.transformers.modeling_albert.AlbertAttention = prim::GetAttr[name=\"attention\"](%115)\n",
       "  %120 : __torch__.torch.nn.modules.normalization.___torch_mangle_9.LayerNorm = prim::GetAttr[name=\"LayerNorm\"](%119)\n",
       "  %121 : __torch__.torch.nn.modules.linear.___torch_mangle_8.Linear = prim::GetAttr[name=\"dense\"](%119)\n",
       "  %122 : Tensor = prim::GetAttr[name=\"bias\"](%121)\n",
       "  %123 : __torch__.torch.nn.modules.linear.___torch_mangle_8.Linear = prim::GetAttr[name=\"dense\"](%119)\n",
       "  %124 : Tensor = prim::GetAttr[name=\"weight\"](%123)\n",
       "  %125 : __torch__.torch.nn.modules.linear.___torch_mangle_5.Linear = prim::GetAttr[name=\"value\"](%119)\n",
       "  %126 : __torch__.torch.nn.modules.linear.___torch_mangle_4.Linear = prim::GetAttr[name=\"key\"](%119)\n",
       "  %127 : __torch__.torch.nn.modules.linear.___torch_mangle_3.Linear = prim::GetAttr[name=\"query\"](%119)\n",
       "  %128 : Tensor = prim::GetAttr[name=\"bias\"](%127)\n",
       "  %129 : Tensor = prim::GetAttr[name=\"weight\"](%127)\n",
       "  %130 : Float(768:1, 768:768, requires_grad=1, device=cpu) = aten::t(%129), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.query # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1692:0\n",
       "  %output.2 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::matmul(%input.6, %130), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.query # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1692:0\n",
       "  %x.1 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::add_(%output.2, %128, %105), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.query # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1694:0\n",
       "  %133 : Tensor = prim::GetAttr[name=\"bias\"](%126)\n",
       "  %134 : Tensor = prim::GetAttr[name=\"weight\"](%126)\n",
       "  %135 : Float(768:1, 768:768, requires_grad=1, device=cpu) = aten::t(%134), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.key # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1692:0\n",
       "  %output.3 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::matmul(%input.6, %135), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.key # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1692:0\n",
       "  %x.3 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::add_(%output.3, %133, %105), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.key # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1694:0\n",
       "  %138 : Tensor = prim::GetAttr[name=\"bias\"](%125)\n",
       "  %139 : Tensor = prim::GetAttr[name=\"weight\"](%125)\n",
       "  %140 : Float(768:1, 768:768, requires_grad=1, device=cpu) = aten::t(%139), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.value # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1692:0\n",
       "  %output.4 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::matmul(%input.6, %140), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.value # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1692:0\n",
       "  %x.5 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::add_(%output.4, %138, %105), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.value # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1694:0\n",
       "  %143 : int = aten::size(%x.1, %89), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:271:0\n",
       "  %144 : int = aten::size(%x.1, %105), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:271:0\n",
       "  %145 : int[] = prim::ListConstruct(%143, %144, %90, %91), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention\n",
       "  %x.2 : Float(1:76800, 100:768, 12:64, 64:1, requires_grad=1, device=cpu) = aten::view(%x.1, %145), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:272:0\n",
       "  %147 : int[] = prim::ListConstruct(%89, %92, %105, %93), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention\n",
       "  %query_layer.1 : Float(1:76800, 12:64, 100:768, 64:1, requires_grad=1, device=cpu) = aten::permute(%x.2, %147), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:273:0\n",
       "  %149 : int = aten::size(%x.3, %89), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:271:0\n",
       "  %150 : int = aten::size(%x.3, %105), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:271:0\n",
       "  %151 : int[] = prim::ListConstruct(%149, %150, %90, %91), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention\n",
       "  %x.4 : Float(1:76800, 100:768, 12:64, 64:1, requires_grad=1, device=cpu) = aten::view(%x.3, %151), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:272:0\n",
       "  %153 : int[] = prim::ListConstruct(%89, %92, %105, %93), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention\n",
       "  %key_layer.1 : Float(1:76800, 12:64, 100:768, 64:1, requires_grad=1, device=cpu) = aten::permute(%x.4, %153), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:273:0\n",
       "  %155 : int = aten::size(%x.5, %89), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:271:0\n",
       "  %156 : int = aten::size(%x.5, %105), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:271:0\n",
       "  %157 : int[] = prim::ListConstruct(%155, %156, %90, %91), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention\n",
       "  %x.6 : Float(1:76800, 100:768, 12:64, 64:1, requires_grad=1, device=cpu) = aten::view(%x.5, %157), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:272:0\n",
       "  %159 : int[] = prim::ListConstruct(%89, %92, %105, %93), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention\n",
       "  %value_layer.1 : Float(1:76800, 12:64, 100:768, 64:1, requires_grad=1, device=cpu) = aten::permute(%x.6, %159), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:273:0\n",
       "  %161 : Float(1:76800, 12:64, 64:1, 100:768, requires_grad=1, device=cpu) = aten::transpose(%key_layer.1, %94, %95), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:303:0\n",
       "  %attention_scores.1 : Float(1:120000, 12:10000, 100:100, 100:1, requires_grad=1, device=cpu) = aten::matmul(%query_layer.1, %161), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:303:0\n",
       "  %attention_scores.2 : Float(1:120000, 12:10000, 100:100, 100:1, requires_grad=1, device=cpu) = aten::div(%attention_scores.1, %96), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:304:0\n",
       "  %input.7 : Float(1:120000, 12:10000, 100:100, 100:1, requires_grad=1, device=cpu) = aten::add(%attention_scores.2, %attention_mask, %105), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:307:0\n",
       "  %input.8 : Float(1:120000, 12:10000, 100:100, 100:1, requires_grad=1, device=cpu) = aten::softmax(%input.7, %94, %97), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1512:0\n",
       "  %attention_probs.1 : Float(1:120000, 12:10000, 100:100, 100:1, requires_grad=1, device=cpu) = aten::dropout(%input.8, %99, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.attention_dropout # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:983:0\n",
       "  %context_layer.1 : Float(1:76800, 12:6400, 100:64, 64:1, requires_grad=1, device=cpu) = aten::matmul(%attention_probs.1, %value_layer.1), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:320:0\n",
       "  %168 : int[] = prim::ListConstruct(%89, %92, %105, %93), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention\n",
       "  %169 : Float(1:76800, 100:64, 12:6400, 64:1, requires_grad=1, device=cpu) = aten::permute(%context_layer.1, %168), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:322:0\n",
       "  %170 : Float(1:76800, 100:768, 12:64, 64:1, requires_grad=1, device=cpu) = aten::contiguous(%169, %89), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:322:0\n",
       "  %171 : Float(768:1, 768:768, requires_grad=1, device=cpu) = aten::t(%124), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:326:0\n",
       "  %172 : int[] = prim::ListConstruct(%90, %91, %100), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention\n",
       "  %173 : Float(12:64, 64:1, 768:768, requires_grad=1, device=cpu) = aten::view(%171, %172), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:326:0\n",
       "  %174 : Float(12:64, 64:1, 768:768, requires_grad=1, device=cpu) = aten::to(%173, %101, %98, %98, %97), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:326:0\n",
       "  %b.1 : Float(768:1, requires_grad=1, device=cpu) = aten::to(%122, %101, %98, %98, %97), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:330:0\n",
       "  %176 : Tensor[] = prim::ListConstruct(%170, %174), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention\n",
       "  %177 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::einsum(%102, %176), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/functional.py:344:0\n",
       "  %input.9 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::add(%177, %b.1, %105), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:332:0\n",
       "  %projected_context_layer.1 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::dropout(%input.9, %99, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.output_dropout # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:983:0\n",
       "  %input.10 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::add(%input.6, %projected_context_layer.1, %105), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:334:0\n",
       "  %181 : Tensor = prim::GetAttr[name=\"bias\"](%120)\n",
       "  %182 : Tensor = prim::GetAttr[name=\"weight\"](%120)\n",
       "  %183 : int[] = prim::ListConstruct(%100), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm\n",
       "  %input_tensor.1 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::layer_norm(%input.10, %183, %182, %181, %104, %103), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:2094:0\n",
       "  %185 : (Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu), Float(768:1, requires_grad=1, device=cpu)) = prim::TupleConstruct(%input_tensor.1, %b.1)\n",
       "  %186 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu), %187 : Float(768:1, requires_grad=1, device=cpu) = prim::TupleUnpack(%185)\n",
       "  %188 : Tensor = prim::GetAttr[name=\"bias\"](%118)\n",
       "  %189 : Tensor = prim::GetAttr[name=\"weight\"](%118)\n",
       "  %190 : Float(768:1, 3072:768, requires_grad=1, device=cpu) = aten::t(%189), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.ffn # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1692:0\n",
       "  %output.5 : Float(1:307200, 100:3072, 3072:1, requires_grad=1, device=cpu) = aten::matmul(%186, %190), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.ffn # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1692:0\n",
       "  %x.7 : Float(1:307200, 100:3072, 3072:1, requires_grad=1, device=cpu) = aten::add_(%output.5, %188, %105), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.ffn # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1694:0\n",
       "  %193 : Float(1:307200, 100:3072, 3072:1, requires_grad=1, device=cpu) = aten::mul(%x.7, %88), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/activations.py:38:0\n",
       "  %194 : Float(1:307200, 100:3072, 3072:1, requires_grad=1, device=cpu) = aten::mul(%x.7, %87), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/activations.py:38:0\n",
       "  %195 : Float(1:307200, 100:3072, 3072:1, requires_grad=1, device=cpu) = aten::mul(%x.7, %86), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/activations.py:38:0\n",
       "  %196 : Float(1:307200, 100:3072, 3072:1, requires_grad=1, device=cpu) = aten::mul(%195, %x.7), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/activations.py:38:0\n",
       "  %197 : Float(1:307200, 100:3072, 3072:1, requires_grad=1, device=cpu) = aten::add(%196, %85, %105), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/activations.py:38:0\n",
       "  %198 : Float(1:307200, 100:3072, 3072:1, requires_grad=1, device=cpu) = aten::mul(%194, %197), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/activations.py:38:0\n",
       "  %199 : Float(1:307200, 100:3072, 3072:1, requires_grad=1, device=cpu) = aten::tanh(%198), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/activations.py:38:0\n",
       "  %200 : Float(1:307200, 100:3072, 3072:1, requires_grad=1, device=cpu) = aten::add(%199, %85, %105), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/activations.py:38:0\n",
       "  %input.11 : Float(1:307200, 100:3072, 3072:1, requires_grad=1, device=cpu) = aten::mul(%193, %200), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/activations.py:38:0\n",
       "  %202 : Tensor = prim::GetAttr[name=\"bias\"](%117)\n",
       "  %203 : Tensor = prim::GetAttr[name=\"weight\"](%117)\n",
       "  %204 : Float(3072:1, 768:3072, requires_grad=1, device=cpu) = aten::t(%203), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1692:0\n",
       "  %output.6 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::matmul(%input.11, %204), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1692:0\n",
       "  %ffn_output.1 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::add_(%output.6, %202, %105), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1694:0\n",
       "  %input.12 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::add(%ffn_output.1, %186, %105), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:363:0\n",
       "  %208 : Tensor = prim::GetAttr[name=\"bias\"](%116)\n",
       "  %209 : Tensor = prim::GetAttr[name=\"weight\"](%116)\n",
       "  %210 : int[] = prim::ListConstruct(%100), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm\n",
       "  %input.13 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::layer_norm(%input.12, %210, %209, %208, %104, %103), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:2094:0\n",
       "  %212 : (Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu), Float(768:1, requires_grad=1, device=cpu)) = prim::TupleConstruct(%input.13, %187)\n",
       "  %213 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu), %214 : Float(768:1, requires_grad=1, device=cpu) = prim::TupleUnpack(%212)\n",
       "  %215 : (Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu), Float(768:1, requires_grad=1, device=cpu)) = prim::TupleConstruct(%213, %214)\n",
       "  %216 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu), %217 : Float(768:1, requires_grad=1, device=cpu) = prim::TupleUnpack(%215)\n",
       "  %218 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name=\"albert_layers\"](%107)\n",
       "  %219 : __torch__.transformers.modeling_albert.AlbertLayer = prim::GetAttr[name=\"0\"](%218)\n",
       "  %220 : __torch__.torch.nn.modules.normalization.___torch_mangle_2.LayerNorm = prim::GetAttr[name=\"full_layer_layer_norm\"](%219)\n",
       "  %221 : __torch__.torch.nn.modules.linear.___torch_mangle_11.Linear = prim::GetAttr[name=\"ffn_output\"](%219)\n",
       "  %222 : __torch__.torch.nn.modules.linear.___torch_mangle_10.Linear = prim::GetAttr[name=\"ffn\"](%219)\n",
       "  %223 : __torch__.transformers.modeling_albert.AlbertAttention = prim::GetAttr[name=\"attention\"](%219)\n",
       "  %224 : __torch__.torch.nn.modules.normalization.___torch_mangle_9.LayerNorm = prim::GetAttr[name=\"LayerNorm\"](%223)\n",
       "  %225 : __torch__.torch.nn.modules.linear.___torch_mangle_8.Linear = prim::GetAttr[name=\"dense\"](%223)\n",
       "  %226 : Tensor = prim::GetAttr[name=\"weight\"](%225)\n",
       "  %227 : __torch__.torch.nn.modules.linear.___torch_mangle_5.Linear = prim::GetAttr[name=\"value\"](%223)\n",
       "  %228 : __torch__.torch.nn.modules.linear.___torch_mangle_4.Linear = prim::GetAttr[name=\"key\"](%223)\n",
       "  %229 : __torch__.torch.nn.modules.linear.___torch_mangle_3.Linear = prim::GetAttr[name=\"query\"](%223)\n",
       "  %230 : Tensor = prim::GetAttr[name=\"bias\"](%229)\n",
       "  %231 : Tensor = prim::GetAttr[name=\"weight\"](%229)\n",
       "  %232 : Float(768:1, 768:768, requires_grad=1, device=cpu) = aten::t(%231), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.query # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1692:0\n",
       "  %output.7 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::matmul(%216, %232), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.query # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1692:0\n",
       "  %x.8 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::add_(%output.7, %230, %105), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.query # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1694:0\n",
       "  %235 : Tensor = prim::GetAttr[name=\"bias\"](%228)\n",
       "  %236 : Tensor = prim::GetAttr[name=\"weight\"](%228)\n",
       "  %237 : Float(768:1, 768:768, requires_grad=1, device=cpu) = aten::t(%236), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.key # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1692:0\n",
       "  %output.8 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::matmul(%216, %237), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.key # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1692:0\n",
       "  %x.10 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::add_(%output.8, %235, %105), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.key # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1694:0\n",
       "  %240 : Tensor = prim::GetAttr[name=\"bias\"](%227)\n",
       "  %241 : Tensor = prim::GetAttr[name=\"weight\"](%227)\n",
       "  %242 : Float(768:1, 768:768, requires_grad=1, device=cpu) = aten::t(%241), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.value # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1692:0\n",
       "  %output.9 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::matmul(%216, %242), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.value # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1692:0\n",
       "  %x.12 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::add_(%output.9, %240, %105), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.value # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1694:0\n",
       "  %245 : int = aten::size(%x.8, %89), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:271:0\n",
       "  %246 : int = aten::size(%x.8, %105), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:271:0\n",
       "  %247 : int[] = prim::ListConstruct(%245, %246, %90, %91), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention\n",
       "  %x.9 : Float(1:76800, 100:768, 12:64, 64:1, requires_grad=1, device=cpu) = aten::view(%x.8, %247), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:272:0\n",
       "  %249 : int[] = prim::ListConstruct(%89, %92, %105, %93), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention\n",
       "  %query_layer.2 : Float(1:76800, 12:64, 100:768, 64:1, requires_grad=1, device=cpu) = aten::permute(%x.9, %249), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:273:0\n",
       "  %251 : int = aten::size(%x.10, %89), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:271:0\n",
       "  %252 : int = aten::size(%x.10, %105), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:271:0\n",
       "  %253 : int[] = prim::ListConstruct(%251, %252, %90, %91), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention\n",
       "  %x.11 : Float(1:76800, 100:768, 12:64, 64:1, requires_grad=1, device=cpu) = aten::view(%x.10, %253), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:272:0\n",
       "  %255 : int[] = prim::ListConstruct(%89, %92, %105, %93), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention\n",
       "  %key_layer.2 : Float(1:76800, 12:64, 100:768, 64:1, requires_grad=1, device=cpu) = aten::permute(%x.11, %255), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:273:0\n",
       "  %257 : int = aten::size(%x.12, %89), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:271:0\n",
       "  %258 : int = aten::size(%x.12, %105), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:271:0\n",
       "  %259 : int[] = prim::ListConstruct(%257, %258, %90, %91), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention\n",
       "  %x.13 : Float(1:76800, 100:768, 12:64, 64:1, requires_grad=1, device=cpu) = aten::view(%x.12, %259), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:272:0\n",
       "  %261 : int[] = prim::ListConstruct(%89, %92, %105, %93), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention\n",
       "  %value_layer.2 : Float(1:76800, 12:64, 100:768, 64:1, requires_grad=1, device=cpu) = aten::permute(%x.13, %261), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:273:0\n",
       "  %263 : Float(1:76800, 12:64, 64:1, 100:768, requires_grad=1, device=cpu) = aten::transpose(%key_layer.2, %94, %95), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:303:0\n",
       "  %attention_scores.3 : Float(1:120000, 12:10000, 100:100, 100:1, requires_grad=1, device=cpu) = aten::matmul(%query_layer.2, %263), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:303:0\n",
       "  %attention_scores.4 : Float(1:120000, 12:10000, 100:100, 100:1, requires_grad=1, device=cpu) = aten::div(%attention_scores.3, %96), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:304:0\n",
       "  %input.14 : Float(1:120000, 12:10000, 100:100, 100:1, requires_grad=1, device=cpu) = aten::add(%attention_scores.4, %attention_mask, %105), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:307:0\n",
       "  %input.15 : Float(1:120000, 12:10000, 100:100, 100:1, requires_grad=1, device=cpu) = aten::softmax(%input.14, %94, %97), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1512:0\n",
       "  %attention_probs.2 : Float(1:120000, 12:10000, 100:100, 100:1, requires_grad=1, device=cpu) = aten::dropout(%input.15, %99, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.attention_dropout # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:983:0\n",
       "  %context_layer.2 : Float(1:76800, 12:6400, 100:64, 64:1, requires_grad=1, device=cpu) = aten::matmul(%attention_probs.2, %value_layer.2), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:320:0\n",
       "  %270 : int[] = prim::ListConstruct(%89, %92, %105, %93), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention\n",
       "  %271 : Float(1:76800, 100:64, 12:6400, 64:1, requires_grad=1, device=cpu) = aten::permute(%context_layer.2, %270), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:322:0\n",
       "  %272 : Float(1:76800, 100:768, 12:64, 64:1, requires_grad=1, device=cpu) = aten::contiguous(%271, %89), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:322:0\n",
       "  %273 : Float(768:1, 768:768, requires_grad=1, device=cpu) = aten::t(%226), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:326:0\n",
       "  %274 : int[] = prim::ListConstruct(%90, %91, %100), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention\n",
       "  %275 : Float(12:64, 64:1, 768:768, requires_grad=1, device=cpu) = aten::view(%273, %274), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:326:0\n",
       "  %276 : Float(12:64, 64:1, 768:768, requires_grad=1, device=cpu) = aten::to(%275, %101, %98, %98, %97), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:326:0\n",
       "  %b.2 : Float(768:1, requires_grad=1, device=cpu) = aten::to(%217, %101, %98, %98, %97), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:330:0\n",
       "  %278 : Tensor[] = prim::ListConstruct(%272, %276), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention\n",
       "  %279 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::einsum(%102, %278), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/functional.py:344:0\n",
       "  %input.16 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::add(%279, %b.2, %105), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:332:0\n",
       "  %projected_context_layer.2 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::dropout(%input.16, %99, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.output_dropout # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:983:0\n",
       "  %input.17 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::add(%216, %projected_context_layer.2, %105), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:334:0\n",
       "  %283 : Tensor = prim::GetAttr[name=\"bias\"](%224)\n",
       "  %284 : Tensor = prim::GetAttr[name=\"weight\"](%224)\n",
       "  %285 : int[] = prim::ListConstruct(%100), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm\n",
       "  %input_tensor.2 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::layer_norm(%input.17, %285, %284, %283, %104, %103), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:2094:0\n",
       "  %287 : (Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu), Float(768:1, requires_grad=1, device=cpu)) = prim::TupleConstruct(%input_tensor.2, %b.2)\n",
       "  %288 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu), %289 : Float(768:1, requires_grad=1, device=cpu) = prim::TupleUnpack(%287)\n",
       "  %290 : Tensor = prim::GetAttr[name=\"bias\"](%222)\n",
       "  %291 : Tensor = prim::GetAttr[name=\"weight\"](%222)\n",
       "  %292 : Float(768:1, 3072:768, requires_grad=1, device=cpu) = aten::t(%291), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.ffn # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1692:0\n",
       "  %output.10 : Float(1:307200, 100:3072, 3072:1, requires_grad=1, device=cpu) = aten::matmul(%288, %292), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.ffn # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1692:0\n",
       "  %x.14 : Float(1:307200, 100:3072, 3072:1, requires_grad=1, device=cpu) = aten::add_(%output.10, %290, %105), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.ffn # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1694:0\n",
       "  %295 : Float(1:307200, 100:3072, 3072:1, requires_grad=1, device=cpu) = aten::mul(%x.14, %88), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/activations.py:38:0\n",
       "  %296 : Float(1:307200, 100:3072, 3072:1, requires_grad=1, device=cpu) = aten::mul(%x.14, %87), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/activations.py:38:0\n",
       "  %297 : Float(1:307200, 100:3072, 3072:1, requires_grad=1, device=cpu) = aten::mul(%x.14, %86), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/activations.py:38:0\n",
       "  %298 : Float(1:307200, 100:3072, 3072:1, requires_grad=1, device=cpu) = aten::mul(%297, %x.14), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/activations.py:38:0\n",
       "  %299 : Float(1:307200, 100:3072, 3072:1, requires_grad=1, device=cpu) = aten::add(%298, %85, %105), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/activations.py:38:0\n",
       "  %300 : Float(1:307200, 100:3072, 3072:1, requires_grad=1, device=cpu) = aten::mul(%296, %299), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/activations.py:38:0\n",
       "  %301 : Float(1:307200, 100:3072, 3072:1, requires_grad=1, device=cpu) = aten::tanh(%300), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/activations.py:38:0\n",
       "  %302 : Float(1:307200, 100:3072, 3072:1, requires_grad=1, device=cpu) = aten::add(%301, %85, %105), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/activations.py:38:0\n",
       "  %input.18 : Float(1:307200, 100:3072, 3072:1, requires_grad=1, device=cpu) = aten::mul(%295, %302), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/activations.py:38:0\n",
       "  %304 : Tensor = prim::GetAttr[name=\"bias\"](%221)\n",
       "  %305 : Tensor = prim::GetAttr[name=\"weight\"](%221)\n",
       "  %306 : Float(3072:1, 768:3072, requires_grad=1, device=cpu) = aten::t(%305), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1692:0\n",
       "  %output.11 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::matmul(%input.18, %306), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1692:0\n",
       "  %ffn_output.2 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::add_(%output.11, %304, %105), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1694:0\n",
       "  %input.19 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::add(%ffn_output.2, %288, %105), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:363:0\n",
       "  %310 : Tensor = prim::GetAttr[name=\"bias\"](%220)\n",
       "  %311 : Tensor = prim::GetAttr[name=\"weight\"](%220)\n",
       "  %312 : int[] = prim::ListConstruct(%100), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm\n",
       "  %input.20 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::layer_norm(%input.19, %312, %311, %310, %104, %103), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:2094:0\n",
       "  %314 : (Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu), Float(768:1, requires_grad=1, device=cpu)) = prim::TupleConstruct(%input.20, %289)\n",
       "  %315 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu), %316 : Float(768:1, requires_grad=1, device=cpu) = prim::TupleUnpack(%314)\n",
       "  %317 : (Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu), Float(768:1, requires_grad=1, device=cpu)) = prim::TupleConstruct(%315, %316)\n",
       "  %318 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu), %319 : Float(768:1, requires_grad=1, device=cpu) = prim::TupleUnpack(%317)\n",
       "  %320 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name=\"albert_layers\"](%107)\n",
       "  %321 : __torch__.transformers.modeling_albert.AlbertLayer = prim::GetAttr[name=\"0\"](%320)\n",
       "  %322 : __torch__.torch.nn.modules.normalization.___torch_mangle_2.LayerNorm = prim::GetAttr[name=\"full_layer_layer_norm\"](%321)\n",
       "  %323 : __torch__.torch.nn.modules.linear.___torch_mangle_11.Linear = prim::GetAttr[name=\"ffn_output\"](%321)\n",
       "  %324 : __torch__.torch.nn.modules.linear.___torch_mangle_10.Linear = prim::GetAttr[name=\"ffn\"](%321)\n",
       "  %325 : __torch__.transformers.modeling_albert.AlbertAttention = prim::GetAttr[name=\"attention\"](%321)\n",
       "  %326 : __torch__.torch.nn.modules.normalization.___torch_mangle_9.LayerNorm = prim::GetAttr[name=\"LayerNorm\"](%325)\n",
       "  %327 : __torch__.torch.nn.modules.linear.___torch_mangle_8.Linear = prim::GetAttr[name=\"dense\"](%325)\n",
       "  %328 : Tensor = prim::GetAttr[name=\"weight\"](%327)\n",
       "  %329 : __torch__.torch.nn.modules.linear.___torch_mangle_5.Linear = prim::GetAttr[name=\"value\"](%325)\n",
       "  %330 : __torch__.torch.nn.modules.linear.___torch_mangle_4.Linear = prim::GetAttr[name=\"key\"](%325)\n",
       "  %331 : __torch__.torch.nn.modules.linear.___torch_mangle_3.Linear = prim::GetAttr[name=\"query\"](%325)\n",
       "  %332 : Tensor = prim::GetAttr[name=\"bias\"](%331)\n",
       "  %333 : Tensor = prim::GetAttr[name=\"weight\"](%331)\n",
       "  %334 : Float(768:1, 768:768, requires_grad=1, device=cpu) = aten::t(%333), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.query # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1692:0\n",
       "  %output.12 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::matmul(%318, %334), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.query # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1692:0\n",
       "  %x.15 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::add_(%output.12, %332, %105), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.query # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1694:0\n",
       "  %337 : Tensor = prim::GetAttr[name=\"bias\"](%330)\n",
       "  %338 : Tensor = prim::GetAttr[name=\"weight\"](%330)\n",
       "  %339 : Float(768:1, 768:768, requires_grad=1, device=cpu) = aten::t(%338), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.key # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1692:0\n",
       "  %output.13 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::matmul(%318, %339), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.key # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1692:0\n",
       "  %x.17 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::add_(%output.13, %337, %105), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.key # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1694:0\n",
       "  %342 : Tensor = prim::GetAttr[name=\"bias\"](%329)\n",
       "  %343 : Tensor = prim::GetAttr[name=\"weight\"](%329)\n",
       "  %344 : Float(768:1, 768:768, requires_grad=1, device=cpu) = aten::t(%343), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.value # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1692:0\n",
       "  %output.14 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::matmul(%318, %344), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.value # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1692:0\n",
       "  %x.19 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::add_(%output.14, %342, %105), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.value # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1694:0\n",
       "  %347 : int = aten::size(%x.15, %89), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:271:0\n",
       "  %348 : int = aten::size(%x.15, %105), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:271:0\n",
       "  %349 : int[] = prim::ListConstruct(%347, %348, %90, %91), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention\n",
       "  %x.16 : Float(1:76800, 100:768, 12:64, 64:1, requires_grad=1, device=cpu) = aten::view(%x.15, %349), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:272:0\n",
       "  %351 : int[] = prim::ListConstruct(%89, %92, %105, %93), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention\n",
       "  %query_layer.3 : Float(1:76800, 12:64, 100:768, 64:1, requires_grad=1, device=cpu) = aten::permute(%x.16, %351), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:273:0\n",
       "  %353 : int = aten::size(%x.17, %89), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:271:0\n",
       "  %354 : int = aten::size(%x.17, %105), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:271:0\n",
       "  %355 : int[] = prim::ListConstruct(%353, %354, %90, %91), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention\n",
       "  %x.18 : Float(1:76800, 100:768, 12:64, 64:1, requires_grad=1, device=cpu) = aten::view(%x.17, %355), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:272:0\n",
       "  %357 : int[] = prim::ListConstruct(%89, %92, %105, %93), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention\n",
       "  %key_layer.3 : Float(1:76800, 12:64, 100:768, 64:1, requires_grad=1, device=cpu) = aten::permute(%x.18, %357), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:273:0\n",
       "  %359 : int = aten::size(%x.19, %89), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:271:0\n",
       "  %360 : int = aten::size(%x.19, %105), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:271:0\n",
       "  %361 : int[] = prim::ListConstruct(%359, %360, %90, %91), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention\n",
       "  %x.20 : Float(1:76800, 100:768, 12:64, 64:1, requires_grad=1, device=cpu) = aten::view(%x.19, %361), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:272:0\n",
       "  %363 : int[] = prim::ListConstruct(%89, %92, %105, %93), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention\n",
       "  %value_layer.3 : Float(1:76800, 12:64, 100:768, 64:1, requires_grad=1, device=cpu) = aten::permute(%x.20, %363), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:273:0\n",
       "  %365 : Float(1:76800, 12:64, 64:1, 100:768, requires_grad=1, device=cpu) = aten::transpose(%key_layer.3, %94, %95), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:303:0\n",
       "  %attention_scores.5 : Float(1:120000, 12:10000, 100:100, 100:1, requires_grad=1, device=cpu) = aten::matmul(%query_layer.3, %365), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:303:0\n",
       "  %attention_scores.6 : Float(1:120000, 12:10000, 100:100, 100:1, requires_grad=1, device=cpu) = aten::div(%attention_scores.5, %96), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:304:0\n",
       "  %input.21 : Float(1:120000, 12:10000, 100:100, 100:1, requires_grad=1, device=cpu) = aten::add(%attention_scores.6, %attention_mask, %105), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:307:0\n",
       "  %input.22 : Float(1:120000, 12:10000, 100:100, 100:1, requires_grad=1, device=cpu) = aten::softmax(%input.21, %94, %97), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1512:0\n",
       "  %attention_probs.3 : Float(1:120000, 12:10000, 100:100, 100:1, requires_grad=1, device=cpu) = aten::dropout(%input.22, %99, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.attention_dropout # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:983:0\n",
       "  %context_layer.3 : Float(1:76800, 12:6400, 100:64, 64:1, requires_grad=1, device=cpu) = aten::matmul(%attention_probs.3, %value_layer.3), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:320:0\n",
       "  %372 : int[] = prim::ListConstruct(%89, %92, %105, %93), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention\n",
       "  %373 : Float(1:76800, 100:64, 12:6400, 64:1, requires_grad=1, device=cpu) = aten::permute(%context_layer.3, %372), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:322:0\n",
       "  %374 : Float(1:76800, 100:768, 12:64, 64:1, requires_grad=1, device=cpu) = aten::contiguous(%373, %89), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:322:0\n",
       "  %375 : Float(768:1, 768:768, requires_grad=1, device=cpu) = aten::t(%328), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:326:0\n",
       "  %376 : int[] = prim::ListConstruct(%90, %91, %100), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention\n",
       "  %377 : Float(12:64, 64:1, 768:768, requires_grad=1, device=cpu) = aten::view(%375, %376), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:326:0\n",
       "  %378 : Float(12:64, 64:1, 768:768, requires_grad=1, device=cpu) = aten::to(%377, %101, %98, %98, %97), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:326:0\n",
       "  %b.3 : Float(768:1, requires_grad=1, device=cpu) = aten::to(%319, %101, %98, %98, %97), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:330:0\n",
       "  %380 : Tensor[] = prim::ListConstruct(%374, %378), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention\n",
       "  %381 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::einsum(%102, %380), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/functional.py:344:0\n",
       "  %input.23 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::add(%381, %b.3, %105), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:332:0\n",
       "  %projected_context_layer.3 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::dropout(%input.23, %99, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.output_dropout # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:983:0\n",
       "  %input.24 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::add(%318, %projected_context_layer.3, %105), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:334:0\n",
       "  %385 : Tensor = prim::GetAttr[name=\"bias\"](%326)\n",
       "  %386 : Tensor = prim::GetAttr[name=\"weight\"](%326)\n",
       "  %387 : int[] = prim::ListConstruct(%100), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm\n",
       "  %input_tensor.3 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::layer_norm(%input.24, %387, %386, %385, %104, %103), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:2094:0\n",
       "  %389 : (Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu), Float(768:1, requires_grad=1, device=cpu)) = prim::TupleConstruct(%input_tensor.3, %b.3)\n",
       "  %390 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu), %391 : Float(768:1, requires_grad=1, device=cpu) = prim::TupleUnpack(%389)\n",
       "  %392 : Tensor = prim::GetAttr[name=\"bias\"](%324)\n",
       "  %393 : Tensor = prim::GetAttr[name=\"weight\"](%324)\n",
       "  %394 : Float(768:1, 3072:768, requires_grad=1, device=cpu) = aten::t(%393), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.ffn # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1692:0\n",
       "  %output.15 : Float(1:307200, 100:3072, 3072:1, requires_grad=1, device=cpu) = aten::matmul(%390, %394), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.ffn # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1692:0\n",
       "  %x.21 : Float(1:307200, 100:3072, 3072:1, requires_grad=1, device=cpu) = aten::add_(%output.15, %392, %105), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.ffn # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1694:0\n",
       "  %397 : Float(1:307200, 100:3072, 3072:1, requires_grad=1, device=cpu) = aten::mul(%x.21, %88), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/activations.py:38:0\n",
       "  %398 : Float(1:307200, 100:3072, 3072:1, requires_grad=1, device=cpu) = aten::mul(%x.21, %87), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/activations.py:38:0\n",
       "  %399 : Float(1:307200, 100:3072, 3072:1, requires_grad=1, device=cpu) = aten::mul(%x.21, %86), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/activations.py:38:0\n",
       "  %400 : Float(1:307200, 100:3072, 3072:1, requires_grad=1, device=cpu) = aten::mul(%399, %x.21), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/activations.py:38:0\n",
       "  %401 : Float(1:307200, 100:3072, 3072:1, requires_grad=1, device=cpu) = aten::add(%400, %85, %105), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/activations.py:38:0\n",
       "  %402 : Float(1:307200, 100:3072, 3072:1, requires_grad=1, device=cpu) = aten::mul(%398, %401), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/activations.py:38:0\n",
       "  %403 : Float(1:307200, 100:3072, 3072:1, requires_grad=1, device=cpu) = aten::tanh(%402), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/activations.py:38:0\n",
       "  %404 : Float(1:307200, 100:3072, 3072:1, requires_grad=1, device=cpu) = aten::add(%403, %85, %105), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/activations.py:38:0\n",
       "  %input.25 : Float(1:307200, 100:3072, 3072:1, requires_grad=1, device=cpu) = aten::mul(%397, %404), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/activations.py:38:0\n",
       "  %406 : Tensor = prim::GetAttr[name=\"bias\"](%323)\n",
       "  %407 : Tensor = prim::GetAttr[name=\"weight\"](%323)\n",
       "  %408 : Float(3072:1, 768:3072, requires_grad=1, device=cpu) = aten::t(%407), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1692:0\n",
       "  %output.16 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::matmul(%input.25, %408), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1692:0\n",
       "  %ffn_output.3 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::add_(%output.16, %406, %105), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1694:0\n",
       "  %input.26 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::add(%ffn_output.3, %390, %105), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:363:0\n",
       "  %412 : Tensor = prim::GetAttr[name=\"bias\"](%322)\n",
       "  %413 : Tensor = prim::GetAttr[name=\"weight\"](%322)\n",
       "  %414 : int[] = prim::ListConstruct(%100), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm\n",
       "  %input.27 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::layer_norm(%input.26, %414, %413, %412, %104, %103), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:2094:0\n",
       "  %416 : (Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu), Float(768:1, requires_grad=1, device=cpu)) = prim::TupleConstruct(%input.27, %391)\n",
       "  %417 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu), %418 : Float(768:1, requires_grad=1, device=cpu) = prim::TupleUnpack(%416)\n",
       "  %419 : (Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu), Float(768:1, requires_grad=1, device=cpu)) = prim::TupleConstruct(%417, %418)\n",
       "  %420 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu), %421 : Float(768:1, requires_grad=1, device=cpu) = prim::TupleUnpack(%419)\n",
       "  %422 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name=\"albert_layers\"](%107)\n",
       "  %423 : __torch__.transformers.modeling_albert.AlbertLayer = prim::GetAttr[name=\"0\"](%422)\n",
       "  %424 : __torch__.torch.nn.modules.normalization.___torch_mangle_2.LayerNorm = prim::GetAttr[name=\"full_layer_layer_norm\"](%423)\n",
       "  %425 : __torch__.torch.nn.modules.linear.___torch_mangle_11.Linear = prim::GetAttr[name=\"ffn_output\"](%423)\n",
       "  %426 : __torch__.torch.nn.modules.linear.___torch_mangle_10.Linear = prim::GetAttr[name=\"ffn\"](%423)\n",
       "  %427 : __torch__.transformers.modeling_albert.AlbertAttention = prim::GetAttr[name=\"attention\"](%423)\n",
       "  %428 : __torch__.torch.nn.modules.normalization.___torch_mangle_9.LayerNorm = prim::GetAttr[name=\"LayerNorm\"](%427)\n",
       "  %429 : __torch__.torch.nn.modules.linear.___torch_mangle_8.Linear = prim::GetAttr[name=\"dense\"](%427)\n",
       "  %430 : Tensor = prim::GetAttr[name=\"weight\"](%429)\n",
       "  %431 : __torch__.torch.nn.modules.linear.___torch_mangle_5.Linear = prim::GetAttr[name=\"value\"](%427)\n",
       "  %432 : __torch__.torch.nn.modules.linear.___torch_mangle_4.Linear = prim::GetAttr[name=\"key\"](%427)\n",
       "  %433 : __torch__.torch.nn.modules.linear.___torch_mangle_3.Linear = prim::GetAttr[name=\"query\"](%427)\n",
       "  %434 : Tensor = prim::GetAttr[name=\"bias\"](%433)\n",
       "  %435 : Tensor = prim::GetAttr[name=\"weight\"](%433)\n",
       "  %436 : Float(768:1, 768:768, requires_grad=1, device=cpu) = aten::t(%435), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.query # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1692:0\n",
       "  %output.17 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::matmul(%420, %436), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.query # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1692:0\n",
       "  %x.22 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::add_(%output.17, %434, %105), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.query # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1694:0\n",
       "  %439 : Tensor = prim::GetAttr[name=\"bias\"](%432)\n",
       "  %440 : Tensor = prim::GetAttr[name=\"weight\"](%432)\n",
       "  %441 : Float(768:1, 768:768, requires_grad=1, device=cpu) = aten::t(%440), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.key # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1692:0\n",
       "  %output.18 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::matmul(%420, %441), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.key # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1692:0\n",
       "  %x.24 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::add_(%output.18, %439, %105), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.key # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1694:0\n",
       "  %444 : Tensor = prim::GetAttr[name=\"bias\"](%431)\n",
       "  %445 : Tensor = prim::GetAttr[name=\"weight\"](%431)\n",
       "  %446 : Float(768:1, 768:768, requires_grad=1, device=cpu) = aten::t(%445), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.value # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1692:0\n",
       "  %output.19 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::matmul(%420, %446), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.value # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1692:0\n",
       "  %x.26 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::add_(%output.19, %444, %105), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.value # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1694:0\n",
       "  %449 : int = aten::size(%x.22, %89), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:271:0\n",
       "  %450 : int = aten::size(%x.22, %105), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:271:0\n",
       "  %451 : int[] = prim::ListConstruct(%449, %450, %90, %91), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention\n",
       "  %x.23 : Float(1:76800, 100:768, 12:64, 64:1, requires_grad=1, device=cpu) = aten::view(%x.22, %451), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:272:0\n",
       "  %453 : int[] = prim::ListConstruct(%89, %92, %105, %93), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention\n",
       "  %query_layer.4 : Float(1:76800, 12:64, 100:768, 64:1, requires_grad=1, device=cpu) = aten::permute(%x.23, %453), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:273:0\n",
       "  %455 : int = aten::size(%x.24, %89), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:271:0\n",
       "  %456 : int = aten::size(%x.24, %105), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:271:0\n",
       "  %457 : int[] = prim::ListConstruct(%455, %456, %90, %91), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention\n",
       "  %x.25 : Float(1:76800, 100:768, 12:64, 64:1, requires_grad=1, device=cpu) = aten::view(%x.24, %457), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:272:0\n",
       "  %459 : int[] = prim::ListConstruct(%89, %92, %105, %93), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention\n",
       "  %key_layer.4 : Float(1:76800, 12:64, 100:768, 64:1, requires_grad=1, device=cpu) = aten::permute(%x.25, %459), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:273:0\n",
       "  %461 : int = aten::size(%x.26, %89), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:271:0\n",
       "  %462 : int = aten::size(%x.26, %105), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:271:0\n",
       "  %463 : int[] = prim::ListConstruct(%461, %462, %90, %91), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention\n",
       "  %x.27 : Float(1:76800, 100:768, 12:64, 64:1, requires_grad=1, device=cpu) = aten::view(%x.26, %463), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:272:0\n",
       "  %465 : int[] = prim::ListConstruct(%89, %92, %105, %93), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention\n",
       "  %value_layer.4 : Float(1:76800, 12:64, 100:768, 64:1, requires_grad=1, device=cpu) = aten::permute(%x.27, %465), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:273:0\n",
       "  %467 : Float(1:76800, 12:64, 64:1, 100:768, requires_grad=1, device=cpu) = aten::transpose(%key_layer.4, %94, %95), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:303:0\n",
       "  %attention_scores.7 : Float(1:120000, 12:10000, 100:100, 100:1, requires_grad=1, device=cpu) = aten::matmul(%query_layer.4, %467), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:303:0\n",
       "  %attention_scores.8 : Float(1:120000, 12:10000, 100:100, 100:1, requires_grad=1, device=cpu) = aten::div(%attention_scores.7, %96), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:304:0\n",
       "  %input.28 : Float(1:120000, 12:10000, 100:100, 100:1, requires_grad=1, device=cpu) = aten::add(%attention_scores.8, %attention_mask, %105), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:307:0\n",
       "  %input.29 : Float(1:120000, 12:10000, 100:100, 100:1, requires_grad=1, device=cpu) = aten::softmax(%input.28, %94, %97), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1512:0\n",
       "  %attention_probs.4 : Float(1:120000, 12:10000, 100:100, 100:1, requires_grad=1, device=cpu) = aten::dropout(%input.29, %99, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.attention_dropout # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:983:0\n",
       "  %context_layer.4 : Float(1:76800, 12:6400, 100:64, 64:1, requires_grad=1, device=cpu) = aten::matmul(%attention_probs.4, %value_layer.4), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:320:0\n",
       "  %474 : int[] = prim::ListConstruct(%89, %92, %105, %93), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention\n",
       "  %475 : Float(1:76800, 100:64, 12:6400, 64:1, requires_grad=1, device=cpu) = aten::permute(%context_layer.4, %474), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:322:0\n",
       "  %476 : Float(1:76800, 100:768, 12:64, 64:1, requires_grad=1, device=cpu) = aten::contiguous(%475, %89), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:322:0\n",
       "  %477 : Float(768:1, 768:768, requires_grad=1, device=cpu) = aten::t(%430), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:326:0\n",
       "  %478 : int[] = prim::ListConstruct(%90, %91, %100), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention\n",
       "  %479 : Float(12:64, 64:1, 768:768, requires_grad=1, device=cpu) = aten::view(%477, %478), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:326:0\n",
       "  %480 : Float(12:64, 64:1, 768:768, requires_grad=1, device=cpu) = aten::to(%479, %101, %98, %98, %97), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:326:0\n",
       "  %b.4 : Float(768:1, requires_grad=1, device=cpu) = aten::to(%421, %101, %98, %98, %97), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:330:0\n",
       "  %482 : Tensor[] = prim::ListConstruct(%476, %480), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention\n",
       "  %483 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::einsum(%102, %482), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/functional.py:344:0\n",
       "  %input.30 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::add(%483, %b.4, %105), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:332:0\n",
       "  %projected_context_layer.4 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::dropout(%input.30, %99, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.output_dropout # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:983:0\n",
       "  %input.31 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::add(%420, %projected_context_layer.4, %105), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:334:0\n",
       "  %487 : Tensor = prim::GetAttr[name=\"bias\"](%428)\n",
       "  %488 : Tensor = prim::GetAttr[name=\"weight\"](%428)\n",
       "  %489 : int[] = prim::ListConstruct(%100), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm\n",
       "  %input_tensor.4 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::layer_norm(%input.31, %489, %488, %487, %104, %103), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:2094:0\n",
       "  %491 : (Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu), Float(768:1, requires_grad=1, device=cpu)) = prim::TupleConstruct(%input_tensor.4, %b.4)\n",
       "  %492 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu), %493 : Float(768:1, requires_grad=1, device=cpu) = prim::TupleUnpack(%491)\n",
       "  %494 : Tensor = prim::GetAttr[name=\"bias\"](%426)\n",
       "  %495 : Tensor = prim::GetAttr[name=\"weight\"](%426)\n",
       "  %496 : Float(768:1, 3072:768, requires_grad=1, device=cpu) = aten::t(%495), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.ffn # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1692:0\n",
       "  %output.20 : Float(1:307200, 100:3072, 3072:1, requires_grad=1, device=cpu) = aten::matmul(%492, %496), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.ffn # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1692:0\n",
       "  %x.28 : Float(1:307200, 100:3072, 3072:1, requires_grad=1, device=cpu) = aten::add_(%output.20, %494, %105), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.ffn # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1694:0\n",
       "  %499 : Float(1:307200, 100:3072, 3072:1, requires_grad=1, device=cpu) = aten::mul(%x.28, %88), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/activations.py:38:0\n",
       "  %500 : Float(1:307200, 100:3072, 3072:1, requires_grad=1, device=cpu) = aten::mul(%x.28, %87), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/activations.py:38:0\n",
       "  %501 : Float(1:307200, 100:3072, 3072:1, requires_grad=1, device=cpu) = aten::mul(%x.28, %86), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/activations.py:38:0\n",
       "  %502 : Float(1:307200, 100:3072, 3072:1, requires_grad=1, device=cpu) = aten::mul(%501, %x.28), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/activations.py:38:0\n",
       "  %503 : Float(1:307200, 100:3072, 3072:1, requires_grad=1, device=cpu) = aten::add(%502, %85, %105), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/activations.py:38:0\n",
       "  %504 : Float(1:307200, 100:3072, 3072:1, requires_grad=1, device=cpu) = aten::mul(%500, %503), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/activations.py:38:0\n",
       "  %505 : Float(1:307200, 100:3072, 3072:1, requires_grad=1, device=cpu) = aten::tanh(%504), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/activations.py:38:0\n",
       "  %506 : Float(1:307200, 100:3072, 3072:1, requires_grad=1, device=cpu) = aten::add(%505, %85, %105), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/activations.py:38:0\n",
       "  %input.32 : Float(1:307200, 100:3072, 3072:1, requires_grad=1, device=cpu) = aten::mul(%499, %506), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/activations.py:38:0\n",
       "  %508 : Tensor = prim::GetAttr[name=\"bias\"](%425)\n",
       "  %509 : Tensor = prim::GetAttr[name=\"weight\"](%425)\n",
       "  %510 : Float(3072:1, 768:3072, requires_grad=1, device=cpu) = aten::t(%509), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1692:0\n",
       "  %output.21 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::matmul(%input.32, %510), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1692:0\n",
       "  %ffn_output.4 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::add_(%output.21, %508, %105), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1694:0\n",
       "  %input.33 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::add(%ffn_output.4, %492, %105), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:363:0\n",
       "  %514 : Tensor = prim::GetAttr[name=\"bias\"](%424)\n",
       "  %515 : Tensor = prim::GetAttr[name=\"weight\"](%424)\n",
       "  %516 : int[] = prim::ListConstruct(%100), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm\n",
       "  %input.34 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::layer_norm(%input.33, %516, %515, %514, %104, %103), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:2094:0\n",
       "  %518 : (Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu), Float(768:1, requires_grad=1, device=cpu)) = prim::TupleConstruct(%input.34, %493)\n",
       "  %519 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu), %520 : Float(768:1, requires_grad=1, device=cpu) = prim::TupleUnpack(%518)\n",
       "  %521 : (Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu), Float(768:1, requires_grad=1, device=cpu)) = prim::TupleConstruct(%519, %520)\n",
       "  %522 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu), %523 : Float(768:1, requires_grad=1, device=cpu) = prim::TupleUnpack(%521)\n",
       "  %524 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name=\"albert_layers\"](%107)\n",
       "  %525 : __torch__.transformers.modeling_albert.AlbertLayer = prim::GetAttr[name=\"0\"](%524)\n",
       "  %526 : __torch__.torch.nn.modules.normalization.___torch_mangle_2.LayerNorm = prim::GetAttr[name=\"full_layer_layer_norm\"](%525)\n",
       "  %527 : __torch__.torch.nn.modules.linear.___torch_mangle_11.Linear = prim::GetAttr[name=\"ffn_output\"](%525)\n",
       "  %528 : __torch__.torch.nn.modules.linear.___torch_mangle_10.Linear = prim::GetAttr[name=\"ffn\"](%525)\n",
       "  %529 : __torch__.transformers.modeling_albert.AlbertAttention = prim::GetAttr[name=\"attention\"](%525)\n",
       "  %530 : __torch__.torch.nn.modules.normalization.___torch_mangle_9.LayerNorm = prim::GetAttr[name=\"LayerNorm\"](%529)\n",
       "  %531 : __torch__.torch.nn.modules.linear.___torch_mangle_8.Linear = prim::GetAttr[name=\"dense\"](%529)\n",
       "  %532 : Tensor = prim::GetAttr[name=\"weight\"](%531)\n",
       "  %533 : __torch__.torch.nn.modules.linear.___torch_mangle_5.Linear = prim::GetAttr[name=\"value\"](%529)\n",
       "  %534 : __torch__.torch.nn.modules.linear.___torch_mangle_4.Linear = prim::GetAttr[name=\"key\"](%529)\n",
       "  %535 : __torch__.torch.nn.modules.linear.___torch_mangle_3.Linear = prim::GetAttr[name=\"query\"](%529)\n",
       "  %536 : Tensor = prim::GetAttr[name=\"bias\"](%535)\n",
       "  %537 : Tensor = prim::GetAttr[name=\"weight\"](%535)\n",
       "  %538 : Float(768:1, 768:768, requires_grad=1, device=cpu) = aten::t(%537), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.query # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1692:0\n",
       "  %output.22 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::matmul(%522, %538), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.query # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1692:0\n",
       "  %x.29 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::add_(%output.22, %536, %105), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.query # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1694:0\n",
       "  %541 : Tensor = prim::GetAttr[name=\"bias\"](%534)\n",
       "  %542 : Tensor = prim::GetAttr[name=\"weight\"](%534)\n",
       "  %543 : Float(768:1, 768:768, requires_grad=1, device=cpu) = aten::t(%542), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.key # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1692:0\n",
       "  %output.23 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::matmul(%522, %543), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.key # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1692:0\n",
       "  %x.31 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::add_(%output.23, %541, %105), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.key # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1694:0\n",
       "  %546 : Tensor = prim::GetAttr[name=\"bias\"](%533)\n",
       "  %547 : Tensor = prim::GetAttr[name=\"weight\"](%533)\n",
       "  %548 : Float(768:1, 768:768, requires_grad=1, device=cpu) = aten::t(%547), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.value # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1692:0\n",
       "  %output.24 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::matmul(%522, %548), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.value # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1692:0\n",
       "  %x.33 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::add_(%output.24, %546, %105), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.value # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1694:0\n",
       "  %551 : int = aten::size(%x.29, %89), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:271:0\n",
       "  %552 : int = aten::size(%x.29, %105), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:271:0\n",
       "  %553 : int[] = prim::ListConstruct(%551, %552, %90, %91), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention\n",
       "  %x.30 : Float(1:76800, 100:768, 12:64, 64:1, requires_grad=1, device=cpu) = aten::view(%x.29, %553), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:272:0\n",
       "  %555 : int[] = prim::ListConstruct(%89, %92, %105, %93), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention\n",
       "  %query_layer.5 : Float(1:76800, 12:64, 100:768, 64:1, requires_grad=1, device=cpu) = aten::permute(%x.30, %555), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:273:0\n",
       "  %557 : int = aten::size(%x.31, %89), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:271:0\n",
       "  %558 : int = aten::size(%x.31, %105), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:271:0\n",
       "  %559 : int[] = prim::ListConstruct(%557, %558, %90, %91), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention\n",
       "  %x.32 : Float(1:76800, 100:768, 12:64, 64:1, requires_grad=1, device=cpu) = aten::view(%x.31, %559), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:272:0\n",
       "  %561 : int[] = prim::ListConstruct(%89, %92, %105, %93), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention\n",
       "  %key_layer.5 : Float(1:76800, 12:64, 100:768, 64:1, requires_grad=1, device=cpu) = aten::permute(%x.32, %561), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:273:0\n",
       "  %563 : int = aten::size(%x.33, %89), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:271:0\n",
       "  %564 : int = aten::size(%x.33, %105), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:271:0\n",
       "  %565 : int[] = prim::ListConstruct(%563, %564, %90, %91), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention\n",
       "  %x.34 : Float(1:76800, 100:768, 12:64, 64:1, requires_grad=1, device=cpu) = aten::view(%x.33, %565), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:272:0\n",
       "  %567 : int[] = prim::ListConstruct(%89, %92, %105, %93), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention\n",
       "  %value_layer.5 : Float(1:76800, 12:64, 100:768, 64:1, requires_grad=1, device=cpu) = aten::permute(%x.34, %567), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:273:0\n",
       "  %569 : Float(1:76800, 12:64, 64:1, 100:768, requires_grad=1, device=cpu) = aten::transpose(%key_layer.5, %94, %95), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:303:0\n",
       "  %attention_scores.9 : Float(1:120000, 12:10000, 100:100, 100:1, requires_grad=1, device=cpu) = aten::matmul(%query_layer.5, %569), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:303:0\n",
       "  %attention_scores.10 : Float(1:120000, 12:10000, 100:100, 100:1, requires_grad=1, device=cpu) = aten::div(%attention_scores.9, %96), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:304:0\n",
       "  %input.35 : Float(1:120000, 12:10000, 100:100, 100:1, requires_grad=1, device=cpu) = aten::add(%attention_scores.10, %attention_mask, %105), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:307:0\n",
       "  %input.36 : Float(1:120000, 12:10000, 100:100, 100:1, requires_grad=1, device=cpu) = aten::softmax(%input.35, %94, %97), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1512:0\n",
       "  %attention_probs.5 : Float(1:120000, 12:10000, 100:100, 100:1, requires_grad=1, device=cpu) = aten::dropout(%input.36, %99, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.attention_dropout # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:983:0\n",
       "  %context_layer.5 : Float(1:76800, 12:6400, 100:64, 64:1, requires_grad=1, device=cpu) = aten::matmul(%attention_probs.5, %value_layer.5), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:320:0\n",
       "  %576 : int[] = prim::ListConstruct(%89, %92, %105, %93), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention\n",
       "  %577 : Float(1:76800, 100:64, 12:6400, 64:1, requires_grad=1, device=cpu) = aten::permute(%context_layer.5, %576), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:322:0\n",
       "  %578 : Float(1:76800, 100:768, 12:64, 64:1, requires_grad=1, device=cpu) = aten::contiguous(%577, %89), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:322:0\n",
       "  %579 : Float(768:1, 768:768, requires_grad=1, device=cpu) = aten::t(%532), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:326:0\n",
       "  %580 : int[] = prim::ListConstruct(%90, %91, %100), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention\n",
       "  %581 : Float(12:64, 64:1, 768:768, requires_grad=1, device=cpu) = aten::view(%579, %580), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:326:0\n",
       "  %582 : Float(12:64, 64:1, 768:768, requires_grad=1, device=cpu) = aten::to(%581, %101, %98, %98, %97), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:326:0\n",
       "  %b.5 : Float(768:1, requires_grad=1, device=cpu) = aten::to(%523, %101, %98, %98, %97), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:330:0\n",
       "  %584 : Tensor[] = prim::ListConstruct(%578, %582), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention\n",
       "  %585 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::einsum(%102, %584), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/functional.py:344:0\n",
       "  %input.37 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::add(%585, %b.5, %105), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:332:0\n",
       "  %projected_context_layer.5 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::dropout(%input.37, %99, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.output_dropout # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:983:0\n",
       "  %input.38 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::add(%522, %projected_context_layer.5, %105), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:334:0\n",
       "  %589 : Tensor = prim::GetAttr[name=\"bias\"](%530)\n",
       "  %590 : Tensor = prim::GetAttr[name=\"weight\"](%530)\n",
       "  %591 : int[] = prim::ListConstruct(%100), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm\n",
       "  %input_tensor.5 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::layer_norm(%input.38, %591, %590, %589, %104, %103), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:2094:0\n",
       "  %593 : (Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu), Float(768:1, requires_grad=1, device=cpu)) = prim::TupleConstruct(%input_tensor.5, %b.5)\n",
       "  %594 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu), %595 : Float(768:1, requires_grad=1, device=cpu) = prim::TupleUnpack(%593)\n",
       "  %596 : Tensor = prim::GetAttr[name=\"bias\"](%528)\n",
       "  %597 : Tensor = prim::GetAttr[name=\"weight\"](%528)\n",
       "  %598 : Float(768:1, 3072:768, requires_grad=1, device=cpu) = aten::t(%597), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.ffn # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1692:0\n",
       "  %output.25 : Float(1:307200, 100:3072, 3072:1, requires_grad=1, device=cpu) = aten::matmul(%594, %598), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.ffn # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1692:0\n",
       "  %x.35 : Float(1:307200, 100:3072, 3072:1, requires_grad=1, device=cpu) = aten::add_(%output.25, %596, %105), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.ffn # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1694:0\n",
       "  %601 : Float(1:307200, 100:3072, 3072:1, requires_grad=1, device=cpu) = aten::mul(%x.35, %88), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/activations.py:38:0\n",
       "  %602 : Float(1:307200, 100:3072, 3072:1, requires_grad=1, device=cpu) = aten::mul(%x.35, %87), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/activations.py:38:0\n",
       "  %603 : Float(1:307200, 100:3072, 3072:1, requires_grad=1, device=cpu) = aten::mul(%x.35, %86), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/activations.py:38:0\n",
       "  %604 : Float(1:307200, 100:3072, 3072:1, requires_grad=1, device=cpu) = aten::mul(%603, %x.35), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/activations.py:38:0\n",
       "  %605 : Float(1:307200, 100:3072, 3072:1, requires_grad=1, device=cpu) = aten::add(%604, %85, %105), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/activations.py:38:0\n",
       "  %606 : Float(1:307200, 100:3072, 3072:1, requires_grad=1, device=cpu) = aten::mul(%602, %605), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/activations.py:38:0\n",
       "  %607 : Float(1:307200, 100:3072, 3072:1, requires_grad=1, device=cpu) = aten::tanh(%606), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/activations.py:38:0\n",
       "  %608 : Float(1:307200, 100:3072, 3072:1, requires_grad=1, device=cpu) = aten::add(%607, %85, %105), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/activations.py:38:0\n",
       "  %input.39 : Float(1:307200, 100:3072, 3072:1, requires_grad=1, device=cpu) = aten::mul(%601, %608), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/activations.py:38:0\n",
       "  %610 : Tensor = prim::GetAttr[name=\"bias\"](%527)\n",
       "  %611 : Tensor = prim::GetAttr[name=\"weight\"](%527)\n",
       "  %612 : Float(3072:1, 768:3072, requires_grad=1, device=cpu) = aten::t(%611), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1692:0\n",
       "  %output.26 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::matmul(%input.39, %612), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1692:0\n",
       "  %ffn_output.5 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::add_(%output.26, %610, %105), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1694:0\n",
       "  %input.40 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::add(%ffn_output.5, %594, %105), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:363:0\n",
       "  %616 : Tensor = prim::GetAttr[name=\"bias\"](%526)\n",
       "  %617 : Tensor = prim::GetAttr[name=\"weight\"](%526)\n",
       "  %618 : int[] = prim::ListConstruct(%100), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm\n",
       "  %input.41 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::layer_norm(%input.40, %618, %617, %616, %104, %103), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:2094:0\n",
       "  %620 : (Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu), Float(768:1, requires_grad=1, device=cpu)) = prim::TupleConstruct(%input.41, %595)\n",
       "  %621 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu), %622 : Float(768:1, requires_grad=1, device=cpu) = prim::TupleUnpack(%620)\n",
       "  %623 : (Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu), Float(768:1, requires_grad=1, device=cpu)) = prim::TupleConstruct(%621, %622)\n",
       "  %624 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu), %625 : Float(768:1, requires_grad=1, device=cpu) = prim::TupleUnpack(%623)\n",
       "  %626 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name=\"albert_layers\"](%107)\n",
       "  %627 : __torch__.transformers.modeling_albert.AlbertLayer = prim::GetAttr[name=\"0\"](%626)\n",
       "  %628 : __torch__.torch.nn.modules.normalization.___torch_mangle_2.LayerNorm = prim::GetAttr[name=\"full_layer_layer_norm\"](%627)\n",
       "  %629 : __torch__.torch.nn.modules.linear.___torch_mangle_11.Linear = prim::GetAttr[name=\"ffn_output\"](%627)\n",
       "  %630 : __torch__.torch.nn.modules.linear.___torch_mangle_10.Linear = prim::GetAttr[name=\"ffn\"](%627)\n",
       "  %631 : __torch__.transformers.modeling_albert.AlbertAttention = prim::GetAttr[name=\"attention\"](%627)\n",
       "  %632 : __torch__.torch.nn.modules.normalization.___torch_mangle_9.LayerNorm = prim::GetAttr[name=\"LayerNorm\"](%631)\n",
       "  %633 : __torch__.torch.nn.modules.linear.___torch_mangle_8.Linear = prim::GetAttr[name=\"dense\"](%631)\n",
       "  %634 : Tensor = prim::GetAttr[name=\"weight\"](%633)\n",
       "  %635 : __torch__.torch.nn.modules.linear.___torch_mangle_5.Linear = prim::GetAttr[name=\"value\"](%631)\n",
       "  %636 : __torch__.torch.nn.modules.linear.___torch_mangle_4.Linear = prim::GetAttr[name=\"key\"](%631)\n",
       "  %637 : __torch__.torch.nn.modules.linear.___torch_mangle_3.Linear = prim::GetAttr[name=\"query\"](%631)\n",
       "  %638 : Tensor = prim::GetAttr[name=\"bias\"](%637)\n",
       "  %639 : Tensor = prim::GetAttr[name=\"weight\"](%637)\n",
       "  %640 : Float(768:1, 768:768, requires_grad=1, device=cpu) = aten::t(%639), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.query # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1692:0\n",
       "  %output.27 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::matmul(%624, %640), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.query # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1692:0\n",
       "  %x.36 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::add_(%output.27, %638, %105), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.query # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1694:0\n",
       "  %643 : Tensor = prim::GetAttr[name=\"bias\"](%636)\n",
       "  %644 : Tensor = prim::GetAttr[name=\"weight\"](%636)\n",
       "  %645 : Float(768:1, 768:768, requires_grad=1, device=cpu) = aten::t(%644), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.key # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1692:0\n",
       "  %output.28 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::matmul(%624, %645), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.key # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1692:0\n",
       "  %x.38 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::add_(%output.28, %643, %105), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.key # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1694:0\n",
       "  %648 : Tensor = prim::GetAttr[name=\"bias\"](%635)\n",
       "  %649 : Tensor = prim::GetAttr[name=\"weight\"](%635)\n",
       "  %650 : Float(768:1, 768:768, requires_grad=1, device=cpu) = aten::t(%649), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.value # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1692:0\n",
       "  %output.29 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::matmul(%624, %650), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.value # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1692:0\n",
       "  %x.40 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::add_(%output.29, %648, %105), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.value # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1694:0\n",
       "  %653 : int = aten::size(%x.36, %89), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:271:0\n",
       "  %654 : int = aten::size(%x.36, %105), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:271:0\n",
       "  %655 : int[] = prim::ListConstruct(%653, %654, %90, %91), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention\n",
       "  %x.37 : Float(1:76800, 100:768, 12:64, 64:1, requires_grad=1, device=cpu) = aten::view(%x.36, %655), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:272:0\n",
       "  %657 : int[] = prim::ListConstruct(%89, %92, %105, %93), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention\n",
       "  %query_layer.6 : Float(1:76800, 12:64, 100:768, 64:1, requires_grad=1, device=cpu) = aten::permute(%x.37, %657), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:273:0\n",
       "  %659 : int = aten::size(%x.38, %89), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:271:0\n",
       "  %660 : int = aten::size(%x.38, %105), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:271:0\n",
       "  %661 : int[] = prim::ListConstruct(%659, %660, %90, %91), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention\n",
       "  %x.39 : Float(1:76800, 100:768, 12:64, 64:1, requires_grad=1, device=cpu) = aten::view(%x.38, %661), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:272:0\n",
       "  %663 : int[] = prim::ListConstruct(%89, %92, %105, %93), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention\n",
       "  %key_layer.6 : Float(1:76800, 12:64, 100:768, 64:1, requires_grad=1, device=cpu) = aten::permute(%x.39, %663), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:273:0\n",
       "  %665 : int = aten::size(%x.40, %89), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:271:0\n",
       "  %666 : int = aten::size(%x.40, %105), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:271:0\n",
       "  %667 : int[] = prim::ListConstruct(%665, %666, %90, %91), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention\n",
       "  %x.41 : Float(1:76800, 100:768, 12:64, 64:1, requires_grad=1, device=cpu) = aten::view(%x.40, %667), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:272:0\n",
       "  %669 : int[] = prim::ListConstruct(%89, %92, %105, %93), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention\n",
       "  %value_layer.6 : Float(1:76800, 12:64, 100:768, 64:1, requires_grad=1, device=cpu) = aten::permute(%x.41, %669), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:273:0\n",
       "  %671 : Float(1:76800, 12:64, 64:1, 100:768, requires_grad=1, device=cpu) = aten::transpose(%key_layer.6, %94, %95), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:303:0\n",
       "  %attention_scores.11 : Float(1:120000, 12:10000, 100:100, 100:1, requires_grad=1, device=cpu) = aten::matmul(%query_layer.6, %671), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:303:0\n",
       "  %attention_scores.12 : Float(1:120000, 12:10000, 100:100, 100:1, requires_grad=1, device=cpu) = aten::div(%attention_scores.11, %96), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:304:0\n",
       "  %input.42 : Float(1:120000, 12:10000, 100:100, 100:1, requires_grad=1, device=cpu) = aten::add(%attention_scores.12, %attention_mask, %105), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:307:0\n",
       "  %input.43 : Float(1:120000, 12:10000, 100:100, 100:1, requires_grad=1, device=cpu) = aten::softmax(%input.42, %94, %97), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1512:0\n",
       "  %attention_probs.6 : Float(1:120000, 12:10000, 100:100, 100:1, requires_grad=1, device=cpu) = aten::dropout(%input.43, %99, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.attention_dropout # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:983:0\n",
       "  %context_layer.6 : Float(1:76800, 12:6400, 100:64, 64:1, requires_grad=1, device=cpu) = aten::matmul(%attention_probs.6, %value_layer.6), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:320:0\n",
       "  %678 : int[] = prim::ListConstruct(%89, %92, %105, %93), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention\n",
       "  %679 : Float(1:76800, 100:64, 12:6400, 64:1, requires_grad=1, device=cpu) = aten::permute(%context_layer.6, %678), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:322:0\n",
       "  %680 : Float(1:76800, 100:768, 12:64, 64:1, requires_grad=1, device=cpu) = aten::contiguous(%679, %89), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:322:0\n",
       "  %681 : Float(768:1, 768:768, requires_grad=1, device=cpu) = aten::t(%634), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:326:0\n",
       "  %682 : int[] = prim::ListConstruct(%90, %91, %100), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention\n",
       "  %683 : Float(12:64, 64:1, 768:768, requires_grad=1, device=cpu) = aten::view(%681, %682), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:326:0\n",
       "  %684 : Float(12:64, 64:1, 768:768, requires_grad=1, device=cpu) = aten::to(%683, %101, %98, %98, %97), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:326:0\n",
       "  %b.6 : Float(768:1, requires_grad=1, device=cpu) = aten::to(%625, %101, %98, %98, %97), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:330:0\n",
       "  %686 : Tensor[] = prim::ListConstruct(%680, %684), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention\n",
       "  %687 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::einsum(%102, %686), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/functional.py:344:0\n",
       "  %input.44 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::add(%687, %b.6, %105), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:332:0\n",
       "  %projected_context_layer.6 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::dropout(%input.44, %99, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.output_dropout # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:983:0\n",
       "  %input.45 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::add(%624, %projected_context_layer.6, %105), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:334:0\n",
       "  %691 : Tensor = prim::GetAttr[name=\"bias\"](%632)\n",
       "  %692 : Tensor = prim::GetAttr[name=\"weight\"](%632)\n",
       "  %693 : int[] = prim::ListConstruct(%100), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm\n",
       "  %input_tensor.6 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::layer_norm(%input.45, %693, %692, %691, %104, %103), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:2094:0\n",
       "  %695 : (Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu), Float(768:1, requires_grad=1, device=cpu)) = prim::TupleConstruct(%input_tensor.6, %b.6)\n",
       "  %696 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu), %697 : Float(768:1, requires_grad=1, device=cpu) = prim::TupleUnpack(%695)\n",
       "  %698 : Tensor = prim::GetAttr[name=\"bias\"](%630)\n",
       "  %699 : Tensor = prim::GetAttr[name=\"weight\"](%630)\n",
       "  %700 : Float(768:1, 3072:768, requires_grad=1, device=cpu) = aten::t(%699), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.ffn # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1692:0\n",
       "  %output.30 : Float(1:307200, 100:3072, 3072:1, requires_grad=1, device=cpu) = aten::matmul(%696, %700), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.ffn # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1692:0\n",
       "  %x.42 : Float(1:307200, 100:3072, 3072:1, requires_grad=1, device=cpu) = aten::add_(%output.30, %698, %105), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.ffn # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1694:0\n",
       "  %703 : Float(1:307200, 100:3072, 3072:1, requires_grad=1, device=cpu) = aten::mul(%x.42, %88), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/activations.py:38:0\n",
       "  %704 : Float(1:307200, 100:3072, 3072:1, requires_grad=1, device=cpu) = aten::mul(%x.42, %87), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/activations.py:38:0\n",
       "  %705 : Float(1:307200, 100:3072, 3072:1, requires_grad=1, device=cpu) = aten::mul(%x.42, %86), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/activations.py:38:0\n",
       "  %706 : Float(1:307200, 100:3072, 3072:1, requires_grad=1, device=cpu) = aten::mul(%705, %x.42), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/activations.py:38:0\n",
       "  %707 : Float(1:307200, 100:3072, 3072:1, requires_grad=1, device=cpu) = aten::add(%706, %85, %105), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/activations.py:38:0\n",
       "  %708 : Float(1:307200, 100:3072, 3072:1, requires_grad=1, device=cpu) = aten::mul(%704, %707), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/activations.py:38:0\n",
       "  %709 : Float(1:307200, 100:3072, 3072:1, requires_grad=1, device=cpu) = aten::tanh(%708), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/activations.py:38:0\n",
       "  %710 : Float(1:307200, 100:3072, 3072:1, requires_grad=1, device=cpu) = aten::add(%709, %85, %105), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/activations.py:38:0\n",
       "  %input.46 : Float(1:307200, 100:3072, 3072:1, requires_grad=1, device=cpu) = aten::mul(%703, %710), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/activations.py:38:0\n",
       "  %712 : Tensor = prim::GetAttr[name=\"bias\"](%629)\n",
       "  %713 : Tensor = prim::GetAttr[name=\"weight\"](%629)\n",
       "  %714 : Float(3072:1, 768:3072, requires_grad=1, device=cpu) = aten::t(%713), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1692:0\n",
       "  %output.31 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::matmul(%input.46, %714), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1692:0\n",
       "  %ffn_output.6 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::add_(%output.31, %712, %105), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1694:0\n",
       "  %input.47 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::add(%ffn_output.6, %696, %105), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:363:0\n",
       "  %718 : Tensor = prim::GetAttr[name=\"bias\"](%628)\n",
       "  %719 : Tensor = prim::GetAttr[name=\"weight\"](%628)\n",
       "  %720 : int[] = prim::ListConstruct(%100), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm\n",
       "  %input.48 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::layer_norm(%input.47, %720, %719, %718, %104, %103), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:2094:0\n",
       "  %722 : (Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu), Float(768:1, requires_grad=1, device=cpu)) = prim::TupleConstruct(%input.48, %697)\n",
       "  %723 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu), %724 : Float(768:1, requires_grad=1, device=cpu) = prim::TupleUnpack(%722)\n",
       "  %725 : (Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu), Float(768:1, requires_grad=1, device=cpu)) = prim::TupleConstruct(%723, %724)\n",
       "  %726 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu), %727 : Float(768:1, requires_grad=1, device=cpu) = prim::TupleUnpack(%725)\n",
       "  %728 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name=\"albert_layers\"](%107)\n",
       "  %729 : __torch__.transformers.modeling_albert.AlbertLayer = prim::GetAttr[name=\"0\"](%728)\n",
       "  %730 : __torch__.torch.nn.modules.normalization.___torch_mangle_2.LayerNorm = prim::GetAttr[name=\"full_layer_layer_norm\"](%729)\n",
       "  %731 : __torch__.torch.nn.modules.linear.___torch_mangle_11.Linear = prim::GetAttr[name=\"ffn_output\"](%729)\n",
       "  %732 : __torch__.torch.nn.modules.linear.___torch_mangle_10.Linear = prim::GetAttr[name=\"ffn\"](%729)\n",
       "  %733 : __torch__.transformers.modeling_albert.AlbertAttention = prim::GetAttr[name=\"attention\"](%729)\n",
       "  %734 : __torch__.torch.nn.modules.normalization.___torch_mangle_9.LayerNorm = prim::GetAttr[name=\"LayerNorm\"](%733)\n",
       "  %735 : __torch__.torch.nn.modules.linear.___torch_mangle_8.Linear = prim::GetAttr[name=\"dense\"](%733)\n",
       "  %736 : Tensor = prim::GetAttr[name=\"weight\"](%735)\n",
       "  %737 : __torch__.torch.nn.modules.linear.___torch_mangle_5.Linear = prim::GetAttr[name=\"value\"](%733)\n",
       "  %738 : __torch__.torch.nn.modules.linear.___torch_mangle_4.Linear = prim::GetAttr[name=\"key\"](%733)\n",
       "  %739 : __torch__.torch.nn.modules.linear.___torch_mangle_3.Linear = prim::GetAttr[name=\"query\"](%733)\n",
       "  %740 : Tensor = prim::GetAttr[name=\"bias\"](%739)\n",
       "  %741 : Tensor = prim::GetAttr[name=\"weight\"](%739)\n",
       "  %742 : Float(768:1, 768:768, requires_grad=1, device=cpu) = aten::t(%741), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.query # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1692:0\n",
       "  %output.32 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::matmul(%726, %742), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.query # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1692:0\n",
       "  %x.43 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::add_(%output.32, %740, %105), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.query # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1694:0\n",
       "  %745 : Tensor = prim::GetAttr[name=\"bias\"](%738)\n",
       "  %746 : Tensor = prim::GetAttr[name=\"weight\"](%738)\n",
       "  %747 : Float(768:1, 768:768, requires_grad=1, device=cpu) = aten::t(%746), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.key # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1692:0\n",
       "  %output.33 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::matmul(%726, %747), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.key # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1692:0\n",
       "  %x.45 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::add_(%output.33, %745, %105), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.key # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1694:0\n",
       "  %750 : Tensor = prim::GetAttr[name=\"bias\"](%737)\n",
       "  %751 : Tensor = prim::GetAttr[name=\"weight\"](%737)\n",
       "  %752 : Float(768:1, 768:768, requires_grad=1, device=cpu) = aten::t(%751), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.value # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1692:0\n",
       "  %output.34 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::matmul(%726, %752), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.value # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1692:0\n",
       "  %x.47 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::add_(%output.34, %750, %105), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.value # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1694:0\n",
       "  %755 : int = aten::size(%x.43, %89), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:271:0\n",
       "  %756 : int = aten::size(%x.43, %105), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:271:0\n",
       "  %757 : int[] = prim::ListConstruct(%755, %756, %90, %91), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention\n",
       "  %x.44 : Float(1:76800, 100:768, 12:64, 64:1, requires_grad=1, device=cpu) = aten::view(%x.43, %757), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:272:0\n",
       "  %759 : int[] = prim::ListConstruct(%89, %92, %105, %93), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention\n",
       "  %query_layer.7 : Float(1:76800, 12:64, 100:768, 64:1, requires_grad=1, device=cpu) = aten::permute(%x.44, %759), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:273:0\n",
       "  %761 : int = aten::size(%x.45, %89), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:271:0\n",
       "  %762 : int = aten::size(%x.45, %105), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:271:0\n",
       "  %763 : int[] = prim::ListConstruct(%761, %762, %90, %91), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention\n",
       "  %x.46 : Float(1:76800, 100:768, 12:64, 64:1, requires_grad=1, device=cpu) = aten::view(%x.45, %763), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:272:0\n",
       "  %765 : int[] = prim::ListConstruct(%89, %92, %105, %93), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention\n",
       "  %key_layer.7 : Float(1:76800, 12:64, 100:768, 64:1, requires_grad=1, device=cpu) = aten::permute(%x.46, %765), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:273:0\n",
       "  %767 : int = aten::size(%x.47, %89), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:271:0\n",
       "  %768 : int = aten::size(%x.47, %105), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:271:0\n",
       "  %769 : int[] = prim::ListConstruct(%767, %768, %90, %91), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention\n",
       "  %x.48 : Float(1:76800, 100:768, 12:64, 64:1, requires_grad=1, device=cpu) = aten::view(%x.47, %769), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:272:0\n",
       "  %771 : int[] = prim::ListConstruct(%89, %92, %105, %93), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention\n",
       "  %value_layer.7 : Float(1:76800, 12:64, 100:768, 64:1, requires_grad=1, device=cpu) = aten::permute(%x.48, %771), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:273:0\n",
       "  %773 : Float(1:76800, 12:64, 64:1, 100:768, requires_grad=1, device=cpu) = aten::transpose(%key_layer.7, %94, %95), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:303:0\n",
       "  %attention_scores.13 : Float(1:120000, 12:10000, 100:100, 100:1, requires_grad=1, device=cpu) = aten::matmul(%query_layer.7, %773), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:303:0\n",
       "  %attention_scores.14 : Float(1:120000, 12:10000, 100:100, 100:1, requires_grad=1, device=cpu) = aten::div(%attention_scores.13, %96), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:304:0\n",
       "  %input.49 : Float(1:120000, 12:10000, 100:100, 100:1, requires_grad=1, device=cpu) = aten::add(%attention_scores.14, %attention_mask, %105), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:307:0\n",
       "  %input.50 : Float(1:120000, 12:10000, 100:100, 100:1, requires_grad=1, device=cpu) = aten::softmax(%input.49, %94, %97), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1512:0\n",
       "  %attention_probs.7 : Float(1:120000, 12:10000, 100:100, 100:1, requires_grad=1, device=cpu) = aten::dropout(%input.50, %99, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.attention_dropout # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:983:0\n",
       "  %context_layer.7 : Float(1:76800, 12:6400, 100:64, 64:1, requires_grad=1, device=cpu) = aten::matmul(%attention_probs.7, %value_layer.7), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:320:0\n",
       "  %780 : int[] = prim::ListConstruct(%89, %92, %105, %93), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention\n",
       "  %781 : Float(1:76800, 100:64, 12:6400, 64:1, requires_grad=1, device=cpu) = aten::permute(%context_layer.7, %780), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:322:0\n",
       "  %782 : Float(1:76800, 100:768, 12:64, 64:1, requires_grad=1, device=cpu) = aten::contiguous(%781, %89), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:322:0\n",
       "  %783 : Float(768:1, 768:768, requires_grad=1, device=cpu) = aten::t(%736), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:326:0\n",
       "  %784 : int[] = prim::ListConstruct(%90, %91, %100), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention\n",
       "  %785 : Float(12:64, 64:1, 768:768, requires_grad=1, device=cpu) = aten::view(%783, %784), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:326:0\n",
       "  %786 : Float(12:64, 64:1, 768:768, requires_grad=1, device=cpu) = aten::to(%785, %101, %98, %98, %97), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:326:0\n",
       "  %b.7 : Float(768:1, requires_grad=1, device=cpu) = aten::to(%727, %101, %98, %98, %97), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:330:0\n",
       "  %788 : Tensor[] = prim::ListConstruct(%782, %786), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention\n",
       "  %789 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::einsum(%102, %788), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/functional.py:344:0\n",
       "  %input.51 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::add(%789, %b.7, %105), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:332:0\n",
       "  %projected_context_layer.7 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::dropout(%input.51, %99, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.output_dropout # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:983:0\n",
       "  %input.52 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::add(%726, %projected_context_layer.7, %105), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:334:0\n",
       "  %793 : Tensor = prim::GetAttr[name=\"bias\"](%734)\n",
       "  %794 : Tensor = prim::GetAttr[name=\"weight\"](%734)\n",
       "  %795 : int[] = prim::ListConstruct(%100), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm\n",
       "  %input_tensor.7 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::layer_norm(%input.52, %795, %794, %793, %104, %103), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:2094:0\n",
       "  %797 : (Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu), Float(768:1, requires_grad=1, device=cpu)) = prim::TupleConstruct(%input_tensor.7, %b.7)\n",
       "  %798 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu), %799 : Float(768:1, requires_grad=1, device=cpu) = prim::TupleUnpack(%797)\n",
       "  %800 : Tensor = prim::GetAttr[name=\"bias\"](%732)\n",
       "  %801 : Tensor = prim::GetAttr[name=\"weight\"](%732)\n",
       "  %802 : Float(768:1, 3072:768, requires_grad=1, device=cpu) = aten::t(%801), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.ffn # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1692:0\n",
       "  %output.35 : Float(1:307200, 100:3072, 3072:1, requires_grad=1, device=cpu) = aten::matmul(%798, %802), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.ffn # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1692:0\n",
       "  %x.49 : Float(1:307200, 100:3072, 3072:1, requires_grad=1, device=cpu) = aten::add_(%output.35, %800, %105), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.ffn # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1694:0\n",
       "  %805 : Float(1:307200, 100:3072, 3072:1, requires_grad=1, device=cpu) = aten::mul(%x.49, %88), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/activations.py:38:0\n",
       "  %806 : Float(1:307200, 100:3072, 3072:1, requires_grad=1, device=cpu) = aten::mul(%x.49, %87), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/activations.py:38:0\n",
       "  %807 : Float(1:307200, 100:3072, 3072:1, requires_grad=1, device=cpu) = aten::mul(%x.49, %86), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/activations.py:38:0\n",
       "  %808 : Float(1:307200, 100:3072, 3072:1, requires_grad=1, device=cpu) = aten::mul(%807, %x.49), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/activations.py:38:0\n",
       "  %809 : Float(1:307200, 100:3072, 3072:1, requires_grad=1, device=cpu) = aten::add(%808, %85, %105), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/activations.py:38:0\n",
       "  %810 : Float(1:307200, 100:3072, 3072:1, requires_grad=1, device=cpu) = aten::mul(%806, %809), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/activations.py:38:0\n",
       "  %811 : Float(1:307200, 100:3072, 3072:1, requires_grad=1, device=cpu) = aten::tanh(%810), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/activations.py:38:0\n",
       "  %812 : Float(1:307200, 100:3072, 3072:1, requires_grad=1, device=cpu) = aten::add(%811, %85, %105), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/activations.py:38:0\n",
       "  %input.53 : Float(1:307200, 100:3072, 3072:1, requires_grad=1, device=cpu) = aten::mul(%805, %812), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/activations.py:38:0\n",
       "  %814 : Tensor = prim::GetAttr[name=\"bias\"](%731)\n",
       "  %815 : Tensor = prim::GetAttr[name=\"weight\"](%731)\n",
       "  %816 : Float(3072:1, 768:3072, requires_grad=1, device=cpu) = aten::t(%815), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1692:0\n",
       "  %output.36 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::matmul(%input.53, %816), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1692:0\n",
       "  %ffn_output.7 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::add_(%output.36, %814, %105), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1694:0\n",
       "  %input.54 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::add(%ffn_output.7, %798, %105), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:363:0\n",
       "  %820 : Tensor = prim::GetAttr[name=\"bias\"](%730)\n",
       "  %821 : Tensor = prim::GetAttr[name=\"weight\"](%730)\n",
       "  %822 : int[] = prim::ListConstruct(%100), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm\n",
       "  %input.55 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::layer_norm(%input.54, %822, %821, %820, %104, %103), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:2094:0\n",
       "  %824 : (Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu), Float(768:1, requires_grad=1, device=cpu)) = prim::TupleConstruct(%input.55, %799)\n",
       "  %825 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu), %826 : Float(768:1, requires_grad=1, device=cpu) = prim::TupleUnpack(%824)\n",
       "  %827 : (Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu), Float(768:1, requires_grad=1, device=cpu)) = prim::TupleConstruct(%825, %826)\n",
       "  %828 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu), %829 : Float(768:1, requires_grad=1, device=cpu) = prim::TupleUnpack(%827)\n",
       "  %830 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name=\"albert_layers\"](%107)\n",
       "  %831 : __torch__.transformers.modeling_albert.AlbertLayer = prim::GetAttr[name=\"0\"](%830)\n",
       "  %832 : __torch__.torch.nn.modules.normalization.___torch_mangle_2.LayerNorm = prim::GetAttr[name=\"full_layer_layer_norm\"](%831)\n",
       "  %833 : __torch__.torch.nn.modules.linear.___torch_mangle_11.Linear = prim::GetAttr[name=\"ffn_output\"](%831)\n",
       "  %834 : __torch__.torch.nn.modules.linear.___torch_mangle_10.Linear = prim::GetAttr[name=\"ffn\"](%831)\n",
       "  %835 : __torch__.transformers.modeling_albert.AlbertAttention = prim::GetAttr[name=\"attention\"](%831)\n",
       "  %836 : __torch__.torch.nn.modules.normalization.___torch_mangle_9.LayerNorm = prim::GetAttr[name=\"LayerNorm\"](%835)\n",
       "  %837 : __torch__.torch.nn.modules.linear.___torch_mangle_8.Linear = prim::GetAttr[name=\"dense\"](%835)\n",
       "  %838 : Tensor = prim::GetAttr[name=\"weight\"](%837)\n",
       "  %839 : __torch__.torch.nn.modules.linear.___torch_mangle_5.Linear = prim::GetAttr[name=\"value\"](%835)\n",
       "  %840 : __torch__.torch.nn.modules.linear.___torch_mangle_4.Linear = prim::GetAttr[name=\"key\"](%835)\n",
       "  %841 : __torch__.torch.nn.modules.linear.___torch_mangle_3.Linear = prim::GetAttr[name=\"query\"](%835)\n",
       "  %842 : Tensor = prim::GetAttr[name=\"bias\"](%841)\n",
       "  %843 : Tensor = prim::GetAttr[name=\"weight\"](%841)\n",
       "  %844 : Float(768:1, 768:768, requires_grad=1, device=cpu) = aten::t(%843), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.query # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1692:0\n",
       "  %output.37 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::matmul(%828, %844), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.query # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1692:0\n",
       "  %x.50 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::add_(%output.37, %842, %105), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.query # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1694:0\n",
       "  %847 : Tensor = prim::GetAttr[name=\"bias\"](%840)\n",
       "  %848 : Tensor = prim::GetAttr[name=\"weight\"](%840)\n",
       "  %849 : Float(768:1, 768:768, requires_grad=1, device=cpu) = aten::t(%848), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.key # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1692:0\n",
       "  %output.38 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::matmul(%828, %849), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.key # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1692:0\n",
       "  %x.52 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::add_(%output.38, %847, %105), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.key # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1694:0\n",
       "  %852 : Tensor = prim::GetAttr[name=\"bias\"](%839)\n",
       "  %853 : Tensor = prim::GetAttr[name=\"weight\"](%839)\n",
       "  %854 : Float(768:1, 768:768, requires_grad=1, device=cpu) = aten::t(%853), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.value # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1692:0\n",
       "  %output.39 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::matmul(%828, %854), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.value # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1692:0\n",
       "  %x.54 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::add_(%output.39, %852, %105), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.value # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1694:0\n",
       "  %857 : int = aten::size(%x.50, %89), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:271:0\n",
       "  %858 : int = aten::size(%x.50, %105), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:271:0\n",
       "  %859 : int[] = prim::ListConstruct(%857, %858, %90, %91), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention\n",
       "  %x.51 : Float(1:76800, 100:768, 12:64, 64:1, requires_grad=1, device=cpu) = aten::view(%x.50, %859), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:272:0\n",
       "  %861 : int[] = prim::ListConstruct(%89, %92, %105, %93), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention\n",
       "  %query_layer.8 : Float(1:76800, 12:64, 100:768, 64:1, requires_grad=1, device=cpu) = aten::permute(%x.51, %861), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:273:0\n",
       "  %863 : int = aten::size(%x.52, %89), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:271:0\n",
       "  %864 : int = aten::size(%x.52, %105), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:271:0\n",
       "  %865 : int[] = prim::ListConstruct(%863, %864, %90, %91), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention\n",
       "  %x.53 : Float(1:76800, 100:768, 12:64, 64:1, requires_grad=1, device=cpu) = aten::view(%x.52, %865), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:272:0\n",
       "  %867 : int[] = prim::ListConstruct(%89, %92, %105, %93), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention\n",
       "  %key_layer.8 : Float(1:76800, 12:64, 100:768, 64:1, requires_grad=1, device=cpu) = aten::permute(%x.53, %867), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:273:0\n",
       "  %869 : int = aten::size(%x.54, %89), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:271:0\n",
       "  %870 : int = aten::size(%x.54, %105), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:271:0\n",
       "  %871 : int[] = prim::ListConstruct(%869, %870, %90, %91), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention\n",
       "  %x.55 : Float(1:76800, 100:768, 12:64, 64:1, requires_grad=1, device=cpu) = aten::view(%x.54, %871), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:272:0\n",
       "  %873 : int[] = prim::ListConstruct(%89, %92, %105, %93), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention\n",
       "  %value_layer.8 : Float(1:76800, 12:64, 100:768, 64:1, requires_grad=1, device=cpu) = aten::permute(%x.55, %873), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:273:0\n",
       "  %875 : Float(1:76800, 12:64, 64:1, 100:768, requires_grad=1, device=cpu) = aten::transpose(%key_layer.8, %94, %95), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:303:0\n",
       "  %attention_scores.15 : Float(1:120000, 12:10000, 100:100, 100:1, requires_grad=1, device=cpu) = aten::matmul(%query_layer.8, %875), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:303:0\n",
       "  %attention_scores.16 : Float(1:120000, 12:10000, 100:100, 100:1, requires_grad=1, device=cpu) = aten::div(%attention_scores.15, %96), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:304:0\n",
       "  %input.56 : Float(1:120000, 12:10000, 100:100, 100:1, requires_grad=1, device=cpu) = aten::add(%attention_scores.16, %attention_mask, %105), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:307:0\n",
       "  %input.57 : Float(1:120000, 12:10000, 100:100, 100:1, requires_grad=1, device=cpu) = aten::softmax(%input.56, %94, %97), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1512:0\n",
       "  %attention_probs.8 : Float(1:120000, 12:10000, 100:100, 100:1, requires_grad=1, device=cpu) = aten::dropout(%input.57, %99, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.attention_dropout # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:983:0\n",
       "  %context_layer.8 : Float(1:76800, 12:6400, 100:64, 64:1, requires_grad=1, device=cpu) = aten::matmul(%attention_probs.8, %value_layer.8), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:320:0\n",
       "  %882 : int[] = prim::ListConstruct(%89, %92, %105, %93), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention\n",
       "  %883 : Float(1:76800, 100:64, 12:6400, 64:1, requires_grad=1, device=cpu) = aten::permute(%context_layer.8, %882), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:322:0\n",
       "  %884 : Float(1:76800, 100:768, 12:64, 64:1, requires_grad=1, device=cpu) = aten::contiguous(%883, %89), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:322:0\n",
       "  %885 : Float(768:1, 768:768, requires_grad=1, device=cpu) = aten::t(%838), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:326:0\n",
       "  %886 : int[] = prim::ListConstruct(%90, %91, %100), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention\n",
       "  %887 : Float(12:64, 64:1, 768:768, requires_grad=1, device=cpu) = aten::view(%885, %886), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:326:0\n",
       "  %888 : Float(12:64, 64:1, 768:768, requires_grad=1, device=cpu) = aten::to(%887, %101, %98, %98, %97), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:326:0\n",
       "  %b.8 : Float(768:1, requires_grad=1, device=cpu) = aten::to(%829, %101, %98, %98, %97), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:330:0\n",
       "  %890 : Tensor[] = prim::ListConstruct(%884, %888), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention\n",
       "  %891 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::einsum(%102, %890), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/functional.py:344:0\n",
       "  %input.58 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::add(%891, %b.8, %105), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:332:0\n",
       "  %projected_context_layer.8 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::dropout(%input.58, %99, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.output_dropout # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:983:0\n",
       "  %input.59 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::add(%828, %projected_context_layer.8, %105), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:334:0\n",
       "  %895 : Tensor = prim::GetAttr[name=\"bias\"](%836)\n",
       "  %896 : Tensor = prim::GetAttr[name=\"weight\"](%836)\n",
       "  %897 : int[] = prim::ListConstruct(%100), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm\n",
       "  %input_tensor.8 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::layer_norm(%input.59, %897, %896, %895, %104, %103), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:2094:0\n",
       "  %899 : (Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu), Float(768:1, requires_grad=1, device=cpu)) = prim::TupleConstruct(%input_tensor.8, %b.8)\n",
       "  %900 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu), %901 : Float(768:1, requires_grad=1, device=cpu) = prim::TupleUnpack(%899)\n",
       "  %902 : Tensor = prim::GetAttr[name=\"bias\"](%834)\n",
       "  %903 : Tensor = prim::GetAttr[name=\"weight\"](%834)\n",
       "  %904 : Float(768:1, 3072:768, requires_grad=1, device=cpu) = aten::t(%903), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.ffn # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1692:0\n",
       "  %output.40 : Float(1:307200, 100:3072, 3072:1, requires_grad=1, device=cpu) = aten::matmul(%900, %904), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.ffn # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1692:0\n",
       "  %x.56 : Float(1:307200, 100:3072, 3072:1, requires_grad=1, device=cpu) = aten::add_(%output.40, %902, %105), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.ffn # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1694:0\n",
       "  %907 : Float(1:307200, 100:3072, 3072:1, requires_grad=1, device=cpu) = aten::mul(%x.56, %88), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/activations.py:38:0\n",
       "  %908 : Float(1:307200, 100:3072, 3072:1, requires_grad=1, device=cpu) = aten::mul(%x.56, %87), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/activations.py:38:0\n",
       "  %909 : Float(1:307200, 100:3072, 3072:1, requires_grad=1, device=cpu) = aten::mul(%x.56, %86), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/activations.py:38:0\n",
       "  %910 : Float(1:307200, 100:3072, 3072:1, requires_grad=1, device=cpu) = aten::mul(%909, %x.56), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/activations.py:38:0\n",
       "  %911 : Float(1:307200, 100:3072, 3072:1, requires_grad=1, device=cpu) = aten::add(%910, %85, %105), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/activations.py:38:0\n",
       "  %912 : Float(1:307200, 100:3072, 3072:1, requires_grad=1, device=cpu) = aten::mul(%908, %911), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/activations.py:38:0\n",
       "  %913 : Float(1:307200, 100:3072, 3072:1, requires_grad=1, device=cpu) = aten::tanh(%912), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/activations.py:38:0\n",
       "  %914 : Float(1:307200, 100:3072, 3072:1, requires_grad=1, device=cpu) = aten::add(%913, %85, %105), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/activations.py:38:0\n",
       "  %input.60 : Float(1:307200, 100:3072, 3072:1, requires_grad=1, device=cpu) = aten::mul(%907, %914), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/activations.py:38:0\n",
       "  %916 : Tensor = prim::GetAttr[name=\"bias\"](%833)\n",
       "  %917 : Tensor = prim::GetAttr[name=\"weight\"](%833)\n",
       "  %918 : Float(3072:1, 768:3072, requires_grad=1, device=cpu) = aten::t(%917), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1692:0\n",
       "  %output.41 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::matmul(%input.60, %918), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1692:0\n",
       "  %ffn_output.8 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::add_(%output.41, %916, %105), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1694:0\n",
       "  %input.61 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::add(%ffn_output.8, %900, %105), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:363:0\n",
       "  %922 : Tensor = prim::GetAttr[name=\"bias\"](%832)\n",
       "  %923 : Tensor = prim::GetAttr[name=\"weight\"](%832)\n",
       "  %924 : int[] = prim::ListConstruct(%100), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm\n",
       "  %input.62 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::layer_norm(%input.61, %924, %923, %922, %104, %103), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:2094:0\n",
       "  %926 : (Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu), Float(768:1, requires_grad=1, device=cpu)) = prim::TupleConstruct(%input.62, %901)\n",
       "  %927 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu), %928 : Float(768:1, requires_grad=1, device=cpu) = prim::TupleUnpack(%926)\n",
       "  %929 : (Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu), Float(768:1, requires_grad=1, device=cpu)) = prim::TupleConstruct(%927, %928)\n",
       "  %930 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu), %931 : Float(768:1, requires_grad=1, device=cpu) = prim::TupleUnpack(%929)\n",
       "  %932 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name=\"albert_layers\"](%107)\n",
       "  %933 : __torch__.transformers.modeling_albert.AlbertLayer = prim::GetAttr[name=\"0\"](%932)\n",
       "  %934 : __torch__.torch.nn.modules.normalization.___torch_mangle_2.LayerNorm = prim::GetAttr[name=\"full_layer_layer_norm\"](%933)\n",
       "  %935 : __torch__.torch.nn.modules.linear.___torch_mangle_11.Linear = prim::GetAttr[name=\"ffn_output\"](%933)\n",
       "  %936 : __torch__.torch.nn.modules.linear.___torch_mangle_10.Linear = prim::GetAttr[name=\"ffn\"](%933)\n",
       "  %937 : __torch__.transformers.modeling_albert.AlbertAttention = prim::GetAttr[name=\"attention\"](%933)\n",
       "  %938 : __torch__.torch.nn.modules.normalization.___torch_mangle_9.LayerNorm = prim::GetAttr[name=\"LayerNorm\"](%937)\n",
       "  %939 : __torch__.torch.nn.modules.linear.___torch_mangle_8.Linear = prim::GetAttr[name=\"dense\"](%937)\n",
       "  %940 : Tensor = prim::GetAttr[name=\"weight\"](%939)\n",
       "  %941 : __torch__.torch.nn.modules.linear.___torch_mangle_5.Linear = prim::GetAttr[name=\"value\"](%937)\n",
       "  %942 : __torch__.torch.nn.modules.linear.___torch_mangle_4.Linear = prim::GetAttr[name=\"key\"](%937)\n",
       "  %943 : __torch__.torch.nn.modules.linear.___torch_mangle_3.Linear = prim::GetAttr[name=\"query\"](%937)\n",
       "  %944 : Tensor = prim::GetAttr[name=\"bias\"](%943)\n",
       "  %945 : Tensor = prim::GetAttr[name=\"weight\"](%943)\n",
       "  %946 : Float(768:1, 768:768, requires_grad=1, device=cpu) = aten::t(%945), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.query # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1692:0\n",
       "  %output.42 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::matmul(%930, %946), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.query # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1692:0\n",
       "  %x.57 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::add_(%output.42, %944, %105), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.query # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1694:0\n",
       "  %949 : Tensor = prim::GetAttr[name=\"bias\"](%942)\n",
       "  %950 : Tensor = prim::GetAttr[name=\"weight\"](%942)\n",
       "  %951 : Float(768:1, 768:768, requires_grad=1, device=cpu) = aten::t(%950), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.key # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1692:0\n",
       "  %output.43 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::matmul(%930, %951), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.key # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1692:0\n",
       "  %x.59 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::add_(%output.43, %949, %105), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.key # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1694:0\n",
       "  %954 : Tensor = prim::GetAttr[name=\"bias\"](%941)\n",
       "  %955 : Tensor = prim::GetAttr[name=\"weight\"](%941)\n",
       "  %956 : Float(768:1, 768:768, requires_grad=1, device=cpu) = aten::t(%955), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.value # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1692:0\n",
       "  %output.44 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::matmul(%930, %956), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.value # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1692:0\n",
       "  %x.61 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::add_(%output.44, %954, %105), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.value # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1694:0\n",
       "  %959 : int = aten::size(%x.57, %89), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:271:0\n",
       "  %960 : int = aten::size(%x.57, %105), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:271:0\n",
       "  %961 : int[] = prim::ListConstruct(%959, %960, %90, %91), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention\n",
       "  %x.58 : Float(1:76800, 100:768, 12:64, 64:1, requires_grad=1, device=cpu) = aten::view(%x.57, %961), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:272:0\n",
       "  %963 : int[] = prim::ListConstruct(%89, %92, %105, %93), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention\n",
       "  %query_layer.9 : Float(1:76800, 12:64, 100:768, 64:1, requires_grad=1, device=cpu) = aten::permute(%x.58, %963), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:273:0\n",
       "  %965 : int = aten::size(%x.59, %89), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:271:0\n",
       "  %966 : int = aten::size(%x.59, %105), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:271:0\n",
       "  %967 : int[] = prim::ListConstruct(%965, %966, %90, %91), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention\n",
       "  %x.60 : Float(1:76800, 100:768, 12:64, 64:1, requires_grad=1, device=cpu) = aten::view(%x.59, %967), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:272:0\n",
       "  %969 : int[] = prim::ListConstruct(%89, %92, %105, %93), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention\n",
       "  %key_layer.9 : Float(1:76800, 12:64, 100:768, 64:1, requires_grad=1, device=cpu) = aten::permute(%x.60, %969), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:273:0\n",
       "  %971 : int = aten::size(%x.61, %89), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:271:0\n",
       "  %972 : int = aten::size(%x.61, %105), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:271:0\n",
       "  %973 : int[] = prim::ListConstruct(%971, %972, %90, %91), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention\n",
       "  %x.62 : Float(1:76800, 100:768, 12:64, 64:1, requires_grad=1, device=cpu) = aten::view(%x.61, %973), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:272:0\n",
       "  %975 : int[] = prim::ListConstruct(%89, %92, %105, %93), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention\n",
       "  %value_layer.9 : Float(1:76800, 12:64, 100:768, 64:1, requires_grad=1, device=cpu) = aten::permute(%x.62, %975), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:273:0\n",
       "  %977 : Float(1:76800, 12:64, 64:1, 100:768, requires_grad=1, device=cpu) = aten::transpose(%key_layer.9, %94, %95), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:303:0\n",
       "  %attention_scores.17 : Float(1:120000, 12:10000, 100:100, 100:1, requires_grad=1, device=cpu) = aten::matmul(%query_layer.9, %977), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:303:0\n",
       "  %attention_scores.18 : Float(1:120000, 12:10000, 100:100, 100:1, requires_grad=1, device=cpu) = aten::div(%attention_scores.17, %96), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:304:0\n",
       "  %input.63 : Float(1:120000, 12:10000, 100:100, 100:1, requires_grad=1, device=cpu) = aten::add(%attention_scores.18, %attention_mask, %105), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:307:0\n",
       "  %input.64 : Float(1:120000, 12:10000, 100:100, 100:1, requires_grad=1, device=cpu) = aten::softmax(%input.63, %94, %97), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1512:0\n",
       "  %attention_probs.9 : Float(1:120000, 12:10000, 100:100, 100:1, requires_grad=1, device=cpu) = aten::dropout(%input.64, %99, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.attention_dropout # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:983:0\n",
       "  %context_layer.9 : Float(1:76800, 12:6400, 100:64, 64:1, requires_grad=1, device=cpu) = aten::matmul(%attention_probs.9, %value_layer.9), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:320:0\n",
       "  %984 : int[] = prim::ListConstruct(%89, %92, %105, %93), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention\n",
       "  %985 : Float(1:76800, 100:64, 12:6400, 64:1, requires_grad=1, device=cpu) = aten::permute(%context_layer.9, %984), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:322:0\n",
       "  %986 : Float(1:76800, 100:768, 12:64, 64:1, requires_grad=1, device=cpu) = aten::contiguous(%985, %89), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:322:0\n",
       "  %987 : Float(768:1, 768:768, requires_grad=1, device=cpu) = aten::t(%940), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:326:0\n",
       "  %988 : int[] = prim::ListConstruct(%90, %91, %100), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention\n",
       "  %989 : Float(12:64, 64:1, 768:768, requires_grad=1, device=cpu) = aten::view(%987, %988), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:326:0\n",
       "  %990 : Float(12:64, 64:1, 768:768, requires_grad=1, device=cpu) = aten::to(%989, %101, %98, %98, %97), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:326:0\n",
       "  %b.9 : Float(768:1, requires_grad=1, device=cpu) = aten::to(%931, %101, %98, %98, %97), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:330:0\n",
       "  %992 : Tensor[] = prim::ListConstruct(%986, %990), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention\n",
       "  %993 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::einsum(%102, %992), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/functional.py:344:0\n",
       "  %input.65 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::add(%993, %b.9, %105), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:332:0\n",
       "  %projected_context_layer.9 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::dropout(%input.65, %99, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.output_dropout # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:983:0\n",
       "  %input.66 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::add(%930, %projected_context_layer.9, %105), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:334:0\n",
       "  %997 : Tensor = prim::GetAttr[name=\"bias\"](%938)\n",
       "  %998 : Tensor = prim::GetAttr[name=\"weight\"](%938)\n",
       "  %999 : int[] = prim::ListConstruct(%100), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm\n",
       "  %input_tensor.9 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::layer_norm(%input.66, %999, %998, %997, %104, %103), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:2094:0\n",
       "  %1001 : (Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu), Float(768:1, requires_grad=1, device=cpu)) = prim::TupleConstruct(%input_tensor.9, %b.9)\n",
       "  %1002 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu), %1003 : Float(768:1, requires_grad=1, device=cpu) = prim::TupleUnpack(%1001)\n",
       "  %1004 : Tensor = prim::GetAttr[name=\"bias\"](%936)\n",
       "  %1005 : Tensor = prim::GetAttr[name=\"weight\"](%936)\n",
       "  %1006 : Float(768:1, 3072:768, requires_grad=1, device=cpu) = aten::t(%1005), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.ffn # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1692:0\n",
       "  %output.45 : Float(1:307200, 100:3072, 3072:1, requires_grad=1, device=cpu) = aten::matmul(%1002, %1006), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.ffn # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1692:0\n",
       "  %x.63 : Float(1:307200, 100:3072, 3072:1, requires_grad=1, device=cpu) = aten::add_(%output.45, %1004, %105), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.ffn # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1694:0\n",
       "  %1009 : Float(1:307200, 100:3072, 3072:1, requires_grad=1, device=cpu) = aten::mul(%x.63, %88), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/activations.py:38:0\n",
       "  %1010 : Float(1:307200, 100:3072, 3072:1, requires_grad=1, device=cpu) = aten::mul(%x.63, %87), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/activations.py:38:0\n",
       "  %1011 : Float(1:307200, 100:3072, 3072:1, requires_grad=1, device=cpu) = aten::mul(%x.63, %86), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/activations.py:38:0\n",
       "  %1012 : Float(1:307200, 100:3072, 3072:1, requires_grad=1, device=cpu) = aten::mul(%1011, %x.63), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/activations.py:38:0\n",
       "  %1013 : Float(1:307200, 100:3072, 3072:1, requires_grad=1, device=cpu) = aten::add(%1012, %85, %105), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/activations.py:38:0\n",
       "  %1014 : Float(1:307200, 100:3072, 3072:1, requires_grad=1, device=cpu) = aten::mul(%1010, %1013), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/activations.py:38:0\n",
       "  %1015 : Float(1:307200, 100:3072, 3072:1, requires_grad=1, device=cpu) = aten::tanh(%1014), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/activations.py:38:0\n",
       "  %1016 : Float(1:307200, 100:3072, 3072:1, requires_grad=1, device=cpu) = aten::add(%1015, %85, %105), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/activations.py:38:0\n",
       "  %input.67 : Float(1:307200, 100:3072, 3072:1, requires_grad=1, device=cpu) = aten::mul(%1009, %1016), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/activations.py:38:0\n",
       "  %1018 : Tensor = prim::GetAttr[name=\"bias\"](%935)\n",
       "  %1019 : Tensor = prim::GetAttr[name=\"weight\"](%935)\n",
       "  %1020 : Float(3072:1, 768:3072, requires_grad=1, device=cpu) = aten::t(%1019), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1692:0\n",
       "  %output.46 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::matmul(%input.67, %1020), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1692:0\n",
       "  %ffn_output.9 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::add_(%output.46, %1018, %105), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1694:0\n",
       "  %input.68 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::add(%ffn_output.9, %1002, %105), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:363:0\n",
       "  %1024 : Tensor = prim::GetAttr[name=\"bias\"](%934)\n",
       "  %1025 : Tensor = prim::GetAttr[name=\"weight\"](%934)\n",
       "  %1026 : int[] = prim::ListConstruct(%100), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm\n",
       "  %input.69 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::layer_norm(%input.68, %1026, %1025, %1024, %104, %103), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:2094:0\n",
       "  %1028 : (Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu), Float(768:1, requires_grad=1, device=cpu)) = prim::TupleConstruct(%input.69, %1003)\n",
       "  %1029 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu), %1030 : Float(768:1, requires_grad=1, device=cpu) = prim::TupleUnpack(%1028)\n",
       "  %1031 : (Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu), Float(768:1, requires_grad=1, device=cpu)) = prim::TupleConstruct(%1029, %1030)\n",
       "  %1032 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu), %1033 : Float(768:1, requires_grad=1, device=cpu) = prim::TupleUnpack(%1031)\n",
       "  %1034 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name=\"albert_layers\"](%107)\n",
       "  %1035 : __torch__.transformers.modeling_albert.AlbertLayer = prim::GetAttr[name=\"0\"](%1034)\n",
       "  %1036 : __torch__.torch.nn.modules.normalization.___torch_mangle_2.LayerNorm = prim::GetAttr[name=\"full_layer_layer_norm\"](%1035)\n",
       "  %1037 : __torch__.torch.nn.modules.linear.___torch_mangle_11.Linear = prim::GetAttr[name=\"ffn_output\"](%1035)\n",
       "  %1038 : __torch__.torch.nn.modules.linear.___torch_mangle_10.Linear = prim::GetAttr[name=\"ffn\"](%1035)\n",
       "  %1039 : __torch__.transformers.modeling_albert.AlbertAttention = prim::GetAttr[name=\"attention\"](%1035)\n",
       "  %1040 : __torch__.torch.nn.modules.normalization.___torch_mangle_9.LayerNorm = prim::GetAttr[name=\"LayerNorm\"](%1039)\n",
       "  %1041 : __torch__.torch.nn.modules.linear.___torch_mangle_8.Linear = prim::GetAttr[name=\"dense\"](%1039)\n",
       "  %1042 : Tensor = prim::GetAttr[name=\"weight\"](%1041)\n",
       "  %1043 : __torch__.torch.nn.modules.linear.___torch_mangle_5.Linear = prim::GetAttr[name=\"value\"](%1039)\n",
       "  %1044 : __torch__.torch.nn.modules.linear.___torch_mangle_4.Linear = prim::GetAttr[name=\"key\"](%1039)\n",
       "  %1045 : __torch__.torch.nn.modules.linear.___torch_mangle_3.Linear = prim::GetAttr[name=\"query\"](%1039)\n",
       "  %1046 : Tensor = prim::GetAttr[name=\"bias\"](%1045)\n",
       "  %1047 : Tensor = prim::GetAttr[name=\"weight\"](%1045)\n",
       "  %1048 : Float(768:1, 768:768, requires_grad=1, device=cpu) = aten::t(%1047), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.query # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1692:0\n",
       "  %output.47 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::matmul(%1032, %1048), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.query # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1692:0\n",
       "  %x.64 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::add_(%output.47, %1046, %105), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.query # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1694:0\n",
       "  %1051 : Tensor = prim::GetAttr[name=\"bias\"](%1044)\n",
       "  %1052 : Tensor = prim::GetAttr[name=\"weight\"](%1044)\n",
       "  %1053 : Float(768:1, 768:768, requires_grad=1, device=cpu) = aten::t(%1052), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.key # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1692:0\n",
       "  %output.48 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::matmul(%1032, %1053), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.key # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1692:0\n",
       "  %x.66 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::add_(%output.48, %1051, %105), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.key # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1694:0\n",
       "  %1056 : Tensor = prim::GetAttr[name=\"bias\"](%1043)\n",
       "  %1057 : Tensor = prim::GetAttr[name=\"weight\"](%1043)\n",
       "  %1058 : Float(768:1, 768:768, requires_grad=1, device=cpu) = aten::t(%1057), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.value # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1692:0\n",
       "  %output.49 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::matmul(%1032, %1058), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.value # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1692:0\n",
       "  %x.68 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::add_(%output.49, %1056, %105), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.value # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1694:0\n",
       "  %1061 : int = aten::size(%x.64, %89), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:271:0\n",
       "  %1062 : int = aten::size(%x.64, %105), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:271:0\n",
       "  %1063 : int[] = prim::ListConstruct(%1061, %1062, %90, %91), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention\n",
       "  %x.65 : Float(1:76800, 100:768, 12:64, 64:1, requires_grad=1, device=cpu) = aten::view(%x.64, %1063), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:272:0\n",
       "  %1065 : int[] = prim::ListConstruct(%89, %92, %105, %93), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention\n",
       "  %query_layer.10 : Float(1:76800, 12:64, 100:768, 64:1, requires_grad=1, device=cpu) = aten::permute(%x.65, %1065), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:273:0\n",
       "  %1067 : int = aten::size(%x.66, %89), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:271:0\n",
       "  %1068 : int = aten::size(%x.66, %105), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:271:0\n",
       "  %1069 : int[] = prim::ListConstruct(%1067, %1068, %90, %91), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention\n",
       "  %x.67 : Float(1:76800, 100:768, 12:64, 64:1, requires_grad=1, device=cpu) = aten::view(%x.66, %1069), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:272:0\n",
       "  %1071 : int[] = prim::ListConstruct(%89, %92, %105, %93), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention\n",
       "  %key_layer.10 : Float(1:76800, 12:64, 100:768, 64:1, requires_grad=1, device=cpu) = aten::permute(%x.67, %1071), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:273:0\n",
       "  %1073 : int = aten::size(%x.68, %89), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:271:0\n",
       "  %1074 : int = aten::size(%x.68, %105), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:271:0\n",
       "  %1075 : int[] = prim::ListConstruct(%1073, %1074, %90, %91), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention\n",
       "  %x.69 : Float(1:76800, 100:768, 12:64, 64:1, requires_grad=1, device=cpu) = aten::view(%x.68, %1075), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:272:0\n",
       "  %1077 : int[] = prim::ListConstruct(%89, %92, %105, %93), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention\n",
       "  %value_layer.10 : Float(1:76800, 12:64, 100:768, 64:1, requires_grad=1, device=cpu) = aten::permute(%x.69, %1077), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:273:0\n",
       "  %1079 : Float(1:76800, 12:64, 64:1, 100:768, requires_grad=1, device=cpu) = aten::transpose(%key_layer.10, %94, %95), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:303:0\n",
       "  %attention_scores.19 : Float(1:120000, 12:10000, 100:100, 100:1, requires_grad=1, device=cpu) = aten::matmul(%query_layer.10, %1079), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:303:0\n",
       "  %attention_scores.20 : Float(1:120000, 12:10000, 100:100, 100:1, requires_grad=1, device=cpu) = aten::div(%attention_scores.19, %96), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:304:0\n",
       "  %input.70 : Float(1:120000, 12:10000, 100:100, 100:1, requires_grad=1, device=cpu) = aten::add(%attention_scores.20, %attention_mask, %105), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:307:0\n",
       "  %input.71 : Float(1:120000, 12:10000, 100:100, 100:1, requires_grad=1, device=cpu) = aten::softmax(%input.70, %94, %97), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1512:0\n",
       "  %attention_probs.10 : Float(1:120000, 12:10000, 100:100, 100:1, requires_grad=1, device=cpu) = aten::dropout(%input.71, %99, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.attention_dropout # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:983:0\n",
       "  %context_layer.10 : Float(1:76800, 12:6400, 100:64, 64:1, requires_grad=1, device=cpu) = aten::matmul(%attention_probs.10, %value_layer.10), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:320:0\n",
       "  %1086 : int[] = prim::ListConstruct(%89, %92, %105, %93), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention\n",
       "  %1087 : Float(1:76800, 100:64, 12:6400, 64:1, requires_grad=1, device=cpu) = aten::permute(%context_layer.10, %1086), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:322:0\n",
       "  %1088 : Float(1:76800, 100:768, 12:64, 64:1, requires_grad=1, device=cpu) = aten::contiguous(%1087, %89), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:322:0\n",
       "  %1089 : Float(768:1, 768:768, requires_grad=1, device=cpu) = aten::t(%1042), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:326:0\n",
       "  %1090 : int[] = prim::ListConstruct(%90, %91, %100), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention\n",
       "  %1091 : Float(12:64, 64:1, 768:768, requires_grad=1, device=cpu) = aten::view(%1089, %1090), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:326:0\n",
       "  %1092 : Float(12:64, 64:1, 768:768, requires_grad=1, device=cpu) = aten::to(%1091, %101, %98, %98, %97), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:326:0\n",
       "  %b.10 : Float(768:1, requires_grad=1, device=cpu) = aten::to(%1033, %101, %98, %98, %97), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:330:0\n",
       "  %1094 : Tensor[] = prim::ListConstruct(%1088, %1092), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention\n",
       "  %1095 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::einsum(%102, %1094), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/functional.py:344:0\n",
       "  %input.72 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::add(%1095, %b.10, %105), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:332:0\n",
       "  %projected_context_layer.10 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::dropout(%input.72, %99, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.output_dropout # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:983:0\n",
       "  %input.73 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::add(%1032, %projected_context_layer.10, %105), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:334:0\n",
       "  %1099 : Tensor = prim::GetAttr[name=\"bias\"](%1040)\n",
       "  %1100 : Tensor = prim::GetAttr[name=\"weight\"](%1040)\n",
       "  %1101 : int[] = prim::ListConstruct(%100), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm\n",
       "  %input_tensor.10 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::layer_norm(%input.73, %1101, %1100, %1099, %104, %103), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:2094:0\n",
       "  %1103 : (Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu), Float(768:1, requires_grad=1, device=cpu)) = prim::TupleConstruct(%input_tensor.10, %b.10)\n",
       "  %1104 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu), %1105 : Float(768:1, requires_grad=1, device=cpu) = prim::TupleUnpack(%1103)\n",
       "  %1106 : Tensor = prim::GetAttr[name=\"bias\"](%1038)\n",
       "  %1107 : Tensor = prim::GetAttr[name=\"weight\"](%1038)\n",
       "  %1108 : Float(768:1, 3072:768, requires_grad=1, device=cpu) = aten::t(%1107), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.ffn # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1692:0\n",
       "  %output.50 : Float(1:307200, 100:3072, 3072:1, requires_grad=1, device=cpu) = aten::matmul(%1104, %1108), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.ffn # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1692:0\n",
       "  %x.70 : Float(1:307200, 100:3072, 3072:1, requires_grad=1, device=cpu) = aten::add_(%output.50, %1106, %105), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.ffn # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1694:0\n",
       "  %1111 : Float(1:307200, 100:3072, 3072:1, requires_grad=1, device=cpu) = aten::mul(%x.70, %88), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/activations.py:38:0\n",
       "  %1112 : Float(1:307200, 100:3072, 3072:1, requires_grad=1, device=cpu) = aten::mul(%x.70, %87), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/activations.py:38:0\n",
       "  %1113 : Float(1:307200, 100:3072, 3072:1, requires_grad=1, device=cpu) = aten::mul(%x.70, %86), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/activations.py:38:0\n",
       "  %1114 : Float(1:307200, 100:3072, 3072:1, requires_grad=1, device=cpu) = aten::mul(%1113, %x.70), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/activations.py:38:0\n",
       "  %1115 : Float(1:307200, 100:3072, 3072:1, requires_grad=1, device=cpu) = aten::add(%1114, %85, %105), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/activations.py:38:0\n",
       "  %1116 : Float(1:307200, 100:3072, 3072:1, requires_grad=1, device=cpu) = aten::mul(%1112, %1115), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/activations.py:38:0\n",
       "  %1117 : Float(1:307200, 100:3072, 3072:1, requires_grad=1, device=cpu) = aten::tanh(%1116), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/activations.py:38:0\n",
       "  %1118 : Float(1:307200, 100:3072, 3072:1, requires_grad=1, device=cpu) = aten::add(%1117, %85, %105), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/activations.py:38:0\n",
       "  %input.74 : Float(1:307200, 100:3072, 3072:1, requires_grad=1, device=cpu) = aten::mul(%1111, %1118), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/activations.py:38:0\n",
       "  %1120 : Tensor = prim::GetAttr[name=\"bias\"](%1037)\n",
       "  %1121 : Tensor = prim::GetAttr[name=\"weight\"](%1037)\n",
       "  %1122 : Float(3072:1, 768:3072, requires_grad=1, device=cpu) = aten::t(%1121), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1692:0\n",
       "  %output.51 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::matmul(%input.74, %1122), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1692:0\n",
       "  %ffn_output.10 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::add_(%output.51, %1120, %105), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1694:0\n",
       "  %input.75 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::add(%ffn_output.10, %1104, %105), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:363:0\n",
       "  %1126 : Tensor = prim::GetAttr[name=\"bias\"](%1036)\n",
       "  %1127 : Tensor = prim::GetAttr[name=\"weight\"](%1036)\n",
       "  %1128 : int[] = prim::ListConstruct(%100), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm\n",
       "  %input.76 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::layer_norm(%input.75, %1128, %1127, %1126, %104, %103), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:2094:0\n",
       "  %1130 : (Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu), Float(768:1, requires_grad=1, device=cpu)) = prim::TupleConstruct(%input.76, %1105)\n",
       "  %1131 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu), %1132 : Float(768:1, requires_grad=1, device=cpu) = prim::TupleUnpack(%1130)\n",
       "  %1133 : (Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu), Float(768:1, requires_grad=1, device=cpu)) = prim::TupleConstruct(%1131, %1132)\n",
       "  %1134 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu), %1135 : Float(768:1, requires_grad=1, device=cpu) = prim::TupleUnpack(%1133)\n",
       "  %1136 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name=\"albert_layers\"](%107)\n",
       "  %1137 : __torch__.transformers.modeling_albert.AlbertLayer = prim::GetAttr[name=\"0\"](%1136)\n",
       "  %1138 : __torch__.torch.nn.modules.normalization.___torch_mangle_2.LayerNorm = prim::GetAttr[name=\"full_layer_layer_norm\"](%1137)\n",
       "  %1139 : __torch__.torch.nn.modules.linear.___torch_mangle_11.Linear = prim::GetAttr[name=\"ffn_output\"](%1137)\n",
       "  %1140 : __torch__.torch.nn.modules.linear.___torch_mangle_10.Linear = prim::GetAttr[name=\"ffn\"](%1137)\n",
       "  %1141 : __torch__.transformers.modeling_albert.AlbertAttention = prim::GetAttr[name=\"attention\"](%1137)\n",
       "  %1142 : __torch__.torch.nn.modules.normalization.___torch_mangle_9.LayerNorm = prim::GetAttr[name=\"LayerNorm\"](%1141)\n",
       "  %1143 : __torch__.torch.nn.modules.linear.___torch_mangle_8.Linear = prim::GetAttr[name=\"dense\"](%1141)\n",
       "  %1144 : Tensor = prim::GetAttr[name=\"weight\"](%1143)\n",
       "  %1145 : __torch__.torch.nn.modules.linear.___torch_mangle_5.Linear = prim::GetAttr[name=\"value\"](%1141)\n",
       "  %1146 : __torch__.torch.nn.modules.linear.___torch_mangle_4.Linear = prim::GetAttr[name=\"key\"](%1141)\n",
       "  %1147 : __torch__.torch.nn.modules.linear.___torch_mangle_3.Linear = prim::GetAttr[name=\"query\"](%1141)\n",
       "  %1148 : Tensor = prim::GetAttr[name=\"bias\"](%1147)\n",
       "  %1149 : Tensor = prim::GetAttr[name=\"weight\"](%1147)\n",
       "  %1150 : Float(768:1, 768:768, requires_grad=1, device=cpu) = aten::t(%1149), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.query # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1692:0\n",
       "  %output.52 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::matmul(%1134, %1150), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.query # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1692:0\n",
       "  %x.71 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::add_(%output.52, %1148, %105), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.query # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1694:0\n",
       "  %1153 : Tensor = prim::GetAttr[name=\"bias\"](%1146)\n",
       "  %1154 : Tensor = prim::GetAttr[name=\"weight\"](%1146)\n",
       "  %1155 : Float(768:1, 768:768, requires_grad=1, device=cpu) = aten::t(%1154), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.key # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1692:0\n",
       "  %output.53 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::matmul(%1134, %1155), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.key # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1692:0\n",
       "  %x.73 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::add_(%output.53, %1153, %105), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.key # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1694:0\n",
       "  %1158 : Tensor = prim::GetAttr[name=\"bias\"](%1145)\n",
       "  %1159 : Tensor = prim::GetAttr[name=\"weight\"](%1145)\n",
       "  %1160 : Float(768:1, 768:768, requires_grad=1, device=cpu) = aten::t(%1159), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.value # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1692:0\n",
       "  %output.54 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::matmul(%1134, %1160), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.value # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1692:0\n",
       "  %x.75 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::add_(%output.54, %1158, %105), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.value # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1694:0\n",
       "  %1163 : int = aten::size(%x.71, %89), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:271:0\n",
       "  %1164 : int = aten::size(%x.71, %105), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:271:0\n",
       "  %1165 : int[] = prim::ListConstruct(%1163, %1164, %90, %91), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention\n",
       "  %x.72 : Float(1:76800, 100:768, 12:64, 64:1, requires_grad=1, device=cpu) = aten::view(%x.71, %1165), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:272:0\n",
       "  %1167 : int[] = prim::ListConstruct(%89, %92, %105, %93), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention\n",
       "  %query_layer.11 : Float(1:76800, 12:64, 100:768, 64:1, requires_grad=1, device=cpu) = aten::permute(%x.72, %1167), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:273:0\n",
       "  %1169 : int = aten::size(%x.73, %89), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:271:0\n",
       "  %1170 : int = aten::size(%x.73, %105), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:271:0\n",
       "  %1171 : int[] = prim::ListConstruct(%1169, %1170, %90, %91), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention\n",
       "  %x.74 : Float(1:76800, 100:768, 12:64, 64:1, requires_grad=1, device=cpu) = aten::view(%x.73, %1171), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:272:0\n",
       "  %1173 : int[] = prim::ListConstruct(%89, %92, %105, %93), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention\n",
       "  %key_layer.11 : Float(1:76800, 12:64, 100:768, 64:1, requires_grad=1, device=cpu) = aten::permute(%x.74, %1173), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:273:0\n",
       "  %1175 : int = aten::size(%x.75, %89), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:271:0\n",
       "  %1176 : int = aten::size(%x.75, %105), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:271:0\n",
       "  %1177 : int[] = prim::ListConstruct(%1175, %1176, %90, %91), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention\n",
       "  %x.76 : Float(1:76800, 100:768, 12:64, 64:1, requires_grad=1, device=cpu) = aten::view(%x.75, %1177), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:272:0\n",
       "  %1179 : int[] = prim::ListConstruct(%89, %92, %105, %93), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention\n",
       "  %value_layer.11 : Float(1:76800, 12:64, 100:768, 64:1, requires_grad=1, device=cpu) = aten::permute(%x.76, %1179), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:273:0\n",
       "  %1181 : Float(1:76800, 12:64, 64:1, 100:768, requires_grad=1, device=cpu) = aten::transpose(%key_layer.11, %94, %95), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:303:0\n",
       "  %attention_scores.21 : Float(1:120000, 12:10000, 100:100, 100:1, requires_grad=1, device=cpu) = aten::matmul(%query_layer.11, %1181), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:303:0\n",
       "  %attention_scores.22 : Float(1:120000, 12:10000, 100:100, 100:1, requires_grad=1, device=cpu) = aten::div(%attention_scores.21, %96), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:304:0\n",
       "  %input.77 : Float(1:120000, 12:10000, 100:100, 100:1, requires_grad=1, device=cpu) = aten::add(%attention_scores.22, %attention_mask, %105), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:307:0\n",
       "  %input.78 : Float(1:120000, 12:10000, 100:100, 100:1, requires_grad=1, device=cpu) = aten::softmax(%input.77, %94, %97), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1512:0\n",
       "  %attention_probs.11 : Float(1:120000, 12:10000, 100:100, 100:1, requires_grad=1, device=cpu) = aten::dropout(%input.78, %99, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.attention_dropout # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:983:0\n",
       "  %context_layer.11 : Float(1:76800, 12:6400, 100:64, 64:1, requires_grad=1, device=cpu) = aten::matmul(%attention_probs.11, %value_layer.11), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:320:0\n",
       "  %1188 : int[] = prim::ListConstruct(%89, %92, %105, %93), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention\n",
       "  %1189 : Float(1:76800, 100:64, 12:6400, 64:1, requires_grad=1, device=cpu) = aten::permute(%context_layer.11, %1188), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:322:0\n",
       "  %1190 : Float(1:76800, 100:768, 12:64, 64:1, requires_grad=1, device=cpu) = aten::contiguous(%1189, %89), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:322:0\n",
       "  %1191 : Float(768:1, 768:768, requires_grad=1, device=cpu) = aten::t(%1144), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:326:0\n",
       "  %1192 : int[] = prim::ListConstruct(%90, %91, %100), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention\n",
       "  %1193 : Float(12:64, 64:1, 768:768, requires_grad=1, device=cpu) = aten::view(%1191, %1192), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:326:0\n",
       "  %1194 : Float(12:64, 64:1, 768:768, requires_grad=1, device=cpu) = aten::to(%1193, %101, %98, %98, %97), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:326:0\n",
       "  %b.11 : Float(768:1, requires_grad=1, device=cpu) = aten::to(%1135, %101, %98, %98, %97), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:330:0\n",
       "  %1196 : Tensor[] = prim::ListConstruct(%1190, %1194), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention\n",
       "  %1197 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::einsum(%102, %1196), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/functional.py:344:0\n",
       "  %input.79 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::add(%1197, %b.11, %105), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:332:0\n",
       "  %projected_context_layer.11 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::dropout(%input.79, %99, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.output_dropout # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:983:0\n",
       "  %input.80 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::add(%1134, %projected_context_layer.11, %105), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:334:0\n",
       "  %1201 : Tensor = prim::GetAttr[name=\"bias\"](%1142)\n",
       "  %1202 : Tensor = prim::GetAttr[name=\"weight\"](%1142)\n",
       "  %1203 : int[] = prim::ListConstruct(%100), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm\n",
       "  %input_tensor.11 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::layer_norm(%input.80, %1203, %1202, %1201, %104, %103), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:2094:0\n",
       "  %1205 : (Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu), Float(768:1, requires_grad=1, device=cpu)) = prim::TupleConstruct(%input_tensor.11, %b.11)\n",
       "  %1206 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu), %1207 : Float(768:1, requires_grad=1, device=cpu) = prim::TupleUnpack(%1205)\n",
       "  %1208 : Tensor = prim::GetAttr[name=\"bias\"](%1140)\n",
       "  %1209 : Tensor = prim::GetAttr[name=\"weight\"](%1140)\n",
       "  %1210 : Float(768:1, 3072:768, requires_grad=1, device=cpu) = aten::t(%1209), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.ffn # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1692:0\n",
       "  %output.55 : Float(1:307200, 100:3072, 3072:1, requires_grad=1, device=cpu) = aten::matmul(%1206, %1210), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.ffn # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1692:0\n",
       "  %x.77 : Float(1:307200, 100:3072, 3072:1, requires_grad=1, device=cpu) = aten::add_(%output.55, %1208, %105), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.ffn # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1694:0\n",
       "  %1213 : Float(1:307200, 100:3072, 3072:1, requires_grad=1, device=cpu) = aten::mul(%x.77, %88), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/activations.py:38:0\n",
       "  %1214 : Float(1:307200, 100:3072, 3072:1, requires_grad=1, device=cpu) = aten::mul(%x.77, %87), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/activations.py:38:0\n",
       "  %1215 : Float(1:307200, 100:3072, 3072:1, requires_grad=1, device=cpu) = aten::mul(%x.77, %86), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/activations.py:38:0\n",
       "  %1216 : Float(1:307200, 100:3072, 3072:1, requires_grad=1, device=cpu) = aten::mul(%1215, %x.77), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/activations.py:38:0\n",
       "  %1217 : Float(1:307200, 100:3072, 3072:1, requires_grad=1, device=cpu) = aten::add(%1216, %85, %105), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/activations.py:38:0\n",
       "  %1218 : Float(1:307200, 100:3072, 3072:1, requires_grad=1, device=cpu) = aten::mul(%1214, %1217), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/activations.py:38:0\n",
       "  %1219 : Float(1:307200, 100:3072, 3072:1, requires_grad=1, device=cpu) = aten::tanh(%1218), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/activations.py:38:0\n",
       "  %1220 : Float(1:307200, 100:3072, 3072:1, requires_grad=1, device=cpu) = aten::add(%1219, %85, %105), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/activations.py:38:0\n",
       "  %input.81 : Float(1:307200, 100:3072, 3072:1, requires_grad=1, device=cpu) = aten::mul(%1213, %1220), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/activations.py:38:0\n",
       "  %1222 : Tensor = prim::GetAttr[name=\"bias\"](%1139)\n",
       "  %1223 : Tensor = prim::GetAttr[name=\"weight\"](%1139)\n",
       "  %1224 : Float(3072:1, 768:3072, requires_grad=1, device=cpu) = aten::t(%1223), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1692:0\n",
       "  %output.56 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::matmul(%input.81, %1224), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1692:0\n",
       "  %ffn_output.11 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::add_(%output.56, %1222, %105), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1694:0\n",
       "  %input.82 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::add(%ffn_output.11, %1206, %105), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:363:0\n",
       "  %1228 : Tensor = prim::GetAttr[name=\"bias\"](%1138)\n",
       "  %1229 : Tensor = prim::GetAttr[name=\"weight\"](%1138)\n",
       "  %1230 : int[] = prim::ListConstruct(%100), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm\n",
       "  %input.83 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::layer_norm(%input.82, %1230, %1229, %1228, %104, %103), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:2094:0\n",
       "  %1232 : (Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu), Float(768:1, requires_grad=1, device=cpu)) = prim::TupleConstruct(%input.83, %1207)\n",
       "  %1233 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu), %1234 : Float(768:1, requires_grad=1, device=cpu) = prim::TupleUnpack(%1232)\n",
       "  %1235 : (Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu), Float(768:1, requires_grad=1, device=cpu)) = prim::TupleConstruct(%1233, %1234)\n",
       "  %1236 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu), %1237 : Float(768:1, requires_grad=1, device=cpu) = prim::TupleUnpack(%1235)\n",
       "  %1238 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name=\"albert_layers\"](%107)\n",
       "  %1239 : __torch__.transformers.modeling_albert.AlbertLayer = prim::GetAttr[name=\"0\"](%1238)\n",
       "  %1240 : __torch__.torch.nn.modules.normalization.___torch_mangle_2.LayerNorm = prim::GetAttr[name=\"full_layer_layer_norm\"](%1239)\n",
       "  %1241 : __torch__.torch.nn.modules.linear.___torch_mangle_11.Linear = prim::GetAttr[name=\"ffn_output\"](%1239)\n",
       "  %1242 : __torch__.torch.nn.modules.linear.___torch_mangle_10.Linear = prim::GetAttr[name=\"ffn\"](%1239)\n",
       "  %1243 : __torch__.transformers.modeling_albert.AlbertAttention = prim::GetAttr[name=\"attention\"](%1239)\n",
       "  %1244 : __torch__.torch.nn.modules.normalization.___torch_mangle_9.LayerNorm = prim::GetAttr[name=\"LayerNorm\"](%1243)\n",
       "  %1245 : __torch__.torch.nn.modules.linear.___torch_mangle_8.Linear = prim::GetAttr[name=\"dense\"](%1243)\n",
       "  %1246 : Tensor = prim::GetAttr[name=\"weight\"](%1245)\n",
       "  %1247 : __torch__.torch.nn.modules.linear.___torch_mangle_5.Linear = prim::GetAttr[name=\"value\"](%1243)\n",
       "  %1248 : __torch__.torch.nn.modules.linear.___torch_mangle_4.Linear = prim::GetAttr[name=\"key\"](%1243)\n",
       "  %1249 : __torch__.torch.nn.modules.linear.___torch_mangle_3.Linear = prim::GetAttr[name=\"query\"](%1243)\n",
       "  %1250 : Tensor = prim::GetAttr[name=\"bias\"](%1249)\n",
       "  %1251 : Tensor = prim::GetAttr[name=\"weight\"](%1249)\n",
       "  %1252 : Float(768:1, 768:768, requires_grad=1, device=cpu) = aten::t(%1251), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.query # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1692:0\n",
       "  %output.57 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::matmul(%1236, %1252), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.query # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1692:0\n",
       "  %x.78 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::add_(%output.57, %1250, %105), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.query # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1694:0\n",
       "  %1255 : Tensor = prim::GetAttr[name=\"bias\"](%1248)\n",
       "  %1256 : Tensor = prim::GetAttr[name=\"weight\"](%1248)\n",
       "  %1257 : Float(768:1, 768:768, requires_grad=1, device=cpu) = aten::t(%1256), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.key # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1692:0\n",
       "  %output.58 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::matmul(%1236, %1257), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.key # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1692:0\n",
       "  %x.80 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::add_(%output.58, %1255, %105), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.key # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1694:0\n",
       "  %1260 : Tensor = prim::GetAttr[name=\"bias\"](%1247)\n",
       "  %1261 : Tensor = prim::GetAttr[name=\"weight\"](%1247)\n",
       "  %1262 : Float(768:1, 768:768, requires_grad=1, device=cpu) = aten::t(%1261), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.value # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1692:0\n",
       "  %output.59 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::matmul(%1236, %1262), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.value # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1692:0\n",
       "  %x.82 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::add_(%output.59, %1260, %105), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.value # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1694:0\n",
       "  %1265 : int = aten::size(%x.78, %89), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:271:0\n",
       "  %1266 : int = aten::size(%x.78, %105), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:271:0\n",
       "  %1267 : int[] = prim::ListConstruct(%1265, %1266, %90, %91), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention\n",
       "  %x.79 : Float(1:76800, 100:768, 12:64, 64:1, requires_grad=1, device=cpu) = aten::view(%x.78, %1267), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:272:0\n",
       "  %1269 : int[] = prim::ListConstruct(%89, %92, %105, %93), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention\n",
       "  %query_layer : Float(1:76800, 12:64, 100:768, 64:1, requires_grad=1, device=cpu) = aten::permute(%x.79, %1269), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:273:0\n",
       "  %1271 : int = aten::size(%x.80, %89), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:271:0\n",
       "  %1272 : int = aten::size(%x.80, %105), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:271:0\n",
       "  %1273 : int[] = prim::ListConstruct(%1271, %1272, %90, %91), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention\n",
       "  %x.81 : Float(1:76800, 100:768, 12:64, 64:1, requires_grad=1, device=cpu) = aten::view(%x.80, %1273), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:272:0\n",
       "  %1275 : int[] = prim::ListConstruct(%89, %92, %105, %93), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention\n",
       "  %key_layer : Float(1:76800, 12:64, 100:768, 64:1, requires_grad=1, device=cpu) = aten::permute(%x.81, %1275), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:273:0\n",
       "  %1277 : int = aten::size(%x.82, %89), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:271:0\n",
       "  %1278 : int = aten::size(%x.82, %105), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:271:0\n",
       "  %1279 : int[] = prim::ListConstruct(%1277, %1278, %90, %91), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention\n",
       "  %x.83 : Float(1:76800, 100:768, 12:64, 64:1, requires_grad=1, device=cpu) = aten::view(%x.82, %1279), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:272:0\n",
       "  %1281 : int[] = prim::ListConstruct(%89, %92, %105, %93), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention\n",
       "  %value_layer : Float(1:76800, 12:64, 100:768, 64:1, requires_grad=1, device=cpu) = aten::permute(%x.83, %1281), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:273:0\n",
       "  %1283 : Float(1:76800, 12:64, 64:1, 100:768, requires_grad=1, device=cpu) = aten::transpose(%key_layer, %94, %95), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:303:0\n",
       "  %attention_scores.23 : Float(1:120000, 12:10000, 100:100, 100:1, requires_grad=1, device=cpu) = aten::matmul(%query_layer, %1283), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:303:0\n",
       "  %attention_scores : Float(1:120000, 12:10000, 100:100, 100:1, requires_grad=1, device=cpu) = aten::div(%attention_scores.23, %96), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:304:0\n",
       "  %input.84 : Float(1:120000, 12:10000, 100:100, 100:1, requires_grad=1, device=cpu) = aten::add(%attention_scores, %attention_mask, %105), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:307:0\n",
       "  %input.85 : Float(1:120000, 12:10000, 100:100, 100:1, requires_grad=1, device=cpu) = aten::softmax(%input.84, %94, %97), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1512:0\n",
       "  %attention_probs : Float(1:120000, 12:10000, 100:100, 100:1, requires_grad=1, device=cpu) = aten::dropout(%input.85, %99, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.attention_dropout # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:983:0\n",
       "  %context_layer : Float(1:76800, 12:6400, 100:64, 64:1, requires_grad=1, device=cpu) = aten::matmul(%attention_probs, %value_layer), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:320:0\n",
       "  %1290 : int[] = prim::ListConstruct(%89, %92, %105, %93), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention\n",
       "  %1291 : Float(1:76800, 100:64, 12:6400, 64:1, requires_grad=1, device=cpu) = aten::permute(%context_layer, %1290), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:322:0\n",
       "  %1292 : Float(1:76800, 100:768, 12:64, 64:1, requires_grad=1, device=cpu) = aten::contiguous(%1291, %89), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:322:0\n",
       "  %1293 : Float(768:1, 768:768, requires_grad=1, device=cpu) = aten::t(%1246), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:326:0\n",
       "  %1294 : int[] = prim::ListConstruct(%90, %91, %100), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention\n",
       "  %1295 : Float(12:64, 64:1, 768:768, requires_grad=1, device=cpu) = aten::view(%1293, %1294), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:326:0\n",
       "  %1296 : Float(12:64, 64:1, 768:768, requires_grad=1, device=cpu) = aten::to(%1295, %101, %98, %98, %97), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:326:0\n",
       "  %b : Float(768:1, requires_grad=1, device=cpu) = aten::to(%1237, %101, %98, %98, %97), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:330:0\n",
       "  %1298 : Tensor[] = prim::ListConstruct(%1292, %1296), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention\n",
       "  %1299 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::einsum(%102, %1298), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/functional.py:344:0\n",
       "  %input.86 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::add(%1299, %b, %105), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:332:0\n",
       "  %projected_context_layer : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::dropout(%input.86, %99, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.output_dropout # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:983:0\n",
       "  %input.87 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::add(%1236, %projected_context_layer, %105), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:334:0\n",
       "  %1303 : Tensor = prim::GetAttr[name=\"bias\"](%1244)\n",
       "  %1304 : Tensor = prim::GetAttr[name=\"weight\"](%1244)\n",
       "  %1305 : int[] = prim::ListConstruct(%100), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm\n",
       "  %input_tensor : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::layer_norm(%input.87, %1305, %1304, %1303, %104, %103), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:2094:0\n",
       "  %1307 : Tensor = prim::GetAttr[name=\"bias\"](%1242)\n",
       "  %1308 : Tensor = prim::GetAttr[name=\"weight\"](%1242)\n",
       "  %1309 : Float(768:1, 3072:768, requires_grad=1, device=cpu) = aten::t(%1308), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.ffn # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1692:0\n",
       "  %output.60 : Float(1:307200, 100:3072, 3072:1, requires_grad=1, device=cpu) = aten::matmul(%input_tensor, %1309), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.ffn # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1692:0\n",
       "  %x : Float(1:307200, 100:3072, 3072:1, requires_grad=1, device=cpu) = aten::add_(%output.60, %1307, %105), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.ffn # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1694:0\n",
       "  %1312 : Float(1:307200, 100:3072, 3072:1, requires_grad=1, device=cpu) = aten::mul(%x, %88), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/activations.py:38:0\n",
       "  %1313 : Float(1:307200, 100:3072, 3072:1, requires_grad=1, device=cpu) = aten::mul(%x, %87), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/activations.py:38:0\n",
       "  %1314 : Float(1:307200, 100:3072, 3072:1, requires_grad=1, device=cpu) = aten::mul(%x, %86), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/activations.py:38:0\n",
       "  %1315 : Float(1:307200, 100:3072, 3072:1, requires_grad=1, device=cpu) = aten::mul(%1314, %x), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/activations.py:38:0\n",
       "  %1316 : Float(1:307200, 100:3072, 3072:1, requires_grad=1, device=cpu) = aten::add(%1315, %85, %105), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/activations.py:38:0\n",
       "  %1317 : Float(1:307200, 100:3072, 3072:1, requires_grad=1, device=cpu) = aten::mul(%1313, %1316), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/activations.py:38:0\n",
       "  %1318 : Float(1:307200, 100:3072, 3072:1, requires_grad=1, device=cpu) = aten::tanh(%1317), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/activations.py:38:0\n",
       "  %1319 : Float(1:307200, 100:3072, 3072:1, requires_grad=1, device=cpu) = aten::add(%1318, %85, %105), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/activations.py:38:0\n",
       "  %input.88 : Float(1:307200, 100:3072, 3072:1, requires_grad=1, device=cpu) = aten::mul(%1312, %1319), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/activations.py:38:0\n",
       "  %1321 : Tensor = prim::GetAttr[name=\"bias\"](%1241)\n",
       "  %1322 : Tensor = prim::GetAttr[name=\"weight\"](%1241)\n",
       "  %1323 : Float(3072:1, 768:3072, requires_grad=1, device=cpu) = aten::t(%1322), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1692:0\n",
       "  %output : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::matmul(%input.88, %1323), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1692:0\n",
       "  %ffn_output : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::add_(%output, %1321, %105), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1694:0\n",
       "  %input.89 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::add(%ffn_output, %input_tensor, %105), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:363:0\n",
       "  %1327 : Tensor = prim::GetAttr[name=\"bias\"](%1240)\n",
       "  %1328 : Tensor = prim::GetAttr[name=\"weight\"](%1240)\n",
       "  %1329 : int[] = prim::ListConstruct(%100), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm\n",
       "  %sequence_output : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::layer_norm(%input.89, %1329, %1328, %1327, %104, %103), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:2094:0\n",
       "  %44 : int = prim::Constant[value=0]() # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:694:0\n",
       "  %45 : int = prim::Constant[value=0]() # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:694:0\n",
       "  %46 : int = prim::Constant[value=9223372036854775807]() # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:694:0\n",
       "  %47 : int = prim::Constant[value=1]() # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:694:0\n",
       "  %48 : Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu) = aten::slice(%sequence_output, %44, %45, %46, %47) # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:694:0\n",
       "  %49 : int = prim::Constant[value=1]() # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:694:0\n",
       "  %50 : int = prim::Constant[value=0]() # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:694:0\n",
       "  %input.90 : Float(1:76800, 768:1, requires_grad=1, device=cpu) = aten::select(%48, %49, %50) # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_albert.py:694:0\n",
       "  %1331 : int = prim::Constant[value=1](), scope: __module.pooler # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1690:0\n",
       "  %1332 : Tensor = prim::GetAttr[name=\"bias\"](%3)\n",
       "  %1333 : Tensor = prim::GetAttr[name=\"weight\"](%3)\n",
       "  %1334 : Float(768:1, 768:768, requires_grad=1, device=cpu) = aten::t(%1333), scope: __module.pooler # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1690:0\n",
       "  %input : Float(1:768, 768:1, requires_grad=1, device=cpu) = aten::addmm(%1332, %input.90, %1334, %1331, %1331), scope: __module.pooler # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1690:0\n",
       "  %1336 : Float(1:768, 768:1, requires_grad=1, device=cpu) = aten::tanh(%input), scope: __module.pooler_activation # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/modules/activation.py:359:0\n",
       "  %54 : (Float(1:76800, 100:768, 768:1, requires_grad=1, device=cpu), Float(1:768, 768:1, requires_grad=1, device=cpu)) = prim::TupleConstruct(%sequence_output, %1336)\n",
       "  return (%54)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ni=list(graph.nodes())\n",
    "ni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ni[0].output().node()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ni[0].output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ni[72].output() # build all input leaf data nodes, then construct the opnode with input and output nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ni[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(ni[0].outputs())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(ni[0].inputs())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pn=ni[0].input().node()\n",
    "pn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gi=list(graph.inputs())\n",
    "gi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(gi[0].node().outputs())[0].type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(gi[1].node().outputs())[1].type().sizes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'encoder/encoder.layer.0/encoder.layer.0.attention/encoder.layer.0.attention.output'.split('/')[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pn)\n",
    "print(gi[1].node())\n",
    "pn==gi[0].node()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[i.user for i in gi[0].uses()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "go=list(graph.outputs())\n",
    "go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphviz import Digraph\n",
    "\n",
    "g = Digraph('G')\n",
    "\n",
    "c0 = Digraph('cluster_0')\n",
    "c0.body.append('style=filled')\n",
    "c0.body.append('color=lightgrey')\n",
    "c0.node_attr.update(style='filled', color='white')\n",
    "c0.edges([('a0', 'a1'), ('a1', 'a2'), ('a2', 'a3')])\n",
    "c0.body.append('label =\"process #1\"')\n",
    "\n",
    "c2 = Digraph('cluster_0')\n",
    "c2.body.append('style=filled')\n",
    "c2.body.append('color=lightgrey')\n",
    "c2.node_attr.update(style='filled', color='white')\n",
    "c2.edges([('c0', 'c1'), ('c1', 'c2'), ('c2', 'c3')])\n",
    "c2.body.append('label =\"process #3\"')\n",
    "c2.body.append('color=lightblue')\n",
    "c0.subgraph(c2)\n",
    "\n",
    "c1 = Digraph('cluster_1')\n",
    "c1.node_attr.update(style='filled')\n",
    "c1.edges([('b0', 'b1'), ('b1', 'b2'), ('b2', 'b3')])\n",
    "c1.body.append('label =\"process #2\"')\n",
    "c1.body.append('color=blue')\n",
    "\n",
    "g.subgraph(c0)\n",
    "g.subgraph(c1)\n",
    "\n",
    "g.edge('start', 'a0')\n",
    "g.edge('start', 'b0')\n",
    "g.edge('a1', 'b3')\n",
    "g.edge('b2', 'a3')\n",
    "g.edge('a3', 'a0')\n",
    "g.edge('a3', 'end')\n",
    "g.edge('b3', 'end')\n",
    "\n",
    "g.node('start', shape='Mdiamond')\n",
    "g.node('end', shape='Msquare')\n",
    "\n",
    "g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sub_scopes='encoder.layer.0.attention.output.LayerNorm'.split('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'true' if '' and len(''.split('.')) else 'false'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_scopes='encoder.layer.0.attention.output.LayerNorm'.split('.')\n",
    "for si in range(len(sub_scopes), 0, -1):\n",
    "    p_scope = '.'.join(sub_scopes[0:si-1])\n",
    "    child_scope = '.'.join(sub_scopes[0:si])\n",
    "    print(si, p_scope, child_scope)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
