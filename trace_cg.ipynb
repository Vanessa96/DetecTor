{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# import autoreload\n",
    "# ?autoreload\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import torch\n",
    "from transformers import BertModel\n",
    "from transformers import AutoConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78430d57071647a2813e4ae3d2131160",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=433.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# config = AutoConfig.from_pretrained(\"prajjwal1/bert-tiny\")\n",
    "config = AutoConfig.from_pretrained(\"bert-base-uncased\")\n",
    "config.hidden_act = 'gelu_fast'\n",
    "config.torchscript = True\n",
    "model = BertModel(config)\n",
    "inputs = torch.randint(1000, size=(1, 100)).long()\n",
    "# model()\n",
    "# with torch.onnx.select_model_mode_for_export(model, False):\n",
    "  # trace, _ = torch.jit._get_trace_graph(model, args=(inputs,))\n",
    "#     trace = torch.jit.trace(model, (inputs, ))\n",
    "mo=model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;33mproperty:\u001b[0m\n",
       "    \u001b[0;36mT_destination\u001b[0m\u001b[1;30m, \u001b[0m\u001b[0;36m_backward_hooks\u001b[0m\u001b[1;30m, \u001b[0m\u001b[0;36m_buffers\u001b[0m\u001b[1;30m, \u001b[0m\u001b[0;36m_forward_hooks\u001b[0m\u001b[1;30m, \u001b[0m\u001b[0;36m_forward_pre_hooks\u001b[0m\u001b[1;30m, \u001b[0m\u001b[0;36m_load_state_dict_pre_hooks\u001b[0m\u001b[1;30m, \u001b[0m\u001b[0;36m_modules\u001b[0m\u001b[1;30m, \u001b[0m\u001b[0;36m_non_persistent_buffers_set\u001b[0m\u001b[1;30m, \u001b[0m\u001b[0;36m_parameters\u001b[0m\u001b[1;30m, \u001b[0m\u001b[0;36m_state_dict_hooks\u001b[0m\u001b[1;30m, \u001b[0m\u001b[0;36m_version\u001b[0m\u001b[1;30m, \u001b[0m\u001b[0;36mauthorized_missing_keys\u001b[0m\u001b[1;30m, \u001b[0m\u001b[0;36mauthorized_unexpected_keys\u001b[0m\u001b[1;30m, \u001b[0m\u001b[0;36mbase_model_prefix\u001b[0m\u001b[1;30m, \u001b[0m\u001b[0;36mconfig\u001b[0m\u001b[1;30m, \u001b[0m\u001b[0;36mdump_patches\u001b[0m\u001b[1;30m, \u001b[0m\u001b[0;36membeddings\u001b[0m\u001b[1;30m, \u001b[0m\u001b[0;36mencoder\u001b[0m\u001b[1;30m, \u001b[0m\u001b[0;36mkeys_to_never_save\u001b[0m\u001b[1;30m, \u001b[0m\u001b[0;36mname_or_path\u001b[0m\u001b[1;30m, \u001b[0m\u001b[0;36mpooler\u001b[0m\u001b[1;30m, \u001b[0m\u001b[0;36mtraining\u001b[0m\n",
       "\u001b[0;33mspecial attribute:\u001b[0m\n",
       "    \u001b[0;36m__annotations__\u001b[0m\u001b[1;30m, \u001b[0m\u001b[0;36m__class__\u001b[0m\u001b[1;30m, \u001b[0m\u001b[0;36m__dict__\u001b[0m\u001b[1;30m, \u001b[0m\u001b[0;36m__doc__\u001b[0m\u001b[1;30m, \u001b[0m\u001b[0;36m__module__\u001b[0m\u001b[1;30m, \u001b[0m\u001b[0;36m__weakref__\u001b[0m\n",
       "\u001b[0;33mabstract class:\u001b[0m\n",
       "    \u001b[0;36m__subclasshook__\u001b[0m\n",
       "\u001b[0;33mobject customization:\u001b[0m\n",
       "    \u001b[0;36m__format__\u001b[0m\u001b[1;30m, \u001b[0m\u001b[0;36m__hash__\u001b[0m\u001b[1;30m, \u001b[0m\u001b[0;36m__init__\u001b[0m\u001b[1;30m, \u001b[0m\u001b[0;36m__new__\u001b[0m\u001b[1;30m, \u001b[0m\u001b[0;36m__repr__\u001b[0m\u001b[1;30m, \u001b[0m\u001b[0;36m__sizeof__\u001b[0m\u001b[1;30m, \u001b[0m\u001b[0;36m__str__\u001b[0m\n",
       "\u001b[0;33mrich comparison:\u001b[0m\n",
       "    \u001b[0;36m__eq__\u001b[0m\u001b[1;30m, \u001b[0m\u001b[0;36m__ge__\u001b[0m\u001b[1;30m, \u001b[0m\u001b[0;36m__gt__\u001b[0m\u001b[1;30m, \u001b[0m\u001b[0;36m__le__\u001b[0m\u001b[1;30m, \u001b[0m\u001b[0;36m__lt__\u001b[0m\u001b[1;30m, \u001b[0m\u001b[0;36m__ne__\u001b[0m\n",
       "\u001b[0;33mattribute access:\u001b[0m\n",
       "    \u001b[0;36m__delattr__\u001b[0m\u001b[1;30m, \u001b[0m\u001b[0;36m__dir__\u001b[0m\u001b[1;30m, \u001b[0m\u001b[0;36m__getattr__\u001b[0m\u001b[1;30m, \u001b[0m\u001b[0;36m__getattribute__\u001b[0m\u001b[1;30m, \u001b[0m\u001b[0;36m__setattr__\u001b[0m\n",
       "\u001b[0;33mclass customization:\u001b[0m\n",
       "    \u001b[0;36m__init_subclass__\u001b[0m\n",
       "\u001b[0;33mpickle:\u001b[0m\n",
       "    \u001b[0;36m__reduce__\u001b[0m\u001b[1;30m, \u001b[0m\u001b[0;36m__reduce_ex__\u001b[0m\u001b[1;30m, \u001b[0m\u001b[0;36m__setstate__\u001b[0m\n",
       "\u001b[0;33mdescriptor:\u001b[0m\n",
       "    \u001b[0;36mbase_model\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30m@property with getter, :obj:`torch.nn.Module`: The main body of the model.\u001b[0m\n",
       "    \u001b[0;36mdevice\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30m@property with getter, :obj:`torch.device`: The device on which the module is (assuming that all the module parameters are on the same\u001b[0m\n",
       "    \u001b[0;36mdtype\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30m@property with getter, :obj:`torch.dtype`: The dtype of the module (assuming that all the module parameters have the same dtype).\u001b[0m\n",
       "    \u001b[0;36mdummy_inputs\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30m@property with getter, :obj:`Dict[str, torch.Tensor]`: Dummy inputs to do a forward pass in the network.\u001b[0m\n",
       "    \u001b[0;36mfrom_pretrained\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30mclass classmethod with getter, classmethod(function) -> method\u001b[0m\n",
       "\u001b[0;33mstatic method:\u001b[0m\n",
       "    \u001b[0;36m_expand_inputs_for_generation\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30mstaticmethod(function) -> method\u001b[0m\n",
       "    \u001b[0;36m_hook_rss_memory_post_forward\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30mstaticmethod(function) -> method\u001b[0m\n",
       "    \u001b[0;36m_hook_rss_memory_pre_forward\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30mstaticmethod(function) -> method\u001b[0m\n",
       "    \u001b[0;36m_init_sequence_length_for_generation\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30mstaticmethod(function) -> method\u001b[0m\n",
       "    \u001b[0;36m_reorder_cache\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30mstaticmethod(function) -> method\u001b[0m\n",
       "    \u001b[0;36m_tie_encoder_decoder_weights\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30mstaticmethod(function) -> method\u001b[0m\n",
       "    \u001b[0;36m_update_model_kwargs_for_generation\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30mstaticmethod(function) -> method\u001b[0m\n",
       "    \u001b[0;36m_update_seq_length_for_generation\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30mstaticmethod(function) -> method\u001b[0m\n",
       "\u001b[0;33mclass:\u001b[0m\n",
       "    \u001b[0;36mconfig_class\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30mThis is the configuration class to store the configuration of a :class:`~transformers.BertModel` or a\u001b[0m\n",
       "\u001b[0;33mfunction:\u001b[0m\n",
       "    \u001b[0;36m_apply\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30m\u001b[0m\n",
       "    \u001b[0;36m_call_impl\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30m\u001b[0m\n",
       "    \u001b[0;36m_convert_head_mask_to_5d\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30m-> [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[0m\n",
       "    \u001b[0;36m_forward_unimplemented\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30m\u001b[0m\n",
       "    \u001b[0;36m_get_decoder_start_token_id\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30m\u001b[0m\n",
       "    \u001b[0;36m_get_logits_processor\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30mThis class returns a :obj:`~transformers.LogitsProcessorList` list object that contains all relevant\u001b[0m\n",
       "    \u001b[0;36m_get_logits_warper\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30mThis class returns a :obj:`~transformers.LogitsProcessorList` list object that contains all relevant\u001b[0m\n",
       "    \u001b[0;36m_get_name\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30m\u001b[0m\n",
       "    \u001b[0;36m_get_pad_token_id\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30m\u001b[0m\n",
       "    \u001b[0;36m_get_resized_embeddings\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30mBuild a resized Embedding Module from a provided token Embedding Module. Increasing the size will add newly\u001b[0m\n",
       "    \u001b[0;36m_init_weights\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30mInitialize the weights \u001b[0m\n",
       "    \u001b[0;36m_load_from_state_dict\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30mCopies parameters and buffers from :attr:`state_dict` into only\u001b[0m\n",
       "    \u001b[0;36m_named_members\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30mHelper method for yielding various names + members of modules.\u001b[0m\n",
       "    \u001b[0;36m_prepare_attention_mask_for_generation\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30m\u001b[0m\n",
       "    \u001b[0;36m_prepare_decoder_input_ids_for_generation\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30m\u001b[0m\n",
       "    \u001b[0;36m_prepare_encoder_decoder_kwargs_for_generation\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30m\u001b[0m\n",
       "    \u001b[0;36m_prepare_input_ids_for_generation\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30m\u001b[0m\n",
       "    \u001b[0;36m_prune_heads\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30mPrunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\u001b[0m\n",
       "    \u001b[0;36m_register_load_state_dict_pre_hook\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30mThese hooks will be called with arguments: `state_dict`, `prefix`,\u001b[0m\n",
       "    \u001b[0;36m_register_state_dict_hook\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30mThese hooks will be called with arguments: `self`, `state_dict`,\u001b[0m\n",
       "    \u001b[0;36m_replicate_for_data_parallel\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30m\u001b[0m\n",
       "    \u001b[0;36m_resize_token_embeddings\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30m\u001b[0m\n",
       "    \u001b[0;36m_save_to_state_dict\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30mSaves module state to `destination` dictionary, containing a state\u001b[0m\n",
       "    \u001b[0;36m_slow_forward\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30m\u001b[0m\n",
       "    \u001b[0;36m_tie_or_clone_weights\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30mTie or clone module weights depending of whether we are using TorchScript or not\u001b[0m\n",
       "    \u001b[0;36madd_memory_hooks\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30mAdd a memory hook before and after each sub-module forward pass to record increase in memory consumption.\u001b[0m\n",
       "    \u001b[0;36madd_module\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30mAdds a child module to the current module.\u001b[0m\n",
       "    \u001b[0;36madjust_logits_during_generation\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30mImplement in subclasses of :class:`~transformers.PreTrainedModel` for custom behavior to adjust the logits in\u001b[0m\n",
       "    \u001b[0;36mapply\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30mApplies ``fn`` recursively to every submodule (as returned by ``.children()``)\u001b[0m\n",
       "    \u001b[0;36mbeam_sample\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30mGenerates sequences for models with a language modeling head using beam search with multinomial sampling.\u001b[0m\n",
       "    \u001b[0;36mbeam_search\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30mGenerates sequences for models with a language modeling head using beam search decoding.\u001b[0m\n",
       "    \u001b[0;36mbfloat16\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30mCasts all floating point parameters and buffers to ``bfloat16`` datatype.\u001b[0m\n",
       "    \u001b[0;36mbuffers\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30mReturns an iterator over module buffers.\u001b[0m\n",
       "    \u001b[0;36mchildren\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30mReturns an iterator over immediate children modules.\u001b[0m\n",
       "    \u001b[0;36mcpu\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30mMoves all model parameters and buffers to the CPU.\u001b[0m\n",
       "    \u001b[0;36mcuda\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30mMoves all model parameters and buffers to the GPU.\u001b[0m\n",
       "    \u001b[0;36mdouble\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30mCasts all floating point parameters and buffers to ``double`` datatype.\u001b[0m\n",
       "    \u001b[0;36mestimate_tokens\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30mHelper function to estimate the total number of tokens from the model inputs.\u001b[0m\n",
       "    \u001b[0;36meval\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30mSets the module in evaluation mode.\u001b[0m\n",
       "    \u001b[0;36mextra_repr\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30mSet the extra representation of the module\u001b[0m\n",
       "    \u001b[0;36mfloat\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30mCasts all floating point parameters and buffers to float datatype.\u001b[0m\n",
       "    \u001b[0;36mfloating_point_ops\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30mGet number of (optionally, non-embeddings) floating-point operations for the forward and backward passes of a\u001b[0m\n",
       "    \u001b[0;36mforward\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30mThe :class:`~transformers.BertModel` forward method, overrides the :func:`__call__` special method.\u001b[0m\n",
       "    \u001b[0;36mgenerate\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30mGenerates sequences for models with a language modeling head. The method currently supports greedy decoding,\u001b[0m\n",
       "    \u001b[0;36mget_extended_attention_mask\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30mMakes broadcastable attention and causal masks so that future and masked tokens are ignored.\u001b[0m\n",
       "    \u001b[0;36mget_head_mask\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30mPrepare the head mask if needed.\u001b[0m\n",
       "    \u001b[0;36mget_input_embeddings\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30mReturns the model's input embeddings.\u001b[0m\n",
       "    \u001b[0;36mget_output_embeddings\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30mReturns the model's output embeddings.\u001b[0m\n",
       "    \u001b[0;36mgreedy_search\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30mGenerates sequences for models with a language modeling head using greedy decoding.\u001b[0m\n",
       "    \u001b[0;36mhalf\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30mCasts all floating point parameters and buffers to ``half`` datatype.\u001b[0m\n",
       "    \u001b[0;36minit_weights\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30mInitializes and prunes weights if needed.\u001b[0m\n",
       "    \u001b[0;36minvert_attention_mask\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30mInvert an attention mask (e.g., switches 0. and 1.).\u001b[0m\n",
       "    \u001b[0;36mload_state_dict\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30mCopies parameters and buffers from :attr:`state_dict` into\u001b[0m\n",
       "    \u001b[0;36mload_tf_weights\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30mLoad tf checkpoints in a pytorch model.\u001b[0m\n",
       "    \u001b[0;36mmodules\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30mReturns an iterator over all modules in the network.\u001b[0m\n",
       "    \u001b[0;36mnamed_buffers\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30mReturns an iterator over module buffers, yielding both the\u001b[0m\n",
       "    \u001b[0;36mnamed_children\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30mReturns an iterator over immediate children modules, yielding both\u001b[0m\n",
       "    \u001b[0;36mnamed_modules\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30mReturns an iterator over all modules in the network, yielding\u001b[0m\n",
       "    \u001b[0;36mnamed_parameters\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30mReturns an iterator over module parameters, yielding both the\u001b[0m\n",
       "    \u001b[0;36mnum_parameters\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30mGet number of (optionally, trainable or non-embeddings) parameters in the module.\u001b[0m\n",
       "    \u001b[0;36mparameters\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30mReturns an iterator over module parameters.\u001b[0m\n",
       "    \u001b[0;36mprepare_inputs_for_generation\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30mImplement in subclasses of :class:`~transformers.PreTrainedModel` for custom behavior to prepare inputs in the\u001b[0m\n",
       "    \u001b[0;36mprune_heads\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30mPrunes heads of the base model.\u001b[0m\n",
       "    \u001b[0;36mregister_backward_hook\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30mRegisters a backward hook on the module.\u001b[0m\n",
       "    \u001b[0;36mregister_buffer\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30mAdds a buffer to the module.\u001b[0m\n",
       "    \u001b[0;36mregister_forward_hook\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30mRegisters a forward hook on the module.\u001b[0m\n",
       "    \u001b[0;36mregister_forward_pre_hook\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30mRegisters a forward pre-hook on the module.\u001b[0m\n",
       "    \u001b[0;36mregister_parameter\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30mAdds a parameter to the module.\u001b[0m\n",
       "    \u001b[0;36mrequires_grad_\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30mChange if autograd should record operations on parameters in this\u001b[0m\n",
       "    \u001b[0;36mreset_memory_hooks_state\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30mReset the :obj:`mem_rss_diff` attribute of each module (see\u001b[0m\n",
       "    \u001b[0;36mresize_token_embeddings\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30mResizes input token embeddings matrix of the model if :obj:`new_num_tokens != config.vocab_size`.\u001b[0m\n",
       "    \u001b[0;36msample\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30mGenerates sequences for models with a language modeling head using multinomial sampling.\u001b[0m\n",
       "    \u001b[0;36msave_pretrained\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30mSave a model and its configuration file to a directory, so that it can be re-loaded using the\u001b[0m\n",
       "    \u001b[0;36mset_input_embeddings\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30mSet model's input embeddings.\u001b[0m\n",
       "    \u001b[0;36mshare_memory\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30m\u001b[0m\n",
       "    \u001b[0;36mstate_dict\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30mReturns a dictionary containing a whole state of the module.\u001b[0m\n",
       "    \u001b[0;36mtie_weights\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30mTie the weights between the input embeddings and the output embeddings.\u001b[0m\n",
       "    \u001b[0;36mto\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30mMoves and/or casts the parameters and buffers.\u001b[0m\n",
       "    \u001b[0;36mtrain\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30mSets the module in training mode.\u001b[0m\n",
       "    \u001b[0;36mtype\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30mCasts all parameters and buffers to :attr:`dst_type`.\u001b[0m\n",
       "    \u001b[0;36mzero_grad\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30mSets gradients of all model parameters to zero.\u001b[0m\n",
       "\u001b[0;33mmagic:\u001b[0m\n",
       "    \u001b[0;36m__call__\u001b[0m\u001b[0;36m: \u001b[0m\u001b[1;30m\u001b[0m"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdir(mo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def log_start_builder(name):\n",
    "    def log_start(module, m_in):\n",
    "        print(name, module.__class__.__name__, 'start', time.time())\n",
    "    return log_start\n",
    "def log_end_builder(name):\n",
    "    def log_end(module, m_in, m_out):\n",
    "        print(name, module.__class__.__name__, 'end', time.time())\n",
    "    return log_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " BertModel\n",
      "embeddings BertEmbeddings\n",
      "embeddings.word_embeddings Embedding\n",
      "embeddings.position_embeddings Embedding\n",
      "embeddings.token_type_embeddings Embedding\n",
      "embeddings.LayerNorm LayerNorm\n",
      "embeddings.dropout Dropout\n",
      "encoder BertEncoder\n",
      "encoder.layer ModuleList\n",
      "encoder.layer.0 BertLayer\n",
      "encoder.layer.0.attention BertAttention\n",
      "encoder.layer.0.attention.self BertSelfAttention\n",
      "encoder.layer.0.attention.self.query Linear\n",
      "encoder.layer.0.attention.self.key Linear\n",
      "encoder.layer.0.attention.self.value Linear\n",
      "encoder.layer.0.attention.self.dropout Dropout\n",
      "encoder.layer.0.attention.output BertSelfOutput\n",
      "encoder.layer.0.attention.output.dense Linear\n",
      "encoder.layer.0.attention.output.LayerNorm LayerNorm\n",
      "encoder.layer.0.attention.output.dropout Dropout\n",
      "encoder.layer.0.intermediate BertIntermediate\n",
      "encoder.layer.0.intermediate.dense Linear\n",
      "encoder.layer.0.output BertOutput\n",
      "encoder.layer.0.output.dense Linear\n",
      "encoder.layer.0.output.LayerNorm LayerNorm\n",
      "encoder.layer.0.output.dropout Dropout\n",
      "encoder.layer.1 BertLayer\n",
      "encoder.layer.1.attention BertAttention\n",
      "encoder.layer.1.attention.self BertSelfAttention\n",
      "encoder.layer.1.attention.self.query Linear\n",
      "encoder.layer.1.attention.self.key Linear\n",
      "encoder.layer.1.attention.self.value Linear\n",
      "encoder.layer.1.attention.self.dropout Dropout\n",
      "encoder.layer.1.attention.output BertSelfOutput\n",
      "encoder.layer.1.attention.output.dense Linear\n",
      "encoder.layer.1.attention.output.LayerNorm LayerNorm\n",
      "encoder.layer.1.attention.output.dropout Dropout\n",
      "encoder.layer.1.intermediate BertIntermediate\n",
      "encoder.layer.1.intermediate.dense Linear\n",
      "encoder.layer.1.output BertOutput\n",
      "encoder.layer.1.output.dense Linear\n",
      "encoder.layer.1.output.LayerNorm LayerNorm\n",
      "encoder.layer.1.output.dropout Dropout\n",
      "encoder.layer.2 BertLayer\n",
      "encoder.layer.2.attention BertAttention\n",
      "encoder.layer.2.attention.self BertSelfAttention\n",
      "encoder.layer.2.attention.self.query Linear\n",
      "encoder.layer.2.attention.self.key Linear\n",
      "encoder.layer.2.attention.self.value Linear\n",
      "encoder.layer.2.attention.self.dropout Dropout\n",
      "encoder.layer.2.attention.output BertSelfOutput\n",
      "encoder.layer.2.attention.output.dense Linear\n",
      "encoder.layer.2.attention.output.LayerNorm LayerNorm\n",
      "encoder.layer.2.attention.output.dropout Dropout\n",
      "encoder.layer.2.intermediate BertIntermediate\n",
      "encoder.layer.2.intermediate.dense Linear\n",
      "encoder.layer.2.output BertOutput\n",
      "encoder.layer.2.output.dense Linear\n",
      "encoder.layer.2.output.LayerNorm LayerNorm\n",
      "encoder.layer.2.output.dropout Dropout\n",
      "encoder.layer.3 BertLayer\n",
      "encoder.layer.3.attention BertAttention\n",
      "encoder.layer.3.attention.self BertSelfAttention\n",
      "encoder.layer.3.attention.self.query Linear\n",
      "encoder.layer.3.attention.self.key Linear\n",
      "encoder.layer.3.attention.self.value Linear\n",
      "encoder.layer.3.attention.self.dropout Dropout\n",
      "encoder.layer.3.attention.output BertSelfOutput\n",
      "encoder.layer.3.attention.output.dense Linear\n",
      "encoder.layer.3.attention.output.LayerNorm LayerNorm\n",
      "encoder.layer.3.attention.output.dropout Dropout\n",
      "encoder.layer.3.intermediate BertIntermediate\n",
      "encoder.layer.3.intermediate.dense Linear\n",
      "encoder.layer.3.output BertOutput\n",
      "encoder.layer.3.output.dense Linear\n",
      "encoder.layer.3.output.LayerNorm LayerNorm\n",
      "encoder.layer.3.output.dropout Dropout\n",
      "encoder.layer.4 BertLayer\n",
      "encoder.layer.4.attention BertAttention\n",
      "encoder.layer.4.attention.self BertSelfAttention\n",
      "encoder.layer.4.attention.self.query Linear\n",
      "encoder.layer.4.attention.self.key Linear\n",
      "encoder.layer.4.attention.self.value Linear\n",
      "encoder.layer.4.attention.self.dropout Dropout\n",
      "encoder.layer.4.attention.output BertSelfOutput\n",
      "encoder.layer.4.attention.output.dense Linear\n",
      "encoder.layer.4.attention.output.LayerNorm LayerNorm\n",
      "encoder.layer.4.attention.output.dropout Dropout\n",
      "encoder.layer.4.intermediate BertIntermediate\n",
      "encoder.layer.4.intermediate.dense Linear\n",
      "encoder.layer.4.output BertOutput\n",
      "encoder.layer.4.output.dense Linear\n",
      "encoder.layer.4.output.LayerNorm LayerNorm\n",
      "encoder.layer.4.output.dropout Dropout\n",
      "encoder.layer.5 BertLayer\n",
      "encoder.layer.5.attention BertAttention\n",
      "encoder.layer.5.attention.self BertSelfAttention\n",
      "encoder.layer.5.attention.self.query Linear\n",
      "encoder.layer.5.attention.self.key Linear\n",
      "encoder.layer.5.attention.self.value Linear\n",
      "encoder.layer.5.attention.self.dropout Dropout\n",
      "encoder.layer.5.attention.output BertSelfOutput\n",
      "encoder.layer.5.attention.output.dense Linear\n",
      "encoder.layer.5.attention.output.LayerNorm LayerNorm\n",
      "encoder.layer.5.attention.output.dropout Dropout\n",
      "encoder.layer.5.intermediate BertIntermediate\n",
      "encoder.layer.5.intermediate.dense Linear\n",
      "encoder.layer.5.output BertOutput\n",
      "encoder.layer.5.output.dense Linear\n",
      "encoder.layer.5.output.LayerNorm LayerNorm\n",
      "encoder.layer.5.output.dropout Dropout\n",
      "encoder.layer.6 BertLayer\n",
      "encoder.layer.6.attention BertAttention\n",
      "encoder.layer.6.attention.self BertSelfAttention\n",
      "encoder.layer.6.attention.self.query Linear\n",
      "encoder.layer.6.attention.self.key Linear\n",
      "encoder.layer.6.attention.self.value Linear\n",
      "encoder.layer.6.attention.self.dropout Dropout\n",
      "encoder.layer.6.attention.output BertSelfOutput\n",
      "encoder.layer.6.attention.output.dense Linear\n",
      "encoder.layer.6.attention.output.LayerNorm LayerNorm\n",
      "encoder.layer.6.attention.output.dropout Dropout\n",
      "encoder.layer.6.intermediate BertIntermediate\n",
      "encoder.layer.6.intermediate.dense Linear\n",
      "encoder.layer.6.output BertOutput\n",
      "encoder.layer.6.output.dense Linear\n",
      "encoder.layer.6.output.LayerNorm LayerNorm\n",
      "encoder.layer.6.output.dropout Dropout\n",
      "encoder.layer.7 BertLayer\n",
      "encoder.layer.7.attention BertAttention\n",
      "encoder.layer.7.attention.self BertSelfAttention\n",
      "encoder.layer.7.attention.self.query Linear\n",
      "encoder.layer.7.attention.self.key Linear\n",
      "encoder.layer.7.attention.self.value Linear\n",
      "encoder.layer.7.attention.self.dropout Dropout\n",
      "encoder.layer.7.attention.output BertSelfOutput\n",
      "encoder.layer.7.attention.output.dense Linear\n",
      "encoder.layer.7.attention.output.LayerNorm LayerNorm\n",
      "encoder.layer.7.attention.output.dropout Dropout\n",
      "encoder.layer.7.intermediate BertIntermediate\n",
      "encoder.layer.7.intermediate.dense Linear\n",
      "encoder.layer.7.output BertOutput\n",
      "encoder.layer.7.output.dense Linear\n",
      "encoder.layer.7.output.LayerNorm LayerNorm\n",
      "encoder.layer.7.output.dropout Dropout\n",
      "encoder.layer.8 BertLayer\n",
      "encoder.layer.8.attention BertAttention\n",
      "encoder.layer.8.attention.self BertSelfAttention\n",
      "encoder.layer.8.attention.self.query Linear\n",
      "encoder.layer.8.attention.self.key Linear\n",
      "encoder.layer.8.attention.self.value Linear\n",
      "encoder.layer.8.attention.self.dropout Dropout\n",
      "encoder.layer.8.attention.output BertSelfOutput\n",
      "encoder.layer.8.attention.output.dense Linear\n",
      "encoder.layer.8.attention.output.LayerNorm LayerNorm\n",
      "encoder.layer.8.attention.output.dropout Dropout\n",
      "encoder.layer.8.intermediate BertIntermediate\n",
      "encoder.layer.8.intermediate.dense Linear\n",
      "encoder.layer.8.output BertOutput\n",
      "encoder.layer.8.output.dense Linear\n",
      "encoder.layer.8.output.LayerNorm LayerNorm\n",
      "encoder.layer.8.output.dropout Dropout\n",
      "encoder.layer.9 BertLayer\n",
      "encoder.layer.9.attention BertAttention\n",
      "encoder.layer.9.attention.self BertSelfAttention\n",
      "encoder.layer.9.attention.self.query Linear\n",
      "encoder.layer.9.attention.self.key Linear\n",
      "encoder.layer.9.attention.self.value Linear\n",
      "encoder.layer.9.attention.self.dropout Dropout\n",
      "encoder.layer.9.attention.output BertSelfOutput\n",
      "encoder.layer.9.attention.output.dense Linear\n",
      "encoder.layer.9.attention.output.LayerNorm LayerNorm\n",
      "encoder.layer.9.attention.output.dropout Dropout\n",
      "encoder.layer.9.intermediate BertIntermediate\n",
      "encoder.layer.9.intermediate.dense Linear\n",
      "encoder.layer.9.output BertOutput\n",
      "encoder.layer.9.output.dense Linear\n",
      "encoder.layer.9.output.LayerNorm LayerNorm\n",
      "encoder.layer.9.output.dropout Dropout\n",
      "encoder.layer.10 BertLayer\n",
      "encoder.layer.10.attention BertAttention\n",
      "encoder.layer.10.attention.self BertSelfAttention\n",
      "encoder.layer.10.attention.self.query Linear\n",
      "encoder.layer.10.attention.self.key Linear\n",
      "encoder.layer.10.attention.self.value Linear\n",
      "encoder.layer.10.attention.self.dropout Dropout\n",
      "encoder.layer.10.attention.output BertSelfOutput\n",
      "encoder.layer.10.attention.output.dense Linear\n",
      "encoder.layer.10.attention.output.LayerNorm LayerNorm\n",
      "encoder.layer.10.attention.output.dropout Dropout\n",
      "encoder.layer.10.intermediate BertIntermediate\n",
      "encoder.layer.10.intermediate.dense Linear\n",
      "encoder.layer.10.output BertOutput\n",
      "encoder.layer.10.output.dense Linear\n",
      "encoder.layer.10.output.LayerNorm LayerNorm\n",
      "encoder.layer.10.output.dropout Dropout\n",
      "encoder.layer.11 BertLayer\n",
      "encoder.layer.11.attention BertAttention\n",
      "encoder.layer.11.attention.self BertSelfAttention\n",
      "encoder.layer.11.attention.self.query Linear\n",
      "encoder.layer.11.attention.self.key Linear\n",
      "encoder.layer.11.attention.self.value Linear\n",
      "encoder.layer.11.attention.self.dropout Dropout\n",
      "encoder.layer.11.attention.output BertSelfOutput\n",
      "encoder.layer.11.attention.output.dense Linear\n",
      "encoder.layer.11.attention.output.LayerNorm LayerNorm\n",
      "encoder.layer.11.attention.output.dropout Dropout\n",
      "encoder.layer.11.intermediate BertIntermediate\n",
      "encoder.layer.11.intermediate.dense Linear\n",
      "encoder.layer.11.output BertOutput\n",
      "encoder.layer.11.output.dense Linear\n",
      "encoder.layer.11.output.LayerNorm LayerNorm\n",
      "encoder.layer.11.output.dropout Dropout\n",
      "pooler BertPooler\n",
      "pooler.dense Linear\n",
      "pooler.activation Tanh\n"
     ]
    }
   ],
   "source": [
    "for name, module in mo.named_modules():\n",
    "    print(name, module.__class__.__name__)\n",
    "    module.register_forward_pre_hook(log_start_builder(name))\n",
    "    module.register_forward_hook(log_end_builder(name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " BertModel start 1605233159.3215292\n",
      "embeddings BertEmbeddings start 1605233159.3250391\n",
      "embeddings.word_embeddings Embedding start 1605233159.3258\n",
      "embeddings.word_embeddings Embedding end 1605233159.3267999\n",
      "embeddings.position_embeddings Embedding start 1605233159.3270159\n",
      "embeddings.position_embeddings Embedding end 1605233159.327935\n",
      "embeddings.token_type_embeddings Embedding start 1605233159.328186\n",
      "embeddings.token_type_embeddings Embedding end 1605233159.329014\n",
      "embeddings.LayerNorm LayerNorm start 1605233159.330348\n",
      "embeddings.LayerNorm LayerNorm end 1605233159.331576\n",
      "embeddings.dropout Dropout start 1605233159.331844\n",
      "embeddings.dropout Dropout end 1605233159.33263\n",
      "embeddings BertEmbeddings end 1605233159.332793\n",
      "encoder BertEncoder start 1605233159.333067\n",
      "encoder.layer.0 BertLayer start 1605233159.333193\n",
      "encoder.layer.0.attention BertAttention start 1605233159.3333042\n",
      "encoder.layer.0.attention.self BertSelfAttention start 1605233159.3334122\n",
      "encoder.layer.0.attention.self.query Linear start 1605233159.333518\n",
      "encoder.layer.0.attention.self.query Linear end 1605233159.340288\n",
      "encoder.layer.0.attention.self.key Linear start 1605233159.340569\n",
      "encoder.layer.0.attention.self.key Linear end 1605233159.343741\n",
      "encoder.layer.0.attention.self.value Linear start 1605233159.344013\n",
      "encoder.layer.0.attention.self.value Linear end 1605233159.3472571\n",
      "encoder.layer.0.attention.self.dropout Dropout start 1605233159.3577778\n",
      "encoder.layer.0.attention.self.dropout Dropout end 1605233159.358691\n",
      "encoder.layer.0.attention.self BertSelfAttention end 1605233159.361547\n",
      "encoder.layer.0.attention.output BertSelfOutput start 1605233159.361693\n",
      "encoder.layer.0.attention.output.dense Linear start 1605233159.3618088\n",
      "encoder.layer.0.attention.output.dense Linear end 1605233159.3647768\n",
      "encoder.layer.0.attention.output.dropout Dropout start 1605233159.365078\n",
      "encoder.layer.0.attention.output.dropout Dropout end 1605233159.365977\n",
      "encoder.layer.0.attention.output.LayerNorm LayerNorm start 1605233159.366739\n",
      "encoder.layer.0.attention.output.LayerNorm LayerNorm end 1605233159.367928\n",
      "encoder.layer.0.attention.output BertSelfOutput end 1605233159.368223\n",
      "encoder.layer.0.attention BertAttention end 1605233159.368343\n",
      "encoder.layer.0.intermediate BertIntermediate start 1605233159.3703098\n",
      "encoder.layer.0.intermediate.dense Linear start 1605233159.370566\n",
      "encoder.layer.0.intermediate.dense Linear end 1605233159.377356\n",
      "encoder.layer.0.intermediate BertIntermediate end 1605233159.387695\n",
      "encoder.layer.0.output BertOutput start 1605233159.387936\n",
      "encoder.layer.0.output.dense Linear start 1605233159.388062\n",
      "encoder.layer.0.output.dense Linear end 1605233159.393122\n",
      "encoder.layer.0.output.dropout Dropout start 1605233159.393305\n",
      "encoder.layer.0.output.dropout Dropout end 1605233159.394222\n",
      "encoder.layer.0.output.LayerNorm LayerNorm start 1605233159.394969\n",
      "encoder.layer.0.output.LayerNorm LayerNorm end 1605233159.396532\n",
      "encoder.layer.0.output BertOutput end 1605233159.3967721\n",
      "encoder.layer.0 BertLayer end 1605233159.396872\n",
      "encoder.layer.1 BertLayer start 1605233159.396965\n",
      "encoder.layer.1.attention BertAttention start 1605233159.397075\n",
      "encoder.layer.1.attention.self BertSelfAttention start 1605233159.397184\n",
      "encoder.layer.1.attention.self.query Linear start 1605233159.397297\n",
      "encoder.layer.1.attention.self.query Linear end 1605233159.399956\n",
      "encoder.layer.1.attention.self.key Linear start 1605233159.400234\n",
      "encoder.layer.1.attention.self.key Linear end 1605233159.402755\n",
      "encoder.layer.1.attention.self.value Linear start 1605233159.403007\n",
      "encoder.layer.1.attention.self.value Linear end 1605233159.405521\n",
      "encoder.layer.1.attention.self.dropout Dropout start 1605233159.415779\n",
      "encoder.layer.1.attention.self.dropout Dropout end 1605233159.416748\n",
      "encoder.layer.1.attention.self BertSelfAttention end 1605233159.420158\n",
      "encoder.layer.1.attention.output BertSelfOutput start 1605233159.420327\n",
      "encoder.layer.1.attention.output.dense Linear start 1605233159.420438\n",
      "encoder.layer.1.attention.output.dense Linear end 1605233159.423431\n",
      "encoder.layer.1.attention.output.dropout Dropout start 1605233159.423737\n",
      "encoder.layer.1.attention.output.dropout Dropout end 1605233159.42469\n",
      "encoder.layer.1.attention.output.LayerNorm LayerNorm start 1605233159.425588\n",
      "encoder.layer.1.attention.output.LayerNorm LayerNorm end 1605233159.426862\n",
      "encoder.layer.1.attention.output BertSelfOutput end 1605233159.427104\n",
      "encoder.layer.1.attention BertAttention end 1605233159.427202\n",
      "encoder.layer.1.intermediate BertIntermediate start 1605233159.428988\n",
      "encoder.layer.1.intermediate.dense Linear start 1605233159.429235\n",
      "encoder.layer.1.intermediate.dense Linear end 1605233159.435842\n",
      "encoder.layer.1.intermediate BertIntermediate end 1605233159.446378\n",
      "encoder.layer.1.output BertOutput start 1605233159.446591\n",
      "encoder.layer.1.output.dense Linear start 1605233159.44672\n",
      "encoder.layer.1.output.dense Linear end 1605233159.4522219\n",
      "encoder.layer.1.output.dropout Dropout start 1605233159.45243\n",
      "encoder.layer.1.output.dropout Dropout end 1605233159.4532962\n",
      "encoder.layer.1.output.LayerNorm LayerNorm start 1605233159.453917\n",
      "encoder.layer.1.output.LayerNorm LayerNorm end 1605233159.4555461\n",
      "encoder.layer.1.output BertOutput end 1605233159.455879\n",
      "encoder.layer.1 BertLayer end 1605233159.456031\n",
      "encoder.layer.2 BertLayer start 1605233159.456179\n",
      "encoder.layer.2.attention BertAttention start 1605233159.45638\n",
      "encoder.layer.2.attention.self BertSelfAttention start 1605233159.456494\n",
      "encoder.layer.2.attention.self.query Linear start 1605233159.4566221\n",
      "encoder.layer.2.attention.self.query Linear end 1605233159.4597461\n",
      "encoder.layer.2.attention.self.key Linear start 1605233159.459997\n",
      "encoder.layer.2.attention.self.key Linear end 1605233159.4628139\n",
      "encoder.layer.2.attention.self.value Linear start 1605233159.463066\n",
      "encoder.layer.2.attention.self.value Linear end 1605233159.465853\n",
      "encoder.layer.2.attention.self.dropout Dropout start 1605233159.476144\n",
      "encoder.layer.2.attention.self.dropout Dropout end 1605233159.477101\n",
      "encoder.layer.2.attention.self BertSelfAttention end 1605233159.480066\n",
      "encoder.layer.2.attention.output BertSelfOutput start 1605233159.480217\n",
      "encoder.layer.2.attention.output.dense Linear start 1605233159.483917\n",
      "encoder.layer.2.attention.output.dense Linear end 1605233159.4870038\n",
      "encoder.layer.2.attention.output.dropout Dropout start 1605233159.487318\n",
      "encoder.layer.2.attention.output.dropout Dropout end 1605233159.489299\n",
      "encoder.layer.2.attention.output.LayerNorm LayerNorm start 1605233159.4901052\n",
      "encoder.layer.2.attention.output.LayerNorm LayerNorm end 1605233159.491481\n",
      "encoder.layer.2.attention.output BertSelfOutput end 1605233159.491724\n",
      "encoder.layer.2.attention BertAttention end 1605233159.4918258\n",
      "encoder.layer.2.intermediate BertIntermediate start 1605233159.4939\n",
      "encoder.layer.2.intermediate.dense Linear start 1605233159.4942489\n",
      "encoder.layer.2.intermediate.dense Linear end 1605233159.50078\n",
      "encoder.layer.2.intermediate BertIntermediate end 1605233159.510871\n",
      "encoder.layer.2.output BertOutput start 1605233159.51108\n",
      "encoder.layer.2.output.dense Linear start 1605233159.5112052\n",
      "encoder.layer.2.output.dense Linear end 1605233159.516737\n",
      "encoder.layer.2.output.dropout Dropout start 1605233159.5169148\n",
      "encoder.layer.2.output.dropout Dropout end 1605233159.517709\n",
      "encoder.layer.2.output.LayerNorm LayerNorm start 1605233159.518511\n",
      "encoder.layer.2.output.LayerNorm LayerNorm end 1605233159.520389\n",
      "encoder.layer.2.output BertOutput end 1605233159.520759\n",
      "encoder.layer.2 BertLayer end 1605233159.52086\n",
      "encoder.layer.3 BertLayer start 1605233159.5209558\n",
      "encoder.layer.3.attention BertAttention start 1605233159.521075\n",
      "encoder.layer.3.attention.self BertSelfAttention start 1605233159.521184\n",
      "encoder.layer.3.attention.self.query Linear start 1605233159.5212932\n",
      "encoder.layer.3.attention.self.query Linear end 1605233159.524957\n",
      "encoder.layer.3.attention.self.key Linear start 1605233159.525166\n",
      "encoder.layer.3.attention.self.key Linear end 1605233159.5288792\n",
      "encoder.layer.3.attention.self.value Linear start 1605233159.5293162\n",
      "encoder.layer.3.attention.self.value Linear end 1605233159.533453\n",
      "encoder.layer.3.attention.self.dropout Dropout start 1605233159.5440462\n",
      "encoder.layer.3.attention.self.dropout Dropout end 1605233159.545216\n",
      "encoder.layer.3.attention.self BertSelfAttention end 1605233159.548612\n",
      "encoder.layer.3.attention.output BertSelfOutput start 1605233159.548828\n",
      "encoder.layer.3.attention.output.dense Linear start 1605233159.548958\n",
      "encoder.layer.3.attention.output.dense Linear end 1605233159.5535622\n",
      "encoder.layer.3.attention.output.dropout Dropout start 1605233159.5540822\n",
      "encoder.layer.3.attention.output.dropout Dropout end 1605233159.555495\n",
      "encoder.layer.3.attention.output.LayerNorm LayerNorm start 1605233159.5567439\n",
      "encoder.layer.3.attention.output.LayerNorm LayerNorm end 1605233159.559602\n",
      "encoder.layer.3.attention.output BertSelfOutput end 1605233159.559972\n",
      "encoder.layer.3.attention BertAttention end 1605233159.560132\n",
      "encoder.layer.3.intermediate BertIntermediate start 1605233159.562367\n",
      "encoder.layer.3.intermediate.dense Linear start 1605233159.5626938\n",
      "encoder.layer.3.intermediate.dense Linear end 1605233159.569911\n",
      "encoder.layer.3.intermediate BertIntermediate end 1605233159.579733\n",
      "encoder.layer.3.output BertOutput start 1605233159.580004\n",
      "encoder.layer.3.output.dense Linear start 1605233159.580147\n",
      "encoder.layer.3.output.dense Linear end 1605233159.5858161\n",
      "encoder.layer.3.output.dropout Dropout start 1605233159.5859919\n",
      "encoder.layer.3.output.dropout Dropout end 1605233159.5870001\n",
      "encoder.layer.3.output.LayerNorm LayerNorm start 1605233159.587747\n",
      "encoder.layer.3.output.LayerNorm LayerNorm end 1605233159.58913\n",
      "encoder.layer.3.output BertOutput end 1605233159.589375\n",
      "encoder.layer.3 BertLayer end 1605233159.5894778\n",
      "encoder.layer.4 BertLayer start 1605233159.589576\n",
      "encoder.layer.4.attention BertAttention start 1605233159.589725\n",
      "encoder.layer.4.attention.self BertSelfAttention start 1605233159.589838\n",
      "encoder.layer.4.attention.self.query Linear start 1605233159.589946\n",
      "encoder.layer.4.attention.self.query Linear end 1605233159.5931342\n",
      "encoder.layer.4.attention.self.key Linear start 1605233159.593388\n",
      "encoder.layer.4.attention.self.key Linear end 1605233159.596311\n",
      "encoder.layer.4.attention.self.value Linear start 1605233159.5965528\n",
      "encoder.layer.4.attention.self.value Linear end 1605233159.5994828\n",
      "encoder.layer.4.attention.self.dropout Dropout start 1605233159.610024\n",
      "encoder.layer.4.attention.self.dropout Dropout end 1605233159.610833\n",
      "encoder.layer.4.attention.self BertSelfAttention end 1605233159.613973\n",
      "encoder.layer.4.attention.output BertSelfOutput start 1605233159.6141272\n",
      "encoder.layer.4.attention.output.dense Linear start 1605233159.6143\n",
      "encoder.layer.4.attention.output.dense Linear end 1605233159.617431\n",
      "encoder.layer.4.attention.output.dropout Dropout start 1605233159.617617\n",
      "encoder.layer.4.attention.output.dropout Dropout end 1605233159.6182988\n",
      "encoder.layer.4.attention.output.LayerNorm LayerNorm start 1605233159.61889\n",
      "encoder.layer.4.attention.output.LayerNorm LayerNorm end 1605233159.620476\n",
      "encoder.layer.4.attention.output BertSelfOutput end 1605233159.6207192\n",
      "encoder.layer.4.attention BertAttention end 1605233159.620817\n",
      "encoder.layer.4.intermediate BertIntermediate start 1605233159.622936\n",
      "encoder.layer.4.intermediate.dense Linear start 1605233159.623275\n",
      "encoder.layer.4.intermediate.dense Linear end 1605233159.6299891\n",
      "encoder.layer.4.intermediate BertIntermediate end 1605233159.639602\n",
      "encoder.layer.4.output BertOutput start 1605233159.6398342\n",
      "encoder.layer.4.output.dense Linear start 1605233159.639971\n",
      "encoder.layer.4.output.dense Linear end 1605233159.645445\n",
      "encoder.layer.4.output.dropout Dropout start 1605233159.645616\n",
      "encoder.layer.4.output.dropout Dropout end 1605233159.64657\n",
      "encoder.layer.4.output.LayerNorm LayerNorm start 1605233159.6473029\n",
      "encoder.layer.4.output.LayerNorm LayerNorm end 1605233159.64858\n",
      "encoder.layer.4.output BertOutput end 1605233159.6488261\n",
      "encoder.layer.4 BertLayer end 1605233159.64897\n",
      "encoder.layer.5 BertLayer start 1605233159.649097\n",
      "encoder.layer.5.attention BertAttention start 1605233159.649207\n",
      "encoder.layer.5.attention.self BertSelfAttention start 1605233159.649316\n",
      "encoder.layer.5.attention.self.query Linear start 1605233159.6494231\n",
      "encoder.layer.5.attention.self.query Linear end 1605233159.6522298\n",
      "encoder.layer.5.attention.self.key Linear start 1605233159.652506\n",
      "encoder.layer.5.attention.self.key Linear end 1605233159.655366\n",
      "encoder.layer.5.attention.self.value Linear start 1605233159.655621\n",
      "encoder.layer.5.attention.self.value Linear end 1605233159.6585312\n",
      "encoder.layer.5.attention.self.dropout Dropout start 1605233159.6688352\n",
      "encoder.layer.5.attention.self.dropout Dropout end 1605233159.6697989\n",
      "encoder.layer.5.attention.self BertSelfAttention end 1605233159.6729548\n",
      "encoder.layer.5.attention.output BertSelfOutput start 1605233159.673167\n",
      "encoder.layer.5.attention.output.dense Linear start 1605233159.673333\n",
      "encoder.layer.5.attention.output.dense Linear end 1605233159.676357\n",
      "encoder.layer.5.attention.output.dropout Dropout start 1605233159.676615\n",
      "encoder.layer.5.attention.output.dropout Dropout end 1605233159.677522\n",
      "encoder.layer.5.attention.output.LayerNorm LayerNorm start 1605233159.678257\n",
      "encoder.layer.5.attention.output.LayerNorm LayerNorm end 1605233159.679574\n",
      "encoder.layer.5.attention.output BertSelfOutput end 1605233159.679822\n",
      "encoder.layer.5.attention BertAttention end 1605233159.6799319\n",
      "encoder.layer.5.intermediate BertIntermediate start 1605233159.681969\n",
      "encoder.layer.5.intermediate.dense Linear start 1605233159.68227\n",
      "encoder.layer.5.intermediate.dense Linear end 1605233159.694039\n",
      "encoder.layer.5.intermediate BertIntermediate end 1605233159.7042098\n",
      "encoder.layer.5.output BertOutput start 1605233159.70443\n",
      "encoder.layer.5.output.dense Linear start 1605233159.7045581\n",
      "encoder.layer.5.output.dense Linear end 1605233159.709856\n",
      "encoder.layer.5.output.dropout Dropout start 1605233159.7100391\n",
      "encoder.layer.5.output.dropout Dropout end 1605233159.71089\n",
      "encoder.layer.5.output.LayerNorm LayerNorm start 1605233159.711643\n",
      "encoder.layer.5.output.LayerNorm LayerNorm end 1605233159.713375\n",
      "encoder.layer.5.output BertOutput end 1605233159.7137558\n",
      "encoder.layer.5 BertLayer end 1605233159.7138612\n",
      "encoder.layer.6 BertLayer start 1605233159.713965\n",
      "encoder.layer.6.attention BertAttention start 1605233159.714087\n",
      "encoder.layer.6.attention.self BertSelfAttention start 1605233159.714227\n",
      "encoder.layer.6.attention.self.query Linear start 1605233159.71436\n",
      "encoder.layer.6.attention.self.query Linear end 1605233159.7170858\n",
      "encoder.layer.6.attention.self.key Linear start 1605233159.7173262\n",
      "encoder.layer.6.attention.self.key Linear end 1605233159.7202492\n",
      "encoder.layer.6.attention.self.value Linear start 1605233159.72052\n",
      "encoder.layer.6.attention.self.value Linear end 1605233159.72331\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder.layer.6.attention.self.dropout Dropout start 1605233159.735748\n",
      "encoder.layer.6.attention.self.dropout Dropout end 1605233159.736635\n",
      "encoder.layer.6.attention.self BertSelfAttention end 1605233159.739626\n",
      "encoder.layer.6.attention.output BertSelfOutput start 1605233159.7397661\n",
      "encoder.layer.6.attention.output.dense Linear start 1605233159.73996\n",
      "encoder.layer.6.attention.output.dense Linear end 1605233159.743243\n",
      "encoder.layer.6.attention.output.dropout Dropout start 1605233159.7435882\n",
      "encoder.layer.6.attention.output.dropout Dropout end 1605233159.744777\n",
      "encoder.layer.6.attention.output.LayerNorm LayerNorm start 1605233159.745652\n",
      "encoder.layer.6.attention.output.LayerNorm LayerNorm end 1605233159.746937\n",
      "encoder.layer.6.attention.output BertSelfOutput end 1605233159.747252\n",
      "encoder.layer.6.attention BertAttention end 1605233159.747392\n",
      "encoder.layer.6.intermediate BertIntermediate start 1605233159.7498848\n",
      "encoder.layer.6.intermediate.dense Linear start 1605233159.750458\n",
      "encoder.layer.6.intermediate.dense Linear end 1605233159.7578602\n",
      "encoder.layer.6.intermediate BertIntermediate end 1605233159.769644\n",
      "encoder.layer.6.output BertOutput start 1605233159.769865\n",
      "encoder.layer.6.output.dense Linear start 1605233159.770006\n",
      "encoder.layer.6.output.dense Linear end 1605233159.776169\n",
      "encoder.layer.6.output.dropout Dropout start 1605233159.776434\n",
      "encoder.layer.6.output.dropout Dropout end 1605233159.7776\n",
      "encoder.layer.6.output.LayerNorm LayerNorm start 1605233159.778396\n",
      "encoder.layer.6.output.LayerNorm LayerNorm end 1605233159.780088\n",
      "encoder.layer.6.output BertOutput end 1605233159.7804759\n",
      "encoder.layer.6 BertLayer end 1605233159.780586\n",
      "encoder.layer.7 BertLayer start 1605233159.78069\n",
      "encoder.layer.7.attention BertAttention start 1605233159.7808168\n",
      "encoder.layer.7.attention.self BertSelfAttention start 1605233159.7809439\n",
      "encoder.layer.7.attention.self.query Linear start 1605233159.781066\n",
      "encoder.layer.7.attention.self.query Linear end 1605233159.783804\n",
      "encoder.layer.7.attention.self.key Linear start 1605233159.7840462\n",
      "encoder.layer.7.attention.self.key Linear end 1605233159.787122\n",
      "encoder.layer.7.attention.self.value Linear start 1605233159.7873778\n",
      "encoder.layer.7.attention.self.value Linear end 1605233159.790449\n",
      "encoder.layer.7.attention.self.dropout Dropout start 1605233159.801167\n",
      "encoder.layer.7.attention.self.dropout Dropout end 1605233159.8019688\n",
      "encoder.layer.7.attention.self BertSelfAttention end 1605233159.805048\n",
      "encoder.layer.7.attention.output BertSelfOutput start 1605233159.805194\n",
      "encoder.layer.7.attention.output.dense Linear start 1605233159.805314\n",
      "encoder.layer.7.attention.output.dense Linear end 1605233159.808011\n",
      "encoder.layer.7.attention.output.dropout Dropout start 1605233159.808268\n",
      "encoder.layer.7.attention.output.dropout Dropout end 1605233159.809102\n",
      "encoder.layer.7.attention.output.LayerNorm LayerNorm start 1605233159.8097112\n",
      "encoder.layer.7.attention.output.LayerNorm LayerNorm end 1605233159.811109\n",
      "encoder.layer.7.attention.output BertSelfOutput end 1605233159.811405\n",
      "encoder.layer.7.attention BertAttention end 1605233159.811562\n",
      "encoder.layer.7.intermediate BertIntermediate start 1605233159.8138041\n",
      "encoder.layer.7.intermediate.dense Linear start 1605233159.814126\n",
      "encoder.layer.7.intermediate.dense Linear end 1605233159.8205922\n",
      "encoder.layer.7.intermediate BertIntermediate end 1605233159.830836\n",
      "encoder.layer.7.output BertOutput start 1605233159.831043\n",
      "encoder.layer.7.output.dense Linear start 1605233159.8311849\n",
      "encoder.layer.7.output.dense Linear end 1605233159.83662\n",
      "encoder.layer.7.output.dropout Dropout start 1605233159.836799\n",
      "encoder.layer.7.output.dropout Dropout end 1605233159.8375611\n",
      "encoder.layer.7.output.LayerNorm LayerNorm start 1605233159.8382049\n",
      "encoder.layer.7.output.LayerNorm LayerNorm end 1605233159.839962\n",
      "encoder.layer.7.output BertOutput end 1605233159.840231\n",
      "encoder.layer.7 BertLayer end 1605233159.8403788\n",
      "encoder.layer.8 BertLayer start 1605233159.84053\n",
      "encoder.layer.8.attention BertAttention start 1605233159.840692\n",
      "encoder.layer.8.attention.self BertSelfAttention start 1605233159.8408709\n",
      "encoder.layer.8.attention.self.query Linear start 1605233159.8410392\n",
      "encoder.layer.8.attention.self.query Linear end 1605233159.8440661\n",
      "encoder.layer.8.attention.self.key Linear start 1605233159.844316\n",
      "encoder.layer.8.attention.self.key Linear end 1605233159.847063\n",
      "encoder.layer.8.attention.self.value Linear start 1605233159.847336\n",
      "encoder.layer.8.attention.self.value Linear end 1605233159.850084\n",
      "encoder.layer.8.attention.self.dropout Dropout start 1605233159.860756\n",
      "encoder.layer.8.attention.self.dropout Dropout end 1605233159.8617642\n",
      "encoder.layer.8.attention.self BertSelfAttention end 1605233159.8646631\n",
      "encoder.layer.8.attention.output BertSelfOutput start 1605233159.8648589\n",
      "encoder.layer.8.attention.output.dense Linear start 1605233159.864973\n",
      "encoder.layer.8.attention.output.dense Linear end 1605233159.8679721\n",
      "encoder.layer.8.attention.output.dropout Dropout start 1605233159.8682299\n",
      "encoder.layer.8.attention.output.dropout Dropout end 1605233159.869111\n",
      "encoder.layer.8.attention.output.LayerNorm LayerNorm start 1605233159.869876\n",
      "encoder.layer.8.attention.output.LayerNorm LayerNorm end 1605233159.871064\n",
      "encoder.layer.8.attention.output BertSelfOutput end 1605233159.8712971\n",
      "encoder.layer.8.attention BertAttention end 1605233159.871396\n",
      "encoder.layer.8.intermediate BertIntermediate start 1605233159.8734012\n",
      "encoder.layer.8.intermediate.dense Linear start 1605233159.873666\n",
      "encoder.layer.8.intermediate.dense Linear end 1605233159.8798149\n",
      "encoder.layer.8.intermediate BertIntermediate end 1605233159.8899899\n",
      "encoder.layer.8.output BertOutput start 1605233159.890196\n",
      "encoder.layer.8.output.dense Linear start 1605233159.8903308\n",
      "encoder.layer.8.output.dense Linear end 1605233159.895543\n",
      "encoder.layer.8.output.dropout Dropout start 1605233159.895723\n",
      "encoder.layer.8.output.dropout Dropout end 1605233159.8965812\n",
      "encoder.layer.8.output.LayerNorm LayerNorm start 1605233159.897231\n",
      "encoder.layer.8.output.LayerNorm LayerNorm end 1605233159.89873\n",
      "encoder.layer.8.output BertOutput end 1605233159.898967\n",
      "encoder.layer.8 BertLayer end 1605233159.899065\n",
      "encoder.layer.9 BertLayer start 1605233159.899158\n",
      "encoder.layer.9.attention BertAttention start 1605233159.899277\n",
      "encoder.layer.9.attention.self BertSelfAttention start 1605233159.8993862\n",
      "encoder.layer.9.attention.self.query Linear start 1605233159.899495\n",
      "encoder.layer.9.attention.self.query Linear end 1605233159.902022\n",
      "encoder.layer.9.attention.self.key Linear start 1605233159.902276\n",
      "encoder.layer.9.attention.self.key Linear end 1605233159.905277\n",
      "encoder.layer.9.attention.self.value Linear start 1605233159.9055328\n",
      "encoder.layer.9.attention.self.value Linear end 1605233159.90849\n",
      "encoder.layer.9.attention.self.dropout Dropout start 1605233159.918953\n",
      "encoder.layer.9.attention.self.dropout Dropout end 1605233159.920091\n",
      "encoder.layer.9.attention.self BertSelfAttention end 1605233159.9241128\n",
      "encoder.layer.9.attention.output BertSelfOutput start 1605233159.924326\n",
      "encoder.layer.9.attention.output.dense Linear start 1605233159.9244878\n",
      "encoder.layer.9.attention.output.dense Linear end 1605233159.927854\n",
      "encoder.layer.9.attention.output.dropout Dropout start 1605233159.928201\n",
      "encoder.layer.9.attention.output.dropout Dropout end 1605233159.929169\n",
      "encoder.layer.9.attention.output.LayerNorm LayerNorm start 1605233159.9299378\n",
      "encoder.layer.9.attention.output.LayerNorm LayerNorm end 1605233159.931504\n",
      "encoder.layer.9.attention.output BertSelfOutput end 1605233159.931759\n",
      "encoder.layer.9.attention BertAttention end 1605233159.9318578\n",
      "encoder.layer.9.intermediate BertIntermediate start 1605233159.934338\n",
      "encoder.layer.9.intermediate.dense Linear start 1605233159.934645\n",
      "encoder.layer.9.intermediate.dense Linear end 1605233159.943004\n",
      "encoder.layer.9.intermediate BertIntermediate end 1605233159.953616\n",
      "encoder.layer.9.output BertOutput start 1605233159.9538171\n",
      "encoder.layer.9.output.dense Linear start 1605233159.953941\n",
      "encoder.layer.9.output.dense Linear end 1605233159.961169\n",
      "encoder.layer.9.output.dropout Dropout start 1605233159.961418\n",
      "encoder.layer.9.output.dropout Dropout end 1605233159.962522\n",
      "encoder.layer.9.output.LayerNorm LayerNorm start 1605233159.963446\n",
      "encoder.layer.9.output.LayerNorm LayerNorm end 1605233159.9654121\n",
      "encoder.layer.9.output BertOutput end 1605233159.965746\n",
      "encoder.layer.9 BertLayer end 1605233159.965899\n",
      "encoder.layer.10 BertLayer start 1605233159.9662879\n",
      "encoder.layer.10.attention BertAttention start 1605233159.966478\n",
      "encoder.layer.10.attention.self BertSelfAttention start 1605233159.966866\n",
      "encoder.layer.10.attention.self.query Linear start 1605233159.966999\n",
      "encoder.layer.10.attention.self.query Linear end 1605233159.970069\n",
      "encoder.layer.10.attention.self.key Linear start 1605233159.9703119\n",
      "encoder.layer.10.attention.self.key Linear end 1605233159.9732401\n",
      "encoder.layer.10.attention.self.value Linear start 1605233159.9735072\n",
      "encoder.layer.10.attention.self.value Linear end 1605233159.976403\n",
      "encoder.layer.10.attention.self.dropout Dropout start 1605233159.98761\n",
      "encoder.layer.10.attention.self.dropout Dropout end 1605233159.9884748\n",
      "encoder.layer.10.attention.self BertSelfAttention end 1605233159.9918988\n",
      "encoder.layer.10.attention.output BertSelfOutput start 1605233159.992061\n",
      "encoder.layer.10.attention.output.dense Linear start 1605233159.9921799\n",
      "encoder.layer.10.attention.output.dense Linear end 1605233159.995121\n",
      "encoder.layer.10.attention.output.dropout Dropout start 1605233159.995421\n",
      "encoder.layer.10.attention.output.dropout Dropout end 1605233159.996264\n",
      "encoder.layer.10.attention.output.LayerNorm LayerNorm start 1605233159.9970298\n",
      "encoder.layer.10.attention.output.LayerNorm LayerNorm end 1605233159.998316\n",
      "encoder.layer.10.attention.output BertSelfOutput end 1605233159.9985652\n",
      "encoder.layer.10.attention BertAttention end 1605233159.99867\n",
      "encoder.layer.10.intermediate BertIntermediate start 1605233160.00075\n",
      "encoder.layer.10.intermediate.dense Linear start 1605233160.001125\n",
      "encoder.layer.10.intermediate.dense Linear end 1605233160.007648\n",
      "encoder.layer.10.intermediate BertIntermediate end 1605233160.019423\n",
      "encoder.layer.10.output BertOutput start 1605233160.019646\n",
      "encoder.layer.10.output.dense Linear start 1605233160.019775\n",
      "encoder.layer.10.output.dense Linear end 1605233160.0266\n",
      "encoder.layer.10.output.dropout Dropout start 1605233160.0269089\n",
      "encoder.layer.10.output.dropout Dropout end 1605233160.0282109\n",
      "encoder.layer.10.output.LayerNorm LayerNorm start 1605233160.0291798\n",
      "encoder.layer.10.output.LayerNorm LayerNorm end 1605233160.031115\n",
      "encoder.layer.10.output BertOutput end 1605233160.031513\n",
      "encoder.layer.10 BertLayer end 1605233160.0316632\n",
      "encoder.layer.11 BertLayer start 1605233160.0318022\n",
      "encoder.layer.11.attention BertAttention start 1605233160.0319738\n",
      "encoder.layer.11.attention.self BertSelfAttention start 1605233160.032131\n",
      "encoder.layer.11.attention.self.query Linear start 1605233160.032286\n",
      "encoder.layer.11.attention.self.query Linear end 1605233160.03559\n",
      "encoder.layer.11.attention.self.key Linear start 1605233160.035876\n",
      "encoder.layer.11.attention.self.key Linear end 1605233160.038811\n",
      "encoder.layer.11.attention.self.value Linear start 1605233160.039212\n",
      "encoder.layer.11.attention.self.value Linear end 1605233160.043016\n",
      "encoder.layer.11.attention.self.dropout Dropout start 1605233160.054305\n",
      "encoder.layer.11.attention.self.dropout Dropout end 1605233160.055097\n",
      "encoder.layer.11.attention.self BertSelfAttention end 1605233160.059106\n",
      "encoder.layer.11.attention.output BertSelfOutput start 1605233160.059398\n",
      "encoder.layer.11.attention.output.dense Linear start 1605233160.0596108\n",
      "encoder.layer.11.attention.output.dense Linear end 1605233160.062882\n",
      "encoder.layer.11.attention.output.dropout Dropout start 1605233160.063156\n",
      "encoder.layer.11.attention.output.dropout Dropout end 1605233160.064209\n",
      "encoder.layer.11.attention.output.LayerNorm LayerNorm start 1605233160.065151\n",
      "encoder.layer.11.attention.output.LayerNorm LayerNorm end 1605233160.0666308\n",
      "encoder.layer.11.attention.output BertSelfOutput end 1605233160.066964\n",
      "encoder.layer.11.attention BertAttention end 1605233160.0671208\n",
      "encoder.layer.11.intermediate BertIntermediate start 1605233160.0690188\n",
      "encoder.layer.11.intermediate.dense Linear start 1605233160.069282\n",
      "encoder.layer.11.intermediate.dense Linear end 1605233160.076173\n",
      "encoder.layer.11.intermediate BertIntermediate end 1605233160.087771\n",
      "encoder.layer.11.output BertOutput start 1605233160.0879848\n",
      "encoder.layer.11.output.dense Linear start 1605233160.0881212\n",
      "encoder.layer.11.output.dense Linear end 1605233160.095155\n",
      "encoder.layer.11.output.dropout Dropout start 1605233160.095665\n",
      "encoder.layer.11.output.dropout Dropout end 1605233160.096755\n",
      "encoder.layer.11.output.LayerNorm LayerNorm start 1605233160.097526\n",
      "encoder.layer.11.output.LayerNorm LayerNorm end 1605233160.099067\n",
      "encoder.layer.11.output BertOutput end 1605233160.099322\n",
      "encoder.layer.11 BertLayer end 1605233160.099425\n",
      "encoder BertEncoder end 1605233160.099527\n",
      "pooler BertPooler start 1605233160.099627\n",
      "pooler.dense Linear start 1605233160.1001039\n",
      "pooler.dense Linear end 1605233160.101702\n",
      "pooler.activation Tanh start 1605233160.1019342\n",
      "pooler.activation Tanh end 1605233160.102388\n",
      "pooler BertPooler end 1605233160.102494\n",
      " BertModel end 1605233160.1026309\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " BertModel start 1605233160.5094929\n",
      "embeddings BertEmbeddings start 1605233160.5146182\n",
      "embeddings.word_embeddings Embedding start 1605233160.515347\n",
      "embeddings.word_embeddings Embedding end 1605233160.516169\n",
      "embeddings.position_embeddings Embedding start 1605233160.516381\n",
      "embeddings.position_embeddings Embedding end 1605233160.5171509\n",
      "embeddings.token_type_embeddings Embedding start 1605233160.517248\n",
      "embeddings.token_type_embeddings Embedding end 1605233160.518071\n",
      "embeddings.LayerNorm LayerNorm start 1605233160.519047\n",
      "embeddings.LayerNorm LayerNorm end 1605233160.519851\n",
      "embeddings.dropout Dropout start 1605233160.520065\n",
      "embeddings.dropout Dropout end 1605233160.52066\n",
      "embeddings BertEmbeddings end 1605233160.5207648\n",
      "encoder BertEncoder start 1605233160.52086\n",
      "encoder.layer.0 BertLayer start 1605233160.5209758\n",
      "encoder.layer.0.attention BertAttention start 1605233160.521217\n",
      "encoder.layer.0.attention.self BertSelfAttention start 1605233160.521323\n",
      "encoder.layer.0.attention.self.query Linear start 1605233160.521437\n",
      "encoder.layer.0.attention.self.query Linear end 1605233160.524052\n",
      "encoder.layer.0.attention.self.key Linear start 1605233160.524361\n",
      "encoder.layer.0.attention.self.key Linear end 1605233160.527148\n",
      "encoder.layer.0.attention.self.value Linear start 1605233160.5273979\n",
      "encoder.layer.0.attention.self.value Linear end 1605233160.530114\n",
      "encoder.layer.0.attention.self.dropout Dropout start 1605233160.540064\n",
      "encoder.layer.0.attention.self.dropout Dropout end 1605233160.541066\n",
      "encoder.layer.0.attention.self BertSelfAttention end 1605233160.544255\n",
      "encoder.layer.0.attention.output BertSelfOutput start 1605233160.5445051\n",
      "encoder.layer.0.attention.output.dense Linear start 1605233160.54462\n",
      "encoder.layer.0.attention.output.dense Linear end 1605233160.547308\n",
      "encoder.layer.0.attention.output.dropout Dropout start 1605233160.547555\n",
      "encoder.layer.0.attention.output.dropout Dropout end 1605233160.54833\n",
      "encoder.layer.0.attention.output.LayerNorm LayerNorm start 1605233160.5489411\n",
      "encoder.layer.0.attention.output.LayerNorm LayerNorm end 1605233160.550427\n",
      "encoder.layer.0.attention.output BertSelfOutput end 1605233160.550666\n",
      "encoder.layer.0.attention BertAttention end 1605233160.550766\n",
      "encoder.layer.0.intermediate BertIntermediate start 1605233160.5526311\n",
      "encoder.layer.0.intermediate.dense Linear start 1605233160.552854\n",
      "encoder.layer.0.intermediate.dense Linear end 1605233160.55948\n",
      "encoder.layer.0.intermediate BertIntermediate end 1605233160.567501\n",
      "encoder.layer.0.output BertOutput start 1605233160.56776\n",
      "encoder.layer.0.output.dense Linear start 1605233160.567879\n",
      "encoder.layer.0.output.dense Linear end 1605233160.573313\n",
      "encoder.layer.0.output.dropout Dropout start 1605233160.573524\n",
      "encoder.layer.0.output.dropout Dropout end 1605233160.574539\n",
      "encoder.layer.0.output.LayerNorm LayerNorm start 1605233160.57532\n",
      "encoder.layer.0.output.LayerNorm LayerNorm end 1605233160.576577\n",
      "encoder.layer.0.output BertOutput end 1605233160.57684\n",
      "encoder.layer.0 BertLayer end 1605233160.576952\n",
      "encoder.layer.1 BertLayer start 1605233160.577044\n",
      "encoder.layer.1.attention BertAttention start 1605233160.5771542\n",
      "encoder.layer.1.attention.self BertSelfAttention start 1605233160.577261\n",
      "encoder.layer.1.attention.self.query Linear start 1605233160.577367\n",
      "encoder.layer.1.attention.self.query Linear end 1605233160.580172\n",
      "encoder.layer.1.attention.self.key Linear start 1605233160.58037\n",
      "encoder.layer.1.attention.self.key Linear end 1605233160.582985\n",
      "encoder.layer.1.attention.self.value Linear start 1605233160.583232\n",
      "encoder.layer.1.attention.self.value Linear end 1605233160.5857122\n",
      "encoder.layer.1.attention.self.dropout Dropout start 1605233160.596184\n",
      "encoder.layer.1.attention.self.dropout Dropout end 1605233160.597326\n",
      "encoder.layer.1.attention.self BertSelfAttention end 1605233160.6006541\n",
      "encoder.layer.1.attention.output BertSelfOutput start 1605233160.6009102\n",
      "encoder.layer.1.attention.output.dense Linear start 1605233160.601024\n",
      "encoder.layer.1.attention.output.dense Linear end 1605233160.603991\n",
      "encoder.layer.1.attention.output.dropout Dropout start 1605233160.604198\n",
      "encoder.layer.1.attention.output.dropout Dropout end 1605233160.605078\n",
      "encoder.layer.1.attention.output.LayerNorm LayerNorm start 1605233160.6056588\n",
      "encoder.layer.1.attention.output.LayerNorm LayerNorm end 1605233160.607293\n",
      "encoder.layer.1.attention.output BertSelfOutput end 1605233160.607554\n",
      "encoder.layer.1.attention BertAttention end 1605233160.607697\n",
      "encoder.layer.1.intermediate BertIntermediate start 1605233160.6099489\n",
      "encoder.layer.1.intermediate.dense Linear start 1605233160.610247\n",
      "encoder.layer.1.intermediate.dense Linear end 1605233160.61663\n",
      "encoder.layer.1.intermediate BertIntermediate end 1605233160.623501\n",
      "encoder.layer.1.output BertOutput start 1605233160.6236842\n",
      "encoder.layer.1.output.dense Linear start 1605233160.62384\n",
      "encoder.layer.1.output.dense Linear end 1605233160.629327\n",
      "encoder.layer.1.output.dropout Dropout start 1605233160.629515\n",
      "encoder.layer.1.output.dropout Dropout end 1605233160.630584\n",
      "encoder.layer.1.output.LayerNorm LayerNorm start 1605233160.6313212\n",
      "encoder.layer.1.output.LayerNorm LayerNorm end 1605233160.632561\n",
      "encoder.layer.1.output BertOutput end 1605233160.6328058\n",
      "encoder.layer.1 BertLayer end 1605233160.632912\n",
      "encoder.layer.2 BertLayer start 1605233160.6330082\n",
      "encoder.layer.2.attention BertAttention start 1605233160.6331341\n",
      "encoder.layer.2.attention.self BertSelfAttention start 1605233160.6332521\n",
      "encoder.layer.2.attention.self.query Linear start 1605233160.633357\n",
      "encoder.layer.2.attention.self.query Linear end 1605233160.6362581\n",
      "encoder.layer.2.attention.self.key Linear start 1605233160.636498\n",
      "encoder.layer.2.attention.self.key Linear end 1605233160.639075\n",
      "encoder.layer.2.attention.self.value Linear start 1605233160.639333\n",
      "encoder.layer.2.attention.self.value Linear end 1605233160.6420832\n",
      "encoder.layer.2.attention.self.dropout Dropout start 1605233160.651613\n",
      "encoder.layer.2.attention.self.dropout Dropout end 1605233160.652556\n",
      "encoder.layer.2.attention.self BertSelfAttention end 1605233160.655741\n",
      "encoder.layer.2.attention.output BertSelfOutput start 1605233160.655992\n",
      "encoder.layer.2.attention.output.dense Linear start 1605233160.6561432\n",
      "encoder.layer.2.attention.output.dense Linear end 1605233160.659189\n",
      "encoder.layer.2.attention.output.dropout Dropout start 1605233160.659492\n",
      "encoder.layer.2.attention.output.dropout Dropout end 1605233160.660393\n",
      "encoder.layer.2.attention.output.LayerNorm LayerNorm start 1605233160.661036\n",
      "encoder.layer.2.attention.output.LayerNorm LayerNorm end 1605233160.662411\n",
      "encoder.layer.2.attention.output BertSelfOutput end 1605233160.662657\n",
      "encoder.layer.2.attention BertAttention end 1605233160.662755\n",
      "encoder.layer.2.intermediate BertIntermediate start 1605233160.664643\n",
      "encoder.layer.2.intermediate.dense Linear start 1605233160.664897\n",
      "encoder.layer.2.intermediate.dense Linear end 1605233160.671267\n",
      "encoder.layer.2.intermediate BertIntermediate end 1605233160.6778991\n",
      "encoder.layer.2.output BertOutput start 1605233160.678089\n",
      "encoder.layer.2.output.dense Linear start 1605233160.678217\n",
      "encoder.layer.2.output.dense Linear end 1605233160.683538\n",
      "encoder.layer.2.output.dropout Dropout start 1605233160.683717\n",
      "encoder.layer.2.output.dropout Dropout end 1605233160.684584\n",
      "encoder.layer.2.output.LayerNorm LayerNorm start 1605233160.6853828\n",
      "encoder.layer.2.output.LayerNorm LayerNorm end 1605233160.686636\n",
      "encoder.layer.2.output BertOutput end 1605233160.686916\n",
      "encoder.layer.2 BertLayer end 1605233160.687078\n",
      "encoder.layer.3 BertLayer start 1605233160.6872299\n",
      "encoder.layer.3.attention BertAttention start 1605233160.687404\n",
      "encoder.layer.3.attention.self BertSelfAttention start 1605233160.687574\n",
      "encoder.layer.3.attention.self.query Linear start 1605233160.687742\n",
      "encoder.layer.3.attention.self.query Linear end 1605233160.690637\n",
      "encoder.layer.3.attention.self.key Linear start 1605233160.6908858\n",
      "encoder.layer.3.attention.self.key Linear end 1605233160.6933079\n",
      "encoder.layer.3.attention.self.value Linear start 1605233160.6935759\n",
      "encoder.layer.3.attention.self.value Linear end 1605233160.696383\n",
      "encoder.layer.3.attention.self.dropout Dropout start 1605233160.7052999\n",
      "encoder.layer.3.attention.self.dropout Dropout end 1605233160.7064059\n",
      "encoder.layer.3.attention.self BertSelfAttention end 1605233160.7097092\n",
      "encoder.layer.3.attention.output BertSelfOutput start 1605233160.709965\n",
      "encoder.layer.3.attention.output.dense Linear start 1605233160.710079\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder.layer.3.attention.output.dense Linear end 1605233160.7135968\n",
      "encoder.layer.3.attention.output.dropout Dropout start 1605233160.713998\n",
      "encoder.layer.3.attention.output.dropout Dropout end 1605233160.71544\n",
      "encoder.layer.3.attention.output.LayerNorm LayerNorm start 1605233160.71666\n",
      "encoder.layer.3.attention.output.LayerNorm LayerNorm end 1605233160.718981\n",
      "encoder.layer.3.attention.output BertSelfOutput end 1605233160.7194188\n",
      "encoder.layer.3.attention BertAttention end 1605233160.719572\n",
      "encoder.layer.3.intermediate BertIntermediate start 1605233160.722333\n",
      "encoder.layer.3.intermediate.dense Linear start 1605233160.722692\n",
      "encoder.layer.3.intermediate.dense Linear end 1605233160.729732\n",
      "encoder.layer.3.intermediate BertIntermediate end 1605233160.736257\n",
      "encoder.layer.3.output BertOutput start 1605233160.736528\n",
      "encoder.layer.3.output.dense Linear start 1605233160.736655\n",
      "encoder.layer.3.output.dense Linear end 1605233160.742783\n",
      "encoder.layer.3.output.dropout Dropout start 1605233160.7430358\n",
      "encoder.layer.3.output.dropout Dropout end 1605233160.744305\n",
      "encoder.layer.3.output.LayerNorm LayerNorm start 1605233160.7452538\n",
      "encoder.layer.3.output.LayerNorm LayerNorm end 1605233160.7468522\n",
      "encoder.layer.3.output BertOutput end 1605233160.7471411\n",
      "encoder.layer.3 BertLayer end 1605233160.747245\n",
      "encoder.layer.4 BertLayer start 1605233160.747343\n",
      "encoder.layer.4.attention BertAttention start 1605233160.747458\n",
      "encoder.layer.4.attention.self BertSelfAttention start 1605233160.747565\n",
      "encoder.layer.4.attention.self.query Linear start 1605233160.747684\n",
      "encoder.layer.4.attention.self.query Linear end 1605233160.750806\n",
      "encoder.layer.4.attention.self.key Linear start 1605233160.75105\n",
      "encoder.layer.4.attention.self.key Linear end 1605233160.753715\n",
      "encoder.layer.4.attention.self.value Linear start 1605233160.7539399\n",
      "encoder.layer.4.attention.self.value Linear end 1605233160.756751\n",
      "encoder.layer.4.attention.self.dropout Dropout start 1605233160.76811\n",
      "encoder.layer.4.attention.self.dropout Dropout end 1605233160.76913\n",
      "encoder.layer.4.attention.self BertSelfAttention end 1605233160.772544\n",
      "encoder.layer.4.attention.output BertSelfOutput start 1605233160.772749\n",
      "encoder.layer.4.attention.output.dense Linear start 1605233160.772916\n",
      "encoder.layer.4.attention.output.dense Linear end 1605233160.776284\n",
      "encoder.layer.4.attention.output.dropout Dropout start 1605233160.776607\n",
      "encoder.layer.4.attention.output.dropout Dropout end 1605233160.7776692\n",
      "encoder.layer.4.attention.output.LayerNorm LayerNorm start 1605233160.778411\n",
      "encoder.layer.4.attention.output.LayerNorm LayerNorm end 1605233160.7797172\n",
      "encoder.layer.4.attention.output BertSelfOutput end 1605233160.779952\n",
      "encoder.layer.4.attention BertAttention end 1605233160.780054\n",
      "encoder.layer.4.intermediate BertIntermediate start 1605233160.7820659\n",
      "encoder.layer.4.intermediate.dense Linear start 1605233160.78232\n",
      "encoder.layer.4.intermediate.dense Linear end 1605233160.78862\n",
      "encoder.layer.4.intermediate BertIntermediate end 1605233160.7957492\n",
      "encoder.layer.4.output BertOutput start 1605233160.795952\n",
      "encoder.layer.4.output.dense Linear start 1605233160.796073\n",
      "encoder.layer.4.output.dense Linear end 1605233160.801527\n",
      "encoder.layer.4.output.dropout Dropout start 1605233160.801714\n",
      "encoder.layer.4.output.dropout Dropout end 1605233160.8026009\n",
      "encoder.layer.4.output.LayerNorm LayerNorm start 1605233160.803413\n",
      "encoder.layer.4.output.LayerNorm LayerNorm end 1605233160.8045912\n",
      "encoder.layer.4.output BertOutput end 1605233160.804856\n",
      "encoder.layer.4 BertLayer end 1605233160.804959\n",
      "encoder.layer.5 BertLayer start 1605233160.805054\n",
      "encoder.layer.5.attention BertAttention start 1605233160.805164\n",
      "encoder.layer.5.attention.self BertSelfAttention start 1605233160.8052711\n",
      "encoder.layer.5.attention.self.query Linear start 1605233160.80538\n",
      "encoder.layer.5.attention.self.query Linear end 1605233160.808952\n",
      "encoder.layer.5.attention.self.key Linear start 1605233160.80932\n",
      "encoder.layer.5.attention.self.key Linear end 1605233160.8120892\n",
      "encoder.layer.5.attention.self.value Linear start 1605233160.812397\n",
      "encoder.layer.5.attention.self.value Linear end 1605233160.8150442\n",
      "encoder.layer.5.attention.self.dropout Dropout start 1605233160.824548\n",
      "encoder.layer.5.attention.self.dropout Dropout end 1605233160.825678\n",
      "encoder.layer.5.attention.self BertSelfAttention end 1605233160.8293939\n",
      "encoder.layer.5.attention.output BertSelfOutput start 1605233160.8296409\n",
      "encoder.layer.5.attention.output.dense Linear start 1605233160.829759\n",
      "encoder.layer.5.attention.output.dense Linear end 1605233160.8324292\n",
      "encoder.layer.5.attention.output.dropout Dropout start 1605233160.8326862\n",
      "encoder.layer.5.attention.output.dropout Dropout end 1605233160.833534\n",
      "encoder.layer.5.attention.output.LayerNorm LayerNorm start 1605233160.834282\n",
      "encoder.layer.5.attention.output.LayerNorm LayerNorm end 1605233160.835577\n",
      "encoder.layer.5.attention.output BertSelfOutput end 1605233160.835973\n",
      "encoder.layer.5.attention BertAttention end 1605233160.836075\n",
      "encoder.layer.5.intermediate BertIntermediate start 1605233160.837985\n",
      "encoder.layer.5.intermediate.dense Linear start 1605233160.838242\n",
      "encoder.layer.5.intermediate.dense Linear end 1605233160.845588\n",
      "encoder.layer.5.intermediate BertIntermediate end 1605233160.852117\n",
      "encoder.layer.5.output BertOutput start 1605233160.852306\n",
      "encoder.layer.5.output.dense Linear start 1605233160.85244\n",
      "encoder.layer.5.output.dense Linear end 1605233160.858284\n",
      "encoder.layer.5.output.dropout Dropout start 1605233160.8584828\n",
      "encoder.layer.5.output.dropout Dropout end 1605233160.8594902\n",
      "encoder.layer.5.output.LayerNorm LayerNorm start 1605233160.860403\n",
      "encoder.layer.5.output.LayerNorm LayerNorm end 1605233160.8618581\n",
      "encoder.layer.5.output BertOutput end 1605233160.862101\n",
      "encoder.layer.5 BertLayer end 1605233160.862208\n",
      "encoder.layer.6 BertLayer start 1605233160.862306\n",
      "encoder.layer.6.attention BertAttention start 1605233160.862418\n",
      "encoder.layer.6.attention.self BertSelfAttention start 1605233160.862526\n",
      "encoder.layer.6.attention.self.query Linear start 1605233160.862633\n",
      "encoder.layer.6.attention.self.query Linear end 1605233160.8655438\n",
      "encoder.layer.6.attention.self.key Linear start 1605233160.865855\n",
      "encoder.layer.6.attention.self.key Linear end 1605233160.8687732\n",
      "encoder.layer.6.attention.self.value Linear start 1605233160.8691368\n",
      "encoder.layer.6.attention.self.value Linear end 1605233160.8718731\n",
      "encoder.layer.6.attention.self.dropout Dropout start 1605233160.8822029\n",
      "encoder.layer.6.attention.self.dropout Dropout end 1605233160.883003\n",
      "encoder.layer.6.attention.self BertSelfAttention end 1605233160.885931\n",
      "encoder.layer.6.attention.output BertSelfOutput start 1605233160.886189\n",
      "encoder.layer.6.attention.output.dense Linear start 1605233160.886312\n",
      "encoder.layer.6.attention.output.dense Linear end 1605233160.889309\n",
      "encoder.layer.6.attention.output.dropout Dropout start 1605233160.889676\n",
      "encoder.layer.6.attention.output.dropout Dropout end 1605233160.890635\n",
      "encoder.layer.6.attention.output.LayerNorm LayerNorm start 1605233160.891497\n",
      "encoder.layer.6.attention.output.LayerNorm LayerNorm end 1605233160.893167\n",
      "encoder.layer.6.attention.output BertSelfOutput end 1605233160.8935091\n",
      "encoder.layer.6.attention BertAttention end 1605233160.8936162\n",
      "encoder.layer.6.intermediate BertIntermediate start 1605233160.895606\n",
      "encoder.layer.6.intermediate.dense Linear start 1605233160.8958662\n",
      "encoder.layer.6.intermediate.dense Linear end 1605233160.90222\n",
      "encoder.layer.6.intermediate BertIntermediate end 1605233160.909419\n",
      "encoder.layer.6.output BertOutput start 1605233160.909707\n",
      "encoder.layer.6.output.dense Linear start 1605233160.909889\n",
      "encoder.layer.6.output.dense Linear end 1605233160.91615\n",
      "encoder.layer.6.output.dropout Dropout start 1605233160.916335\n",
      "encoder.layer.6.output.dropout Dropout end 1605233160.917197\n",
      "encoder.layer.6.output.LayerNorm LayerNorm start 1605233160.9182022\n",
      "encoder.layer.6.output.LayerNorm LayerNorm end 1605233160.919861\n",
      "encoder.layer.6.output BertOutput end 1605233160.920148\n",
      "encoder.layer.6 BertLayer end 1605233160.920254\n",
      "encoder.layer.7 BertLayer start 1605233160.920352\n",
      "encoder.layer.7.attention BertAttention start 1605233160.920483\n",
      "encoder.layer.7.attention.self BertSelfAttention start 1605233160.9205928\n",
      "encoder.layer.7.attention.self.query Linear start 1605233160.920699\n",
      "encoder.layer.7.attention.self.query Linear end 1605233160.9236538\n",
      "encoder.layer.7.attention.self.key Linear start 1605233160.923976\n",
      "encoder.layer.7.attention.self.key Linear end 1605233160.927071\n",
      "encoder.layer.7.attention.self.value Linear start 1605233160.927383\n",
      "encoder.layer.7.attention.self.value Linear end 1605233160.930847\n",
      "encoder.layer.7.attention.self.dropout Dropout start 1605233160.942026\n",
      "encoder.layer.7.attention.self.dropout Dropout end 1605233160.943486\n",
      "encoder.layer.7.attention.self BertSelfAttention end 1605233160.948282\n",
      "encoder.layer.7.attention.output BertSelfOutput start 1605233160.9485178\n",
      "encoder.layer.7.attention.output.dense Linear start 1605233160.948724\n",
      "encoder.layer.7.attention.output.dense Linear end 1605233160.95187\n",
      "encoder.layer.7.attention.output.dropout Dropout start 1605233160.952134\n",
      "encoder.layer.7.attention.output.dropout Dropout end 1605233160.953086\n",
      "encoder.layer.7.attention.output.LayerNorm LayerNorm start 1605233160.954006\n",
      "encoder.layer.7.attention.output.LayerNorm LayerNorm end 1605233160.9553332\n",
      "encoder.layer.7.attention.output BertSelfOutput end 1605233160.955928\n",
      "encoder.layer.7.attention BertAttention end 1605233160.9562979\n",
      "encoder.layer.7.intermediate BertIntermediate start 1605233160.959146\n",
      "encoder.layer.7.intermediate.dense Linear start 1605233160.95958\n",
      "encoder.layer.7.intermediate.dense Linear end 1605233160.9656582\n",
      "encoder.layer.7.intermediate BertIntermediate end 1605233160.97282\n",
      "encoder.layer.7.output BertOutput start 1605233160.973093\n",
      "encoder.layer.7.output.dense Linear start 1605233160.9732978\n",
      "encoder.layer.7.output.dense Linear end 1605233160.978868\n",
      "encoder.layer.7.output.dropout Dropout start 1605233160.979042\n",
      "encoder.layer.7.output.dropout Dropout end 1605233160.979925\n",
      "encoder.layer.7.output.LayerNorm LayerNorm start 1605233160.9805372\n",
      "encoder.layer.7.output.LayerNorm LayerNorm end 1605233160.982003\n",
      "encoder.layer.7.output BertOutput end 1605233160.9822562\n",
      "encoder.layer.7 BertLayer end 1605233160.982369\n",
      "encoder.layer.8 BertLayer start 1605233160.982477\n",
      "encoder.layer.8.attention BertAttention start 1605233160.982604\n",
      "encoder.layer.8.attention.self BertSelfAttention start 1605233160.9827268\n",
      "encoder.layer.8.attention.self.query Linear start 1605233160.98284\n",
      "encoder.layer.8.attention.self.query Linear end 1605233160.985774\n",
      "encoder.layer.8.attention.self.key Linear start 1605233160.986033\n",
      "encoder.layer.8.attention.self.key Linear end 1605233160.988541\n",
      "encoder.layer.8.attention.self.value Linear start 1605233160.9888089\n",
      "encoder.layer.8.attention.self.value Linear end 1605233160.991572\n",
      "encoder.layer.8.attention.self.dropout Dropout start 1605233161.001353\n",
      "encoder.layer.8.attention.self.dropout Dropout end 1605233161.002496\n",
      "encoder.layer.8.attention.self BertSelfAttention end 1605233161.006454\n",
      "encoder.layer.8.attention.output BertSelfOutput start 1605233161.006727\n",
      "encoder.layer.8.attention.output.dense Linear start 1605233161.006849\n",
      "encoder.layer.8.attention.output.dense Linear end 1605233161.00978\n",
      "encoder.layer.8.attention.output.dropout Dropout start 1605233161.010187\n",
      "encoder.layer.8.attention.output.dropout Dropout end 1605233161.011411\n",
      "encoder.layer.8.attention.output.LayerNorm LayerNorm start 1605233161.012286\n",
      "encoder.layer.8.attention.output.LayerNorm LayerNorm end 1605233161.013618\n",
      "encoder.layer.8.attention.output BertSelfOutput end 1605233161.013916\n",
      "encoder.layer.8.attention BertAttention end 1605233161.014078\n",
      "encoder.layer.8.intermediate BertIntermediate start 1605233161.016512\n",
      "encoder.layer.8.intermediate.dense Linear start 1605233161.0168982\n",
      "encoder.layer.8.intermediate.dense Linear end 1605233161.023148\n",
      "encoder.layer.8.intermediate BertIntermediate end 1605233161.030459\n",
      "encoder.layer.8.output BertOutput start 1605233161.030684\n",
      "encoder.layer.8.output.dense Linear start 1605233161.030814\n",
      "encoder.layer.8.output.dense Linear end 1605233161.0364091\n",
      "encoder.layer.8.output.dropout Dropout start 1605233161.03659\n",
      "encoder.layer.8.output.dropout Dropout end 1605233161.0375\n",
      "encoder.layer.8.output.LayerNorm LayerNorm start 1605233161.038264\n",
      "encoder.layer.8.output.LayerNorm LayerNorm end 1605233161.040102\n",
      "encoder.layer.8.output BertOutput end 1605233161.0403368\n",
      "encoder.layer.8 BertLayer end 1605233161.0404491\n",
      "encoder.layer.9 BertLayer start 1605233161.040552\n",
      "encoder.layer.9.attention BertAttention start 1605233161.04068\n",
      "encoder.layer.9.attention.self BertSelfAttention start 1605233161.0407958\n",
      "encoder.layer.9.attention.self.query Linear start 1605233161.040916\n",
      "encoder.layer.9.attention.self.query Linear end 1605233161.043804\n",
      "encoder.layer.9.attention.self.key Linear start 1605233161.044076\n",
      "encoder.layer.9.attention.self.key Linear end 1605233161.046884\n",
      "encoder.layer.9.attention.self.value Linear start 1605233161.047072\n",
      "encoder.layer.9.attention.self.value Linear end 1605233161.049813\n",
      "encoder.layer.9.attention.self.dropout Dropout start 1605233161.0611572\n",
      "encoder.layer.9.attention.self.dropout Dropout end 1605233161.062421\n",
      "encoder.layer.9.attention.self BertSelfAttention end 1605233161.0666702\n",
      "encoder.layer.9.attention.output BertSelfOutput start 1605233161.067018\n",
      "encoder.layer.9.attention.output.dense Linear start 1605233161.067138\n",
      "encoder.layer.9.attention.output.dense Linear end 1605233161.069748\n",
      "encoder.layer.9.attention.output.dropout Dropout start 1605233161.069995\n",
      "encoder.layer.9.attention.output.dropout Dropout end 1605233161.0706959\n",
      "encoder.layer.9.attention.output.LayerNorm LayerNorm start 1605233161.071358\n",
      "encoder.layer.9.attention.output.LayerNorm LayerNorm end 1605233161.072927\n",
      "encoder.layer.9.attention.output BertSelfOutput end 1605233161.0732381\n",
      "encoder.layer.9.attention BertAttention end 1605233161.0733519\n",
      "encoder.layer.9.intermediate BertIntermediate start 1605233161.075318\n",
      "encoder.layer.9.intermediate.dense Linear start 1605233161.07567\n",
      "encoder.layer.9.intermediate.dense Linear end 1605233161.082456\n",
      "encoder.layer.9.intermediate BertIntermediate end 1605233161.08947\n",
      "encoder.layer.9.output BertOutput start 1605233161.0896928\n",
      "encoder.layer.9.output.dense Linear start 1605233161.089869\n",
      "encoder.layer.9.output.dense Linear end 1605233161.095551\n",
      "encoder.layer.9.output.dropout Dropout start 1605233161.095731\n",
      "encoder.layer.9.output.dropout Dropout end 1605233161.096833\n",
      "encoder.layer.9.output.LayerNorm LayerNorm start 1605233161.097623\n",
      "encoder.layer.9.output.LayerNorm LayerNorm end 1605233161.098992\n",
      "encoder.layer.9.output BertOutput end 1605233161.09923\n",
      "encoder.layer.9 BertLayer end 1605233161.099349\n",
      "encoder.layer.10 BertLayer start 1605233161.099447\n",
      "encoder.layer.10.attention BertAttention start 1605233161.0995798\n",
      "encoder.layer.10.attention.self BertSelfAttention start 1605233161.099694\n",
      "encoder.layer.10.attention.self.query Linear start 1605233161.099814\n",
      "encoder.layer.10.attention.self.query Linear end 1605233161.102738\n",
      "encoder.layer.10.attention.self.key Linear start 1605233161.1029792\n",
      "encoder.layer.10.attention.self.key Linear end 1605233161.1056159\n",
      "encoder.layer.10.attention.self.value Linear start 1605233161.105938\n",
      "encoder.layer.10.attention.self.value Linear end 1605233161.10874\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder.layer.10.attention.self.dropout Dropout start 1605233161.1193442\n",
      "encoder.layer.10.attention.self.dropout Dropout end 1605233161.121492\n",
      "encoder.layer.10.attention.self BertSelfAttention end 1605233161.1254988\n",
      "encoder.layer.10.attention.output BertSelfOutput start 1605233161.12571\n",
      "encoder.layer.10.attention.output.dense Linear start 1605233161.1258721\n",
      "encoder.layer.10.attention.output.dense Linear end 1605233161.129436\n",
      "encoder.layer.10.attention.output.dropout Dropout start 1605233161.1298308\n",
      "encoder.layer.10.attention.output.dropout Dropout end 1605233161.130971\n",
      "encoder.layer.10.attention.output.LayerNorm LayerNorm start 1605233161.1318002\n",
      "encoder.layer.10.attention.output.LayerNorm LayerNorm end 1605233161.1331258\n",
      "encoder.layer.10.attention.output BertSelfOutput end 1605233161.133429\n",
      "encoder.layer.10.attention BertAttention end 1605233161.133591\n",
      "encoder.layer.10.intermediate BertIntermediate start 1605233161.135606\n",
      "encoder.layer.10.intermediate.dense Linear start 1605233161.135862\n",
      "encoder.layer.10.intermediate.dense Linear end 1605233161.142632\n",
      "encoder.layer.10.intermediate BertIntermediate end 1605233161.149208\n",
      "encoder.layer.10.output BertOutput start 1605233161.149408\n",
      "encoder.layer.10.output.dense Linear start 1605233161.149535\n",
      "encoder.layer.10.output.dense Linear end 1605233161.155073\n",
      "encoder.layer.10.output.dropout Dropout start 1605233161.1552498\n",
      "encoder.layer.10.output.dropout Dropout end 1605233161.1563148\n",
      "encoder.layer.10.output.LayerNorm LayerNorm start 1605233161.157119\n",
      "encoder.layer.10.output.LayerNorm LayerNorm end 1605233161.158392\n",
      "encoder.layer.10.output BertOutput end 1605233161.1590421\n",
      "encoder.layer.10 BertLayer end 1605233161.1592052\n",
      "encoder.layer.11 BertLayer start 1605233161.159362\n",
      "encoder.layer.11.attention BertAttention start 1605233161.1595268\n",
      "encoder.layer.11.attention.self BertSelfAttention start 1605233161.159955\n",
      "encoder.layer.11.attention.self.query Linear start 1605233161.160178\n",
      "encoder.layer.11.attention.self.query Linear end 1605233161.163692\n",
      "encoder.layer.11.attention.self.key Linear start 1605233161.163976\n",
      "encoder.layer.11.attention.self.key Linear end 1605233161.166851\n",
      "encoder.layer.11.attention.self.value Linear start 1605233161.167163\n",
      "encoder.layer.11.attention.self.value Linear end 1605233161.1701372\n",
      "encoder.layer.11.attention.self.dropout Dropout start 1605233161.180387\n",
      "encoder.layer.11.attention.self.dropout Dropout end 1605233161.181375\n",
      "encoder.layer.11.attention.self BertSelfAttention end 1605233161.184716\n",
      "encoder.layer.11.attention.output BertSelfOutput start 1605233161.184973\n",
      "encoder.layer.11.attention.output.dense Linear start 1605233161.185091\n",
      "encoder.layer.11.attention.output.dense Linear end 1605233161.187874\n",
      "encoder.layer.11.attention.output.dropout Dropout start 1605233161.188121\n",
      "encoder.layer.11.attention.output.dropout Dropout end 1605233161.189086\n",
      "encoder.layer.11.attention.output.LayerNorm LayerNorm start 1605233161.190002\n",
      "encoder.layer.11.attention.output.LayerNorm LayerNorm end 1605233161.191474\n",
      "encoder.layer.11.attention.output BertSelfOutput end 1605233161.1917639\n",
      "encoder.layer.11.attention BertAttention end 1605233161.191904\n",
      "encoder.layer.11.intermediate BertIntermediate start 1605233161.19437\n",
      "encoder.layer.11.intermediate.dense Linear start 1605233161.194636\n",
      "encoder.layer.11.intermediate.dense Linear end 1605233161.200659\n",
      "encoder.layer.11.intermediate BertIntermediate end 1605233161.207577\n",
      "encoder.layer.11.output BertOutput start 1605233161.207823\n",
      "encoder.layer.11.output.dense Linear start 1605233161.208009\n",
      "encoder.layer.11.output.dense Linear end 1605233161.2136729\n",
      "encoder.layer.11.output.dropout Dropout start 1605233161.213877\n",
      "encoder.layer.11.output.dropout Dropout end 1605233161.2147908\n",
      "encoder.layer.11.output.LayerNorm LayerNorm start 1605233161.215549\n",
      "encoder.layer.11.output.LayerNorm LayerNorm end 1605233161.216897\n",
      "encoder.layer.11.output BertOutput end 1605233161.21715\n",
      "encoder.layer.11 BertLayer end 1605233161.217254\n",
      "encoder BertEncoder end 1605233161.2173572\n",
      "pooler BertPooler start 1605233161.2174568\n",
      "pooler.dense Linear start 1605233161.217953\n",
      "pooler.dense Linear end 1605233161.219597\n",
      "pooler.activation Tanh start 1605233161.219844\n",
      "pooler.activation Tanh end 1605233161.220193\n",
      "pooler BertPooler end 1605233161.220299\n",
      " BertModel end 1605233161.220403\n",
      " BertModel start 1605233161.990131\n",
      "embeddings BertEmbeddings start 1605233161.9914398\n",
      "embeddings.word_embeddings Embedding start 1605233161.991789\n",
      "embeddings.word_embeddings Embedding end 1605233161.992162\n",
      "embeddings.position_embeddings Embedding start 1605233161.99232\n",
      "embeddings.position_embeddings Embedding end 1605233161.9926789\n",
      "embeddings.token_type_embeddings Embedding start 1605233161.9930198\n",
      "embeddings.token_type_embeddings Embedding end 1605233161.993373\n",
      "embeddings.LayerNorm LayerNorm start 1605233161.994097\n",
      "embeddings.LayerNorm LayerNorm end 1605233161.994621\n",
      "embeddings.dropout Dropout start 1605233161.994747\n",
      "embeddings.dropout Dropout end 1605233161.994926\n",
      "embeddings BertEmbeddings end 1605233161.9950452\n",
      "encoder BertEncoder start 1605233161.9951649\n",
      "encoder.layer.0 BertLayer start 1605233161.995274\n",
      "encoder.layer.0.attention BertAttention start 1605233161.9955\n",
      "encoder.layer.0.attention.self BertSelfAttention start 1605233161.995595\n",
      "encoder.layer.0.attention.self.query Linear start 1605233161.995692\n",
      "encoder.layer.0.attention.self.query Linear end 1605233161.9974232\n",
      "encoder.layer.0.attention.self.key Linear start 1605233161.9977548\n",
      "encoder.layer.0.attention.self.key Linear end 1605233161.999541\n",
      "encoder.layer.0.attention.self.value Linear start 1605233161.999795\n",
      "encoder.layer.0.attention.self.value Linear end 1605233162.001641\n",
      "encoder.layer.0.attention.self.dropout Dropout start 1605233162.0042171\n",
      "encoder.layer.0.attention.self.dropout Dropout end 1605233162.0046499\n",
      "encoder.layer.0.attention.self BertSelfAttention end 1605233162.005669\n",
      "encoder.layer.0.attention.output BertSelfOutput start 1605233162.006001\n",
      "encoder.layer.0.attention.output.dense Linear start 1605233162.0061498\n",
      "encoder.layer.0.attention.output.dense Linear end 1605233162.0083158\n",
      "encoder.layer.0.attention.output.dropout Dropout start 1605233162.0086222\n",
      "encoder.layer.0.attention.output.dropout Dropout end 1605233162.008793\n",
      "encoder.layer.0.attention.output.LayerNorm LayerNorm start 1605233162.0091069\n",
      "encoder.layer.0.attention.output.LayerNorm LayerNorm end 1605233162.00945\n",
      "encoder.layer.0.attention.output BertSelfOutput end 1605233162.009757\n",
      "encoder.layer.0.attention BertAttention end 1605233162.009932\n",
      "encoder.layer.0.intermediate BertIntermediate start 1605233162.0101671\n",
      "encoder.layer.0.intermediate.dense Linear start 1605233162.010323\n",
      "encoder.layer.0.intermediate.dense Linear end 1605233162.01568\n",
      "encoder.layer.0.intermediate BertIntermediate end 1605233162.0200472\n",
      "encoder.layer.0.output BertOutput start 1605233162.020319\n",
      "encoder.layer.0.output.dense Linear start 1605233162.020428\n",
      "encoder.layer.0.output.dense Linear end 1605233162.025528\n",
      "encoder.layer.0.output.dropout Dropout start 1605233162.025805\n",
      "encoder.layer.0.output.dropout Dropout end 1605233162.026082\n",
      "encoder.layer.0.output.LayerNorm LayerNorm start 1605233162.026339\n",
      "encoder.layer.0.output.LayerNorm LayerNorm end 1605233162.026706\n",
      "encoder.layer.0.output BertOutput end 1605233162.026997\n",
      "encoder.layer.0 BertLayer end 1605233162.027115\n",
      "encoder.layer.1 BertLayer start 1605233162.027219\n",
      "encoder.layer.1.attention BertAttention start 1605233162.027318\n",
      "encoder.layer.1.attention.self BertSelfAttention start 1605233162.027423\n",
      "encoder.layer.1.attention.self.query Linear start 1605233162.0275662\n",
      "encoder.layer.1.attention.self.query Linear end 1605233162.029523\n",
      "encoder.layer.1.attention.self.key Linear start 1605233162.0297651\n",
      "encoder.layer.1.attention.self.key Linear end 1605233162.03159\n",
      "encoder.layer.1.attention.self.value Linear start 1605233162.031849\n",
      "encoder.layer.1.attention.self.value Linear end 1605233162.033584\n",
      "encoder.layer.1.attention.self.dropout Dropout start 1605233162.036273\n",
      "encoder.layer.1.attention.self.dropout Dropout end 1605233162.0365849\n",
      "encoder.layer.1.attention.self BertSelfAttention end 1605233162.0376818\n",
      "encoder.layer.1.attention.output BertSelfOutput start 1605233162.0379782\n",
      "encoder.layer.1.attention.output.dense Linear start 1605233162.0381608\n",
      "encoder.layer.1.attention.output.dense Linear end 1605233162.03999\n",
      "encoder.layer.1.attention.output.dropout Dropout start 1605233162.040318\n",
      "encoder.layer.1.attention.output.dropout Dropout end 1605233162.040576\n",
      "encoder.layer.1.attention.output.LayerNorm LayerNorm start 1605233162.040843\n",
      "encoder.layer.1.attention.output.LayerNorm LayerNorm end 1605233162.041148\n",
      "encoder.layer.1.attention.output BertSelfOutput end 1605233162.041397\n",
      "encoder.layer.1.attention BertAttention end 1605233162.041554\n",
      "encoder.layer.1.intermediate BertIntermediate start 1605233162.041736\n",
      "encoder.layer.1.intermediate.dense Linear start 1605233162.041831\n",
      "encoder.layer.1.intermediate.dense Linear end 1605233162.046852\n",
      "encoder.layer.1.intermediate BertIntermediate end 1605233162.051336\n",
      "encoder.layer.1.output BertOutput start 1605233162.05161\n",
      "encoder.layer.1.output.dense Linear start 1605233162.051724\n",
      "encoder.layer.1.output.dense Linear end 1605233162.057065\n",
      "encoder.layer.1.output.dropout Dropout start 1605233162.057367\n",
      "encoder.layer.1.output.dropout Dropout end 1605233162.057497\n",
      "encoder.layer.1.output.LayerNorm LayerNorm start 1605233162.057751\n",
      "encoder.layer.1.output.LayerNorm LayerNorm end 1605233162.058076\n",
      "encoder.layer.1.output BertOutput end 1605233162.0581849\n",
      "encoder.layer.1 BertLayer end 1605233162.0584488\n",
      "encoder.layer.2 BertLayer start 1605233162.058607\n",
      "encoder.layer.2.attention BertAttention start 1605233162.058762\n",
      "encoder.layer.2.attention.self BertSelfAttention start 1605233162.0589151\n",
      "encoder.layer.2.attention.self.query Linear start 1605233162.059063\n",
      "encoder.layer.2.attention.self.query Linear end 1605233162.0609128\n",
      "encoder.layer.2.attention.self.key Linear start 1605233162.0611682\n",
      "encoder.layer.2.attention.self.key Linear end 1605233162.0628622\n",
      "encoder.layer.2.attention.self.value Linear start 1605233162.063123\n",
      "encoder.layer.2.attention.self.value Linear end 1605233162.064748\n",
      "encoder.layer.2.attention.self.dropout Dropout start 1605233162.0672488\n",
      "encoder.layer.2.attention.self.dropout Dropout end 1605233162.0676632\n",
      "encoder.layer.2.attention.self BertSelfAttention end 1605233162.068418\n",
      "encoder.layer.2.attention.output BertSelfOutput start 1605233162.068704\n",
      "encoder.layer.2.attention.output.dense Linear start 1605233162.0688589\n",
      "encoder.layer.2.attention.output.dense Linear end 1605233162.0706198\n",
      "encoder.layer.2.attention.output.dropout Dropout start 1605233162.070864\n",
      "encoder.layer.2.attention.output.dropout Dropout end 1605233162.070975\n",
      "encoder.layer.2.attention.output.LayerNorm LayerNorm start 1605233162.071229\n",
      "encoder.layer.2.attention.output.LayerNorm LayerNorm end 1605233162.0715\n",
      "encoder.layer.2.attention.output BertSelfOutput end 1605233162.0716019\n",
      "encoder.layer.2.attention BertAttention end 1605233162.071698\n",
      "encoder.layer.2.intermediate BertIntermediate start 1605233162.071882\n",
      "encoder.layer.2.intermediate.dense Linear start 1605233162.072122\n",
      "encoder.layer.2.intermediate.dense Linear end 1605233162.077226\n",
      "encoder.layer.2.intermediate BertIntermediate end 1605233162.081706\n",
      "encoder.layer.2.output BertOutput start 1605233162.0819688\n",
      "encoder.layer.2.output.dense Linear start 1605233162.082068\n",
      "encoder.layer.2.output.dense Linear end 1605233162.087303\n",
      "encoder.layer.2.output.dropout Dropout start 1605233162.087563\n",
      "encoder.layer.2.output.dropout Dropout end 1605233162.0876842\n",
      "encoder.layer.2.output.LayerNorm LayerNorm start 1605233162.087938\n",
      "encoder.layer.2.output.LayerNorm LayerNorm end 1605233162.088202\n",
      "encoder.layer.2.output BertOutput end 1605233162.088306\n",
      "encoder.layer.2 BertLayer end 1605233162.088407\n",
      "encoder.layer.3 BertLayer start 1605233162.0885031\n",
      "encoder.layer.3.attention BertAttention start 1605233162.088726\n",
      "encoder.layer.3.attention.self BertSelfAttention start 1605233162.088819\n",
      "encoder.layer.3.attention.self.query Linear start 1605233162.0889332\n",
      "encoder.layer.3.attention.self.query Linear end 1605233162.090709\n",
      "encoder.layer.3.attention.self.key Linear start 1605233162.090961\n",
      "encoder.layer.3.attention.self.key Linear end 1605233162.092629\n",
      "encoder.layer.3.attention.self.value Linear start 1605233162.0929291\n",
      "encoder.layer.3.attention.self.value Linear end 1605233162.094815\n",
      "encoder.layer.3.attention.self.dropout Dropout start 1605233162.0976262\n",
      "encoder.layer.3.attention.self.dropout Dropout end 1605233162.097976\n",
      "encoder.layer.3.attention.self BertSelfAttention end 1605233162.099096\n",
      "encoder.layer.3.attention.output BertSelfOutput start 1605233162.099388\n",
      "encoder.layer.3.attention.output.dense Linear start 1605233162.099545\n",
      "encoder.layer.3.attention.output.dense Linear end 1605233162.101509\n",
      "encoder.layer.3.attention.output.dropout Dropout start 1605233162.101834\n",
      "encoder.layer.3.attention.output.dropout Dropout end 1605233162.102005\n",
      "encoder.layer.3.attention.output.LayerNorm LayerNorm start 1605233162.102287\n",
      "encoder.layer.3.attention.output.LayerNorm LayerNorm end 1605233162.102543\n",
      "encoder.layer.3.attention.output BertSelfOutput end 1605233162.102685\n",
      "encoder.layer.3.attention BertAttention end 1605233162.1029482\n",
      "encoder.layer.3.intermediate BertIntermediate start 1605233162.103144\n",
      "encoder.layer.3.intermediate.dense Linear start 1605233162.103241\n",
      "encoder.layer.3.intermediate.dense Linear end 1605233162.1083572\n",
      "encoder.layer.3.intermediate BertIntermediate end 1605233162.1130729\n",
      "encoder.layer.3.output BertOutput start 1605233162.1133409\n",
      "encoder.layer.3.output.dense Linear start 1605233162.1134489\n",
      "encoder.layer.3.output.dense Linear end 1605233162.1184661\n",
      "encoder.layer.3.output.dropout Dropout start 1605233162.118827\n",
      "encoder.layer.3.output.dropout Dropout end 1605233162.119137\n",
      "encoder.layer.3.output.LayerNorm LayerNorm start 1605233162.119394\n",
      "encoder.layer.3.output.LayerNorm LayerNorm end 1605233162.119745\n",
      "encoder.layer.3.output BertOutput end 1605233162.119999\n",
      "encoder.layer.3 BertLayer end 1605233162.120099\n",
      "encoder.layer.4 BertLayer start 1605233162.120206\n",
      "encoder.layer.4.attention BertAttention start 1605233162.12031\n",
      "encoder.layer.4.attention.self BertSelfAttention start 1605233162.1204128\n",
      "encoder.layer.4.attention.self.query Linear start 1605233162.1205199\n",
      "encoder.layer.4.attention.self.query Linear end 1605233162.122255\n",
      "encoder.layer.4.attention.self.key Linear start 1605233162.122453\n",
      "encoder.layer.4.attention.self.key Linear end 1605233162.1240551\n",
      "encoder.layer.4.attention.self.value Linear start 1605233162.124371\n",
      "encoder.layer.4.attention.self.value Linear end 1605233162.126133\n",
      "encoder.layer.4.attention.self.dropout Dropout start 1605233162.128638\n",
      "encoder.layer.4.attention.self.dropout Dropout end 1605233162.12892\n",
      "encoder.layer.4.attention.self BertSelfAttention end 1605233162.129757\n",
      "encoder.layer.4.attention.output BertSelfOutput start 1605233162.130018\n",
      "encoder.layer.4.attention.output.dense Linear start 1605233162.130111\n",
      "encoder.layer.4.attention.output.dense Linear end 1605233162.131789\n",
      "encoder.layer.4.attention.output.dropout Dropout start 1605233162.1320739\n",
      "encoder.layer.4.attention.output.dropout Dropout end 1605233162.132313\n",
      "encoder.layer.4.attention.output.LayerNorm LayerNorm start 1605233162.132555\n",
      "encoder.layer.4.attention.output.LayerNorm LayerNorm end 1605233162.1327438\n",
      "encoder.layer.4.attention.output BertSelfOutput end 1605233162.1328561\n",
      "encoder.layer.4.attention BertAttention end 1605233162.1329532\n",
      "encoder.layer.4.intermediate BertIntermediate start 1605233162.1333592\n",
      "encoder.layer.4.intermediate.dense Linear start 1605233162.133465\n",
      "encoder.layer.4.intermediate.dense Linear end 1605233162.138735\n",
      "encoder.layer.4.intermediate BertIntermediate end 1605233162.14349\n",
      "encoder.layer.4.output BertOutput start 1605233162.143831\n",
      "encoder.layer.4.output.dense Linear start 1605233162.143977\n",
      "encoder.layer.4.output.dense Linear end 1605233162.149316\n",
      "encoder.layer.4.output.dropout Dropout start 1605233162.14963\n",
      "encoder.layer.4.output.dropout Dropout end 1605233162.149992\n",
      "encoder.layer.4.output.LayerNorm LayerNorm start 1605233162.1504178\n",
      "encoder.layer.4.output.LayerNorm LayerNorm end 1605233162.1508741\n",
      "encoder.layer.4.output BertOutput end 1605233162.15098\n",
      "encoder.layer.4 BertLayer end 1605233162.151078\n",
      "encoder.layer.5 BertLayer start 1605233162.151186\n",
      "encoder.layer.5.attention BertAttention start 1605233162.1512809\n",
      "encoder.layer.5.attention.self BertSelfAttention start 1605233162.151374\n",
      "encoder.layer.5.attention.self.query Linear start 1605233162.151467\n",
      "encoder.layer.5.attention.self.query Linear end 1605233162.153204\n",
      "encoder.layer.5.attention.self.key Linear start 1605233162.153516\n",
      "encoder.layer.5.attention.self.key Linear end 1605233162.155205\n",
      "encoder.layer.5.attention.self.value Linear start 1605233162.155493\n",
      "encoder.layer.5.attention.self.value Linear end 1605233162.1571321\n",
      "encoder.layer.5.attention.self.dropout Dropout start 1605233162.15975\n",
      "encoder.layer.5.attention.self.dropout Dropout end 1605233162.160002\n",
      "encoder.layer.5.attention.self BertSelfAttention end 1605233162.160863\n",
      "encoder.layer.5.attention.output BertSelfOutput start 1605233162.161144\n",
      "encoder.layer.5.attention.output.dense Linear start 1605233162.1612449\n",
      "encoder.layer.5.attention.output.dense Linear end 1605233162.162938\n",
      "encoder.layer.5.attention.output.dropout Dropout start 1605233162.1633081\n",
      "encoder.layer.5.attention.output.dropout Dropout end 1605233162.163433\n",
      "encoder.layer.5.attention.output.LayerNorm LayerNorm start 1605233162.163689\n",
      "encoder.layer.5.attention.output.LayerNorm LayerNorm end 1605233162.1639302\n",
      "encoder.layer.5.attention.output BertSelfOutput end 1605233162.1640298\n",
      "encoder.layer.5.attention BertAttention end 1605233162.164128\n",
      "encoder.layer.5.intermediate BertIntermediate start 1605233162.1644568\n",
      "encoder.layer.5.intermediate.dense Linear start 1605233162.164565\n",
      "encoder.layer.5.intermediate.dense Linear end 1605233162.169182\n",
      "encoder.layer.5.intermediate BertIntermediate end 1605233162.1746922\n",
      "encoder.layer.5.output BertOutput start 1605233162.175033\n",
      "encoder.layer.5.output.dense Linear start 1605233162.17517\n",
      "encoder.layer.5.output.dense Linear end 1605233162.180483\n",
      "encoder.layer.5.output.dropout Dropout start 1605233162.1807842\n",
      "encoder.layer.5.output.dropout Dropout end 1605233162.181032\n",
      "encoder.layer.5.output.LayerNorm LayerNorm start 1605233162.1813672\n",
      "encoder.layer.5.output.LayerNorm LayerNorm end 1605233162.1816928\n",
      "encoder.layer.5.output BertOutput end 1605233162.1819289\n",
      "encoder.layer.5 BertLayer end 1605233162.182026\n",
      "encoder.layer.6 BertLayer start 1605233162.182126\n",
      "encoder.layer.6.attention BertAttention start 1605233162.1822221\n",
      "encoder.layer.6.attention.self BertSelfAttention start 1605233162.182315\n",
      "encoder.layer.6.attention.self.query Linear start 1605233162.182409\n",
      "encoder.layer.6.attention.self.query Linear end 1605233162.1841228\n",
      "encoder.layer.6.attention.self.key Linear start 1605233162.184352\n",
      "encoder.layer.6.attention.self.key Linear end 1605233162.185995\n",
      "encoder.layer.6.attention.self.value Linear start 1605233162.18622\n",
      "encoder.layer.6.attention.self.value Linear end 1605233162.1878452\n",
      "encoder.layer.6.attention.self.dropout Dropout start 1605233162.190228\n",
      "encoder.layer.6.attention.self.dropout Dropout end 1605233162.190627\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder.layer.6.attention.self BertSelfAttention end 1605233162.191394\n",
      "encoder.layer.6.attention.output BertSelfOutput start 1605233162.191551\n",
      "encoder.layer.6.attention.output.dense Linear start 1605233162.191653\n",
      "encoder.layer.6.attention.output.dense Linear end 1605233162.193556\n",
      "encoder.layer.6.attention.output.dropout Dropout start 1605233162.1939359\n",
      "encoder.layer.6.attention.output.dropout Dropout end 1605233162.1940532\n",
      "encoder.layer.6.attention.output.LayerNorm LayerNorm start 1605233162.194396\n",
      "encoder.layer.6.attention.output.LayerNorm LayerNorm end 1605233162.194803\n",
      "encoder.layer.6.attention.output BertSelfOutput end 1605233162.195133\n",
      "encoder.layer.6.attention BertAttention end 1605233162.19524\n",
      "encoder.layer.6.intermediate BertIntermediate start 1605233162.195433\n",
      "encoder.layer.6.intermediate.dense Linear start 1605233162.195544\n",
      "encoder.layer.6.intermediate.dense Linear end 1605233162.201525\n",
      "encoder.layer.6.intermediate BertIntermediate end 1605233162.2062552\n",
      "encoder.layer.6.output BertOutput start 1605233162.206744\n",
      "encoder.layer.6.output.dense Linear start 1605233162.2068968\n",
      "encoder.layer.6.output.dense Linear end 1605233162.213151\n",
      "encoder.layer.6.output.dropout Dropout start 1605233162.213585\n",
      "encoder.layer.6.output.dropout Dropout end 1605233162.2137449\n",
      "encoder.layer.6.output.LayerNorm LayerNorm start 1605233162.214182\n",
      "encoder.layer.6.output.LayerNorm LayerNorm end 1605233162.214596\n",
      "encoder.layer.6.output BertOutput end 1605233162.214995\n",
      "encoder.layer.6 BertLayer end 1605233162.215155\n",
      "encoder.layer.7 BertLayer start 1605233162.215306\n",
      "encoder.layer.7.attention BertAttention start 1605233162.215466\n",
      "encoder.layer.7.attention.self BertSelfAttention start 1605233162.215617\n",
      "encoder.layer.7.attention.self.query Linear start 1605233162.2157838\n",
      "encoder.layer.7.attention.self.query Linear end 1605233162.218177\n",
      "encoder.layer.7.attention.self.key Linear start 1605233162.2185411\n",
      "encoder.layer.7.attention.self.key Linear end 1605233162.220469\n",
      "encoder.layer.7.attention.self.value Linear start 1605233162.2209659\n",
      "encoder.layer.7.attention.self.value Linear end 1605233162.223503\n",
      "encoder.layer.7.attention.self.dropout Dropout start 1605233162.2281759\n",
      "encoder.layer.7.attention.self.dropout Dropout end 1605233162.228786\n",
      "encoder.layer.7.attention.self BertSelfAttention end 1605233162.230766\n",
      "encoder.layer.7.attention.output BertSelfOutput start 1605233162.2310882\n",
      "encoder.layer.7.attention.output.dense Linear start 1605233162.2312489\n",
      "encoder.layer.7.attention.output.dense Linear end 1605233162.233782\n",
      "encoder.layer.7.attention.output.dropout Dropout start 1605233162.234252\n",
      "encoder.layer.7.attention.output.dropout Dropout end 1605233162.2344372\n",
      "encoder.layer.7.attention.output.LayerNorm LayerNorm start 1605233162.234872\n",
      "encoder.layer.7.attention.output.LayerNorm LayerNorm end 1605233162.235231\n",
      "encoder.layer.7.attention.output BertSelfOutput end 1605233162.235552\n",
      "encoder.layer.7.attention BertAttention end 1605233162.235648\n",
      "encoder.layer.7.intermediate BertIntermediate start 1605233162.23583\n",
      "encoder.layer.7.intermediate.dense Linear start 1605233162.235928\n",
      "encoder.layer.7.intermediate.dense Linear end 1605233162.241484\n",
      "encoder.layer.7.intermediate BertIntermediate end 1605233162.246352\n",
      "encoder.layer.7.output BertOutput start 1605233162.246746\n",
      "encoder.layer.7.output.dense Linear start 1605233162.246846\n",
      "encoder.layer.7.output.dense Linear end 1605233162.252413\n",
      "encoder.layer.7.output.dropout Dropout start 1605233162.2527769\n",
      "encoder.layer.7.output.dropout Dropout end 1605233162.253113\n",
      "encoder.layer.7.output.LayerNorm LayerNorm start 1605233162.25346\n",
      "encoder.layer.7.output.LayerNorm LayerNorm end 1605233162.25383\n",
      "encoder.layer.7.output BertOutput end 1605233162.25422\n",
      "encoder.layer.7 BertLayer end 1605233162.254378\n",
      "encoder.layer.8 BertLayer start 1605233162.254538\n",
      "encoder.layer.8.attention BertAttention start 1605233162.254695\n",
      "encoder.layer.8.attention.self BertSelfAttention start 1605233162.2548501\n",
      "encoder.layer.8.attention.self.query Linear start 1605233162.2550058\n",
      "encoder.layer.8.attention.self.query Linear end 1605233162.257191\n",
      "encoder.layer.8.attention.self.key Linear start 1605233162.257536\n",
      "encoder.layer.8.attention.self.key Linear end 1605233162.259386\n",
      "encoder.layer.8.attention.self.value Linear start 1605233162.259732\n",
      "encoder.layer.8.attention.self.value Linear end 1605233162.261593\n",
      "encoder.layer.8.attention.self.dropout Dropout start 1605233162.264727\n",
      "encoder.layer.8.attention.self.dropout Dropout end 1605233162.265146\n",
      "encoder.layer.8.attention.self BertSelfAttention end 1605233162.26629\n",
      "encoder.layer.8.attention.output BertSelfOutput start 1605233162.266609\n",
      "encoder.layer.8.attention.output.dense Linear start 1605233162.2667022\n",
      "encoder.layer.8.attention.output.dense Linear end 1605233162.268526\n",
      "encoder.layer.8.attention.output.dropout Dropout start 1605233162.2688792\n",
      "encoder.layer.8.attention.output.dropout Dropout end 1605233162.269218\n",
      "encoder.layer.8.attention.output.LayerNorm LayerNorm start 1605233162.2695549\n",
      "encoder.layer.8.attention.output.LayerNorm LayerNorm end 1605233162.269902\n",
      "encoder.layer.8.attention.output BertSelfOutput end 1605233162.2702148\n",
      "encoder.layer.8.attention BertAttention end 1605233162.2703118\n",
      "encoder.layer.8.intermediate BertIntermediate start 1605233162.270514\n",
      "encoder.layer.8.intermediate.dense Linear start 1605233162.2706182\n",
      "encoder.layer.8.intermediate.dense Linear end 1605233162.275638\n",
      "encoder.layer.8.intermediate BertIntermediate end 1605233162.280989\n",
      "encoder.layer.8.output BertOutput start 1605233162.281334\n",
      "encoder.layer.8.output.dense Linear start 1605233162.281433\n",
      "encoder.layer.8.output.dense Linear end 1605233162.2870662\n",
      "encoder.layer.8.output.dropout Dropout start 1605233162.287457\n",
      "encoder.layer.8.output.dropout Dropout end 1605233162.287585\n",
      "encoder.layer.8.output.LayerNorm LayerNorm start 1605233162.287982\n",
      "encoder.layer.8.output.LayerNorm LayerNorm end 1605233162.288216\n",
      "encoder.layer.8.output BertOutput end 1605233162.288502\n",
      "encoder.layer.8 BertLayer end 1605233162.288608\n",
      "encoder.layer.9 BertLayer start 1605233162.288714\n",
      "encoder.layer.9.attention BertAttention start 1605233162.2888079\n",
      "encoder.layer.9.attention.self BertSelfAttention start 1605233162.288901\n",
      "encoder.layer.9.attention.self.query Linear start 1605233162.288991\n",
      "encoder.layer.9.attention.self.query Linear end 1605233162.2911842\n",
      "encoder.layer.9.attention.self.key Linear start 1605233162.2915149\n",
      "encoder.layer.9.attention.self.key Linear end 1605233162.293348\n",
      "encoder.layer.9.attention.self.value Linear start 1605233162.293705\n",
      "encoder.layer.9.attention.self.value Linear end 1605233162.29553\n",
      "encoder.layer.9.attention.self.dropout Dropout start 1605233162.298611\n",
      "encoder.layer.9.attention.self.dropout Dropout end 1605233162.298825\n",
      "encoder.layer.9.attention.self BertSelfAttention end 1605233162.300032\n",
      "encoder.layer.9.attention.output BertSelfOutput start 1605233162.3003361\n",
      "encoder.layer.9.attention.output.dense Linear start 1605233162.300429\n",
      "encoder.layer.9.attention.output.dense Linear end 1605233162.302647\n",
      "encoder.layer.9.attention.output.dropout Dropout start 1605233162.30303\n",
      "encoder.layer.9.attention.output.dropout Dropout end 1605233162.3031528\n",
      "encoder.layer.9.attention.output.LayerNorm LayerNorm start 1605233162.3037891\n",
      "encoder.layer.9.attention.output.LayerNorm LayerNorm end 1605233162.304476\n",
      "encoder.layer.9.attention.output BertSelfOutput end 1605233162.3045828\n",
      "encoder.layer.9.attention BertAttention end 1605233162.304682\n",
      "encoder.layer.9.intermediate BertIntermediate start 1605233162.304867\n",
      "encoder.layer.9.intermediate.dense Linear start 1605233162.304965\n",
      "encoder.layer.9.intermediate.dense Linear end 1605233162.3104482\n",
      "encoder.layer.9.intermediate BertIntermediate end 1605233162.3149781\n",
      "encoder.layer.9.output BertOutput start 1605233162.315559\n",
      "encoder.layer.9.output.dense Linear start 1605233162.315657\n",
      "encoder.layer.9.output.dense Linear end 1605233162.321236\n",
      "encoder.layer.9.output.dropout Dropout start 1605233162.321573\n",
      "encoder.layer.9.output.dropout Dropout end 1605233162.321693\n",
      "encoder.layer.9.output.LayerNorm LayerNorm start 1605233162.322015\n",
      "encoder.layer.9.output.LayerNorm LayerNorm end 1605233162.32235\n",
      "encoder.layer.9.output BertOutput end 1605233162.322656\n",
      "encoder.layer.9 BertLayer end 1605233162.323039\n",
      "encoder.layer.10 BertLayer start 1605233162.3231769\n",
      "encoder.layer.10.attention BertAttention start 1605233162.3232992\n",
      "encoder.layer.10.attention.self BertSelfAttention start 1605233162.3234072\n",
      "encoder.layer.10.attention.self.query Linear start 1605233162.323503\n",
      "encoder.layer.10.attention.self.query Linear end 1605233162.325622\n",
      "encoder.layer.10.attention.self.key Linear start 1605233162.325988\n",
      "encoder.layer.10.attention.self.key Linear end 1605233162.32783\n",
      "encoder.layer.10.attention.self.value Linear start 1605233162.328157\n",
      "encoder.layer.10.attention.self.value Linear end 1605233162.330045\n",
      "encoder.layer.10.attention.self.dropout Dropout start 1605233162.333024\n",
      "encoder.layer.10.attention.self.dropout Dropout end 1605233162.3333561\n",
      "encoder.layer.10.attention.self BertSelfAttention end 1605233162.33462\n",
      "encoder.layer.10.attention.output BertSelfOutput start 1605233162.334922\n",
      "encoder.layer.10.attention.output.dense Linear start 1605233162.3350248\n",
      "encoder.layer.10.attention.output.dense Linear end 1605233162.336885\n",
      "encoder.layer.10.attention.output.dropout Dropout start 1605233162.3371992\n",
      "encoder.layer.10.attention.output.dropout Dropout end 1605233162.337312\n",
      "encoder.layer.10.attention.output.LayerNorm LayerNorm start 1605233162.3376498\n",
      "encoder.layer.10.attention.output.LayerNorm LayerNorm end 1605233162.337975\n",
      "encoder.layer.10.attention.output BertSelfOutput end 1605233162.3380759\n",
      "encoder.layer.10.attention BertAttention end 1605233162.338476\n",
      "encoder.layer.10.intermediate BertIntermediate start 1605233162.338711\n",
      "encoder.layer.10.intermediate.dense Linear start 1605233162.338867\n",
      "encoder.layer.10.intermediate.dense Linear end 1605233162.345335\n",
      "encoder.layer.10.intermediate BertIntermediate end 1605233162.350241\n",
      "encoder.layer.10.output BertOutput start 1605233162.3507168\n",
      "encoder.layer.10.output.dense Linear start 1605233162.3508658\n",
      "encoder.layer.10.output.dense Linear end 1605233162.35675\n",
      "encoder.layer.10.output.dropout Dropout start 1605233162.357172\n",
      "encoder.layer.10.output.dropout Dropout end 1605233162.357297\n",
      "encoder.layer.10.output.LayerNorm LayerNorm start 1605233162.3576388\n",
      "encoder.layer.10.output.LayerNorm LayerNorm end 1605233162.358017\n",
      "encoder.layer.10.output BertOutput end 1605233162.358426\n",
      "encoder.layer.10 BertLayer end 1605233162.358568\n",
      "encoder.layer.11 BertLayer start 1605233162.358717\n",
      "encoder.layer.11.attention BertAttention start 1605233162.3588538\n",
      "encoder.layer.11.attention.self BertSelfAttention start 1605233162.358969\n",
      "encoder.layer.11.attention.self.query Linear start 1605233162.359066\n",
      "encoder.layer.11.attention.self.query Linear end 1605233162.361297\n",
      "encoder.layer.11.attention.self.key Linear start 1605233162.361726\n",
      "encoder.layer.11.attention.self.key Linear end 1605233162.363866\n",
      "encoder.layer.11.attention.self.value Linear start 1605233162.364241\n",
      "encoder.layer.11.attention.self.value Linear end 1605233162.366428\n",
      "encoder.layer.11.attention.self.dropout Dropout start 1605233162.3698728\n",
      "encoder.layer.11.attention.self.dropout Dropout end 1605233162.370535\n",
      "encoder.layer.11.attention.self BertSelfAttention end 1605233162.371926\n",
      "encoder.layer.11.attention.output BertSelfOutput start 1605233162.372365\n",
      "encoder.layer.11.attention.output.dense Linear start 1605233162.372531\n",
      "encoder.layer.11.attention.output.dense Linear end 1605233162.3750029\n",
      "encoder.layer.11.attention.output.dropout Dropout start 1605233162.37539\n",
      "encoder.layer.11.attention.output.dropout Dropout end 1605233162.375753\n",
      "encoder.layer.11.attention.output.LayerNorm LayerNorm start 1605233162.376182\n",
      "encoder.layer.11.attention.output.LayerNorm LayerNorm end 1605233162.3767831\n",
      "encoder.layer.11.attention.output BertSelfOutput end 1605233162.37696\n",
      "encoder.layer.11.attention BertAttention end 1605233162.3771331\n",
      "encoder.layer.11.intermediate BertIntermediate start 1605233162.377375\n",
      "encoder.layer.11.intermediate.dense Linear start 1605233162.377802\n",
      "encoder.layer.11.intermediate.dense Linear end 1605233162.3835561\n",
      "encoder.layer.11.intermediate BertIntermediate end 1605233162.388265\n",
      "encoder.layer.11.output BertOutput start 1605233162.388607\n",
      "encoder.layer.11.output.dense Linear start 1605233162.388708\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder.layer.11.output.dense Linear end 1605233162.39436\n",
      "encoder.layer.11.output.dropout Dropout start 1605233162.394857\n",
      "encoder.layer.11.output.dropout Dropout end 1605233162.3949811\n",
      "encoder.layer.11.output.LayerNorm LayerNorm start 1605233162.395479\n",
      "encoder.layer.11.output.LayerNorm LayerNorm end 1605233162.396487\n",
      "encoder.layer.11.output BertOutput end 1605233162.3974621\n",
      "encoder.layer.11 BertLayer end 1605233162.397602\n",
      "encoder BertEncoder end 1605233162.397761\n",
      "pooler BertPooler start 1605233162.3979242\n",
      "pooler.dense Linear start 1605233162.3981068\n",
      "pooler.dense Linear end 1605233162.399553\n",
      "pooler.activation Tanh start 1605233162.4000041\n",
      "pooler.activation Tanh end 1605233162.400415\n",
      "pooler BertPooler end 1605233162.4005141\n",
      " BertModel end 1605233162.4006221\n"
     ]
    }
   ],
   "source": [
    "trace = torch.jit.trace(model, inputs) # 1605233162.4006221"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"_name_or_path\": \"prajjwal1/bert-tiny\",\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"gradient_checkpointing\": false,\n",
       "  \"hidden_act\": \"gelu_fast\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 128,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 512,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 2,\n",
       "  \"num_hidden_layers\": 2,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"torchscript\": true,\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"vocab_size\": 30522\n",
       "}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config=model.config\n",
    "config.hidden_act = 'gelu_fast'\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from transformers.activations import ACT2FN\n",
    "\n",
    "class BertIntermediate(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)\n",
    "        if isinstance(config.hidden_act, str):\n",
    "            self.intermediate_act_fn = ACT2FN[config.hidden_act]\n",
    "        else:\n",
    "            self.intermediate_act_fn = config.hidden_act\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "#         hidden_states = self.intermediate_act_fn(hidden_states)\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertIntermediate(\n",
       "  (dense): Linear(in_features=128, out_features=512, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc_model = BertIntermediate(config)\n",
    "input_len = 32\n",
    "input_states = torch.rand((input_len, config.hidden_size))\n",
    "fc_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_trace = torch.jit.trace(fc_model, input_states)\n",
    "fc_graph = fc_trace.inlined_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "graph(%self.1 : __torch__.BertIntermediate,\n",
       "      %input : Float(32:128, 128:1)):\n",
       "  %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name=\"dense\"](%self.1)\n",
       "  %4 : int = prim::Constant[value=1](), scope: __module.dense # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1674:0\n",
       "  %5 : Tensor = prim::GetAttr[name=\"bias\"](%2)\n",
       "  %6 : Tensor = prim::GetAttr[name=\"weight\"](%2)\n",
       "  %7 : Float(128:1, 512:128) = aten::t(%6), scope: __module.dense # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1674:0\n",
       "  %8 : Float(32:512, 512:1) = aten::addmm(%5, %input, %7, %4, %4), scope: __module.dense # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1674:0\n",
       "  return (%8)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[%2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name=\"dense\"](%self.1),\n",
       " %4 : int = prim::Constant[value=1](), scope: __module.dense # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1674:0,\n",
       " %5 : Tensor = prim::GetAttr[name=\"bias\"](%2),\n",
       " %6 : Tensor = prim::GetAttr[name=\"weight\"](%2),\n",
       " %7 : Float(128:1, 512:128) = aten::t(%6), scope: __module.dense # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1674:0,\n",
       " %8 : Float(32:512, 512:1) = aten::addmm(%5, %input, %7, %4, %4), scope: __module.dense # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1674:0]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(fc_graph.nodes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['copyMetadata',\n",
       " 'debugName',\n",
       " 'inferTypeFrom',\n",
       " 'isCompleteTensor',\n",
       " 'node',\n",
       " 'offset',\n",
       " 'replaceAllUsesWith',\n",
       " 'requiresGrad',\n",
       " 'requires_grad',\n",
       " 'setDebugName',\n",
       " 'setType',\n",
       " 'setTypeAs',\n",
       " 'toIValue',\n",
       " 'type',\n",
       " 'unique',\n",
       " 'uses']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc_in_nodes=list(fc_graph.inputs())\n",
    "[i for i in dir(fc_in_nodes[0]) if not i.startswith('__')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'input'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc_in_nodes[1].debugName()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8 defined in (%8 : Float(32:512, 512:1) = aten::addmm(%5, %input, %7, %4, %4), scope: __module.dense # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1674:0\n",
       " )]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(fc_graph.outputs())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_bert.py:191: TracerWarning: Converting a tensor to a Python index might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  position_ids = self.position_ids[:, :seq_length]\n",
      "/Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_utils.py:1673: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  input_tensor.shape == tensor_shape for input_tensor in input_tensors\n"
     ]
    }
   ],
   "source": [
    "trace = torch.jit.trace(model, inputs)\n",
    "graph = trace.inlined_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "graph(%self.1 : __torch__.transformers.modeling_bert.___torch_mangle_124.BertModel,\n",
       "      %input_ids : Long(1:100, 100:1)):\n",
       "  %2 : __torch__.transformers.modeling_bert.___torch_mangle_123.BertPooler = prim::GetAttr[name=\"pooler\"](%self.1)\n",
       "  %3 : __torch__.transformers.modeling_bert.___torch_mangle_120.BertEncoder = prim::GetAttr[name=\"encoder\"](%self.1)\n",
       "  %4 : __torch__.transformers.modeling_bert.___torch_mangle_84.BertEmbeddings = prim::GetAttr[name=\"embeddings\"](%self.1)\n",
       "  %5 : int = prim::Constant[value=0]() # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_bert.py:802:0\n",
       "  %6 : int = aten::size(%input_ids, %5) # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_bert.py:802:0\n",
       "  %7 : Long() = prim::NumToTensor(%6)\n",
       "  %8 : int = aten::Int(%7)\n",
       "  %9 : int = aten::Int(%7)\n",
       "  %10 : int = prim::Constant[value=1]() # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_bert.py:802:0\n",
       "  %11 : int = aten::size(%input_ids, %10) # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_bert.py:802:0\n",
       "  %12 : Long() = prim::NumToTensor(%11)\n",
       "  %13 : int = aten::Int(%12)\n",
       "  %14 : int = aten::Int(%12)\n",
       "  %15 : int[] = prim::ListConstruct(%9, %14)\n",
       "  %16 : int = prim::Constant[value=6]() # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_bert.py:811:0\n",
       "  %17 : int = prim::Constant[value=0]() # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_bert.py:811:0\n",
       "  %18 : Device = prim::Constant[value=\"cpu\"]() # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_bert.py:811:0\n",
       "  %19 : bool = prim::Constant[value=0]() # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_bert.py:811:0\n",
       "  %attention_mask.1 : Float(1:100, 100:1) = aten::ones(%15, %16, %17, %18, %19) # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_bert.py:811:0\n",
       "  %21 : int[] = prim::ListConstruct(%8, %13)\n",
       "  %22 : int = prim::Constant[value=4]() # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_bert.py:813:0\n",
       "  %23 : int = prim::Constant[value=0]() # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_bert.py:813:0\n",
       "  %24 : Device = prim::Constant[value=\"cpu\"]() # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_bert.py:813:0\n",
       "  %25 : bool = prim::Constant[value=0]() # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_bert.py:813:0\n",
       "  %input.2 : Long(1:100, 100:1) = aten::zeros(%21, %22, %23, %24, %25) # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_bert.py:813:0\n",
       "  %27 : int = prim::Constant[value=0]() # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_utils.py:258:0\n",
       "  %28 : int = prim::Constant[value=0]() # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_utils.py:258:0\n",
       "  %29 : int = prim::Constant[value=9223372036854775807]() # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_utils.py:258:0\n",
       "  %30 : int = prim::Constant[value=1]() # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_utils.py:258:0\n",
       "  %31 : Float(1:100, 100:1) = aten::slice(%attention_mask.1, %27, %28, %29, %30) # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_utils.py:258:0\n",
       "  %32 : int = prim::Constant[value=1]() # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_utils.py:258:0\n",
       "  %33 : Float(1:100, 1:100, 100:1) = aten::unsqueeze(%31, %32) # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_utils.py:258:0\n",
       "  %34 : int = prim::Constant[value=2]() # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_utils.py:258:0\n",
       "  %35 : Float(1:100, 1:100, 1:100, 100:1) = aten::unsqueeze(%33, %34) # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_utils.py:258:0\n",
       "  %36 : int = prim::Constant[value=3]() # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_utils.py:258:0\n",
       "  %37 : int = prim::Constant[value=0]() # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_utils.py:258:0\n",
       "  %38 : int = prim::Constant[value=9223372036854775807]() # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_utils.py:258:0\n",
       "  %39 : int = prim::Constant[value=1]() # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_utils.py:258:0\n",
       "  %extended_attention_mask : Float(1:100, 1:100, 1:100, 100:1) = aten::slice(%35, %36, %37, %38, %39) # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_utils.py:258:0\n",
       "  %41 : int = prim::Constant[value=6]() # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_utils.py:271:0\n",
       "  %42 : bool = prim::Constant[value=0]() # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_utils.py:271:0\n",
       "  %43 : bool = prim::Constant[value=0]() # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_utils.py:271:0\n",
       "  %44 : None = prim::Constant()\n",
       "  %45 : Float(1:100, 1:100, 1:100, 100:1) = aten::to(%extended_attention_mask, %41, %42, %43, %44) # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_utils.py:271:0\n",
       "  %46 : float = prim::Constant[value=1.]() # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/tensor.py:396:0\n",
       "  %47 : int = prim::Constant[value=1]() # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/tensor.py:396:0\n",
       "  %48 : Float(1:100, 1:100, 1:100, 100:1) = aten::rsub(%45, %46, %47) # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/tensor.py:396:0\n",
       "  %49 : Double() = prim::Constant[value={-10000}]() # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_utils.py:272:0\n",
       "  %attention_mask : Float(1:100, 1:100, 1:100, 100:1) = aten::mul(%48, %49) # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_utils.py:272:0\n",
       "  %55 : float = prim::Constant[value=0.10000000000000001](), scope: __module.embeddings/__module.embeddings.dropout # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:973:0\n",
       "  %56 : int = prim::Constant[value=128](), scope: __module.embeddings/__module.embeddings.LayerNorm # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:2048:0\n",
       "  %57 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.embeddings/__module.embeddings.LayerNorm # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:2048:0\n",
       "  %58 : bool = prim::Constant[value=1](), scope: __module.embeddings/__module.embeddings.LayerNorm # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:2048:0\n",
       "  %59 : int = prim::Constant[value=-1](), scope: __module.embeddings/__module.embeddings.position_embeddings # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1814:0\n",
       "  %60 : bool = prim::Constant[value=0](), scope: __module.embeddings/__module.embeddings.word_embeddings # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1814:0\n",
       "  %61 : int = prim::Constant[value=9223372036854775807](), scope: __module.embeddings # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_bert.py:191:0\n",
       "  %62 : int = prim::Constant[value=0](), scope: __module.embeddings # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_bert.py:191:0\n",
       "  %63 : int = prim::Constant[value=1](), scope: __module.embeddings # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_bert.py:184:0\n",
       "  %64 : __torch__.torch.nn.modules.normalization.___torch_mangle_82.LayerNorm = prim::GetAttr[name=\"LayerNorm\"](%4)\n",
       "  %65 : __torch__.torch.nn.modules.sparse.___torch_mangle_81.Embedding = prim::GetAttr[name=\"token_type_embeddings\"](%4)\n",
       "  %66 : __torch__.torch.nn.modules.sparse.___torch_mangle_80.Embedding = prim::GetAttr[name=\"position_embeddings\"](%4)\n",
       "  %67 : __torch__.torch.nn.modules.sparse.___torch_mangle_79.Embedding = prim::GetAttr[name=\"word_embeddings\"](%4)\n",
       "  %68 : Tensor = prim::GetAttr[name=\"position_ids\"](%4)\n",
       "  %69 : int = aten::size(%input_ids, %63), scope: __module.embeddings # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_bert.py:184:0\n",
       "  %70 : Long(1:512, 512:1) = aten::slice(%68, %62, %62, %61, %63), scope: __module.embeddings # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_bert.py:191:0\n",
       "  %input.1 : Long(1:512, 100:1) = aten::slice(%70, %63, %62, %69, %63), scope: __module.embeddings # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_bert.py:191:0\n",
       "  %72 : Tensor = prim::GetAttr[name=\"weight\"](%67)\n",
       "  %inputs_embeds : Float(1:12800, 100:128, 128:1) = aten::embedding(%72, %input_ids, %62, %60, %60), scope: __module.embeddings/__module.embeddings.word_embeddings # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1814:0\n",
       "  %74 : Tensor = prim::GetAttr[name=\"weight\"](%66)\n",
       "  %position_embeddings : Float(1:12800, 100:128, 128:1) = aten::embedding(%74, %input.1, %59, %60, %60), scope: __module.embeddings/__module.embeddings.position_embeddings # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1814:0\n",
       "  %76 : Tensor = prim::GetAttr[name=\"weight\"](%65)\n",
       "  %token_type_embeddings : Float(1:12800, 100:128, 128:1) = aten::embedding(%76, %input.2, %59, %60, %60), scope: __module.embeddings/__module.embeddings.token_type_embeddings # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1814:0\n",
       "  %78 : Float(1:12800, 100:128, 128:1) = aten::add(%inputs_embeds, %position_embeddings, %63), scope: __module.embeddings # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_bert.py:201:0\n",
       "  %input.3 : Float(1:12800, 100:128, 128:1) = aten::add(%78, %token_type_embeddings, %63), scope: __module.embeddings # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_bert.py:201:0\n",
       "  %80 : Tensor = prim::GetAttr[name=\"bias\"](%64)\n",
       "  %81 : Tensor = prim::GetAttr[name=\"weight\"](%64)\n",
       "  %82 : int[] = prim::ListConstruct(%56), scope: __module.embeddings/__module.embeddings.LayerNorm\n",
       "  %input.4 : Float(1:12800, 100:128, 128:1) = aten::layer_norm(%input.3, %82, %81, %80, %57, %58), scope: __module.embeddings/__module.embeddings.LayerNorm # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:2048:0\n",
       "  %input.5 : Float(1:12800, 100:128, 128:1) = aten::dropout(%input.4, %55, %60), scope: __module.embeddings/__module.embeddings.dropout # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:973:0\n",
       "  %85 : bool = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.output/__module.encoder.layer.0.attention.output.LayerNorm # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:2048:0\n",
       "  %86 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.output/__module.encoder.layer.0.attention.output.LayerNorm # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:2048:0\n",
       "  %87 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self/__module.encoder.layer.0.attention.self.query # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1678:0\n",
       "  %88 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_bert.py:227:0\n",
       "  %89 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_bert.py:228:0\n",
       "  %90 : int = prim::Constant[value=64](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_bert.py:228:0\n",
       "  %91 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_bert.py:229:0\n",
       "  %92 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_bert.py:258:0\n",
       "  %93 : int = prim::Constant[value=-2](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_bert.py:258:0\n",
       "  %94 : Double() = prim::Constant[value={8}](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_bert.py:259:0\n",
       "  %95 : None = prim::Constant(), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self\n",
       "  %96 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self/__module.encoder.layer.0.attention.self.dropout # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:973:0\n",
       "  %97 : float = prim::Constant[value=0.10000000000000001](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self/__module.encoder.layer.0.attention.self.dropout # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:973:0\n",
       "  %98 : int = prim::Constant[value=128](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_bert.py:279:0\n",
       "  %99 : Double() = prim::Constant[value={1}](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.intermediate # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/activations.py:38:0\n",
       "  %100 : Double() = prim::Constant[value={0.044715}](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.intermediate # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/activations.py:38:0\n",
       "  %101 : Double() = prim::Constant[value={0.797885}](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.intermediate # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/activations.py:38:0\n",
       "  %102 : Double() = prim::Constant[value={0.5}](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.intermediate # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/activations.py:38:0\n",
       "  %103 : __torch__.torch.nn.modules.container.___torch_mangle_119.ModuleList = prim::GetAttr[name=\"layer\"](%3)\n",
       "  %104 : __torch__.transformers.modeling_bert.___torch_mangle_118.BertLayer = prim::GetAttr[name=\"1\"](%103)\n",
       "  %105 : __torch__.torch.nn.modules.container.___torch_mangle_119.ModuleList = prim::GetAttr[name=\"layer\"](%3)\n",
       "  %106 : __torch__.transformers.modeling_bert.___torch_mangle_101.BertLayer = prim::GetAttr[name=\"0\"](%105)\n",
       "  %107 : __torch__.transformers.modeling_bert.___torch_mangle_100.BertOutput = prim::GetAttr[name=\"output\"](%106)\n",
       "  %108 : __torch__.transformers.modeling_bert.___torch_mangle_96.BertIntermediate = prim::GetAttr[name=\"intermediate\"](%106)\n",
       "  %109 : __torch__.transformers.modeling_bert.___torch_mangle_94.BertAttention = prim::GetAttr[name=\"attention\"](%106)\n",
       "  %110 : __torch__.transformers.modeling_bert.___torch_mangle_93.BertSelfOutput = prim::GetAttr[name=\"output\"](%109)\n",
       "  %111 : __torch__.transformers.modeling_bert.___torch_mangle_89.BertSelfAttention = prim::GetAttr[name=\"self\"](%109)\n",
       "  %112 : __torch__.torch.nn.modules.linear.___torch_mangle_87.Linear = prim::GetAttr[name=\"value\"](%111)\n",
       "  %113 : __torch__.torch.nn.modules.linear.___torch_mangle_86.Linear = prim::GetAttr[name=\"key\"](%111)\n",
       "  %114 : __torch__.torch.nn.modules.linear.___torch_mangle_85.Linear = prim::GetAttr[name=\"query\"](%111)\n",
       "  %115 : Tensor = prim::GetAttr[name=\"bias\"](%114)\n",
       "  %116 : Tensor = prim::GetAttr[name=\"weight\"](%114)\n",
       "  %117 : Float(128:1, 128:128) = aten::t(%116), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self/__module.encoder.layer.0.attention.self.query # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1676:0\n",
       "  %output.1 : Float(1:12800, 100:128, 128:1) = aten::matmul(%input.5, %117), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self/__module.encoder.layer.0.attention.self.query # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1676:0\n",
       "  %x.1 : Float(1:12800, 100:128, 128:1) = aten::add_(%output.1, %115, %87), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self/__module.encoder.layer.0.attention.self.query # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1678:0\n",
       "  %120 : Tensor = prim::GetAttr[name=\"bias\"](%113)\n",
       "  %121 : Tensor = prim::GetAttr[name=\"weight\"](%113)\n",
       "  %122 : Float(128:1, 128:128) = aten::t(%121), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self/__module.encoder.layer.0.attention.self.key # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1676:0\n",
       "  %output.2 : Float(1:12800, 100:128, 128:1) = aten::matmul(%input.5, %122), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self/__module.encoder.layer.0.attention.self.key # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1676:0\n",
       "  %x.3 : Float(1:12800, 100:128, 128:1) = aten::add_(%output.2, %120, %87), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self/__module.encoder.layer.0.attention.self.key # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1678:0\n",
       "  %125 : Tensor = prim::GetAttr[name=\"bias\"](%112)\n",
       "  %126 : Tensor = prim::GetAttr[name=\"weight\"](%112)\n",
       "  %127 : Float(128:1, 128:128) = aten::t(%126), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self/__module.encoder.layer.0.attention.self.value # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1676:0\n",
       "  %output.3 : Float(1:12800, 100:128, 128:1) = aten::matmul(%input.5, %127), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self/__module.encoder.layer.0.attention.self.value # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1676:0\n",
       "  %x.5 : Float(1:12800, 100:128, 128:1) = aten::add_(%output.3, %125, %87), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self/__module.encoder.layer.0.attention.self.value # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1678:0\n",
       "  %130 : int = aten::size(%x.1, %88), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_bert.py:227:0\n",
       "  %131 : int = aten::size(%x.1, %87), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_bert.py:227:0\n",
       "  %132 : int[] = prim::ListConstruct(%130, %131, %89, %90), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self\n",
       "  %x.2 : Float(1:12800, 100:128, 2:64, 64:1) = aten::view(%x.1, %132), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_bert.py:228:0\n",
       "  %134 : int[] = prim::ListConstruct(%88, %89, %87, %91), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self\n",
       "  %query_layer.1 : Float(1:12800, 2:64, 100:128, 64:1) = aten::permute(%x.2, %134), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_bert.py:229:0\n",
       "  %136 : int = aten::size(%x.3, %88), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_bert.py:227:0\n",
       "  %137 : int = aten::size(%x.3, %87), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_bert.py:227:0\n",
       "  %138 : int[] = prim::ListConstruct(%136, %137, %89, %90), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self\n",
       "  %x.4 : Float(1:12800, 100:128, 2:64, 64:1) = aten::view(%x.3, %138), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_bert.py:228:0\n",
       "  %140 : int[] = prim::ListConstruct(%88, %89, %87, %91), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self\n",
       "  %key_layer.1 : Float(1:12800, 2:64, 100:128, 64:1) = aten::permute(%x.4, %140), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_bert.py:229:0\n",
       "  %142 : int = aten::size(%x.5, %88), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_bert.py:227:0\n",
       "  %143 : int = aten::size(%x.5, %87), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_bert.py:227:0\n",
       "  %144 : int[] = prim::ListConstruct(%142, %143, %89, %90), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self\n",
       "  %x.6 : Float(1:12800, 100:128, 2:64, 64:1) = aten::view(%x.5, %144), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_bert.py:228:0\n",
       "  %146 : int[] = prim::ListConstruct(%88, %89, %87, %91), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self\n",
       "  %value_layer.1 : Float(1:12800, 2:64, 100:128, 64:1) = aten::permute(%x.6, %146), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_bert.py:229:0\n",
       "  %148 : Float(1:12800, 2:64, 64:1, 100:128) = aten::transpose(%key_layer.1, %92, %93), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_bert.py:258:0\n",
       "  %attention_scores.1 : Float(1:20000, 2:10000, 100:100, 100:1) = aten::matmul(%query_layer.1, %148), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_bert.py:258:0\n",
       "  %attention_scores.2 : Float(1:20000, 2:10000, 100:100, 100:1) = aten::div(%attention_scores.1, %94), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_bert.py:259:0\n",
       "  %input.6 : Float(1:20000, 2:10000, 100:100, 100:1) = aten::add(%attention_scores.2, %attention_mask, %87), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_bert.py:262:0\n",
       "  %input.7 : Float(1:20000, 2:10000, 100:100, 100:1) = aten::softmax(%input.6, %92, %95), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1498:0\n",
       "  %attention_probs.1 : Float(1:20000, 2:10000, 100:100, 100:1) = aten::dropout(%input.7, %97, %96), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self/__module.encoder.layer.0.attention.self.dropout # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:973:0\n",
       "  %context_layer.1 : Float(1:12800, 2:6400, 100:64, 64:1) = aten::matmul(%attention_probs.1, %value_layer.1), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_bert.py:275:0\n",
       "  %155 : int[] = prim::ListConstruct(%88, %89, %87, %91), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self\n",
       "  %156 : Float(1:12800, 100:64, 2:6400, 64:1) = aten::permute(%context_layer.1, %155), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_bert.py:277:0\n",
       "  %context_layer.2 : Float(1:12800, 100:128, 2:64, 64:1) = aten::contiguous(%156, %88), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_bert.py:277:0\n",
       "  %158 : int = aten::size(%context_layer.2, %88), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_bert.py:278:0\n",
       "  %159 : int = aten::size(%context_layer.2, %87), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_bert.py:278:0\n",
       "  %160 : int[] = prim::ListConstruct(%158, %159, %98), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self\n",
       "  %input.8 : Float(1:12800, 100:128, 128:1) = aten::view(%context_layer.2, %160), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_bert.py:279:0\n",
       "  %162 : __torch__.torch.nn.modules.normalization.___torch_mangle_91.LayerNorm = prim::GetAttr[name=\"LayerNorm\"](%110)\n",
       "  %163 : __torch__.torch.nn.modules.linear.___torch_mangle_90.Linear = prim::GetAttr[name=\"dense\"](%110)\n",
       "  %164 : Tensor = prim::GetAttr[name=\"bias\"](%163)\n",
       "  %165 : Tensor = prim::GetAttr[name=\"weight\"](%163)\n",
       "  %166 : Float(128:1, 128:128) = aten::t(%165), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.output/__module.encoder.layer.0.attention.output.dense # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1676:0\n",
       "  %output.4 : Float(1:12800, 100:128, 128:1) = aten::matmul(%input.8, %166), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.output/__module.encoder.layer.0.attention.output.dense # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1676:0\n",
       "  %input.9 : Float(1:12800, 100:128, 128:1) = aten::add_(%output.4, %164, %87), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.output/__module.encoder.layer.0.attention.output.dense # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1678:0\n",
       "  %hidden_states.1 : Float(1:12800, 100:128, 128:1) = aten::dropout(%input.9, %97, %96), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.output/__module.encoder.layer.0.attention.output.dropout # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:973:0\n",
       "  %input.10 : Float(1:12800, 100:128, 128:1) = aten::add(%hidden_states.1, %input.5, %87), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.output # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_bert.py:295:0\n",
       "  %171 : Tensor = prim::GetAttr[name=\"bias\"](%162)\n",
       "  %172 : Tensor = prim::GetAttr[name=\"weight\"](%162)\n",
       "  %173 : int[] = prim::ListConstruct(%98), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.output/__module.encoder.layer.0.attention.output.LayerNorm\n",
       "  %input_tensor.1 : Float(1:12800, 100:128, 128:1) = aten::layer_norm(%input.10, %173, %172, %171, %86, %85), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.output/__module.encoder.layer.0.attention.output.LayerNorm # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:2048:0\n",
       "  %175 : __torch__.torch.nn.modules.linear.___torch_mangle_95.Linear = prim::GetAttr[name=\"dense\"](%108)\n",
       "  %176 : Tensor = prim::GetAttr[name=\"bias\"](%175)\n",
       "  %177 : Tensor = prim::GetAttr[name=\"weight\"](%175)\n",
       "  %178 : Float(128:1, 512:128) = aten::t(%177), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.intermediate/__module.encoder.layer.0.intermediate.dense # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1676:0\n",
       "  %output.5 : Float(1:51200, 100:512, 512:1) = aten::matmul(%input_tensor.1, %178), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.intermediate/__module.encoder.layer.0.intermediate.dense # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1676:0\n",
       "  %x.7 : Float(1:51200, 100:512, 512:1) = aten::add_(%output.5, %176, %87), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.intermediate/__module.encoder.layer.0.intermediate.dense # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1678:0\n",
       "  %181 : Float(1:51200, 100:512, 512:1) = aten::mul(%x.7, %102), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.intermediate # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/activations.py:38:0\n",
       "  %182 : Float(1:51200, 100:512, 512:1) = aten::mul(%x.7, %101), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.intermediate # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/activations.py:38:0\n",
       "  %183 : Float(1:51200, 100:512, 512:1) = aten::mul(%x.7, %100), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.intermediate # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/activations.py:38:0\n",
       "  %184 : Float(1:51200, 100:512, 512:1) = aten::mul(%183, %x.7), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.intermediate # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/activations.py:38:0\n",
       "  %185 : Float(1:51200, 100:512, 512:1) = aten::add(%184, %99, %87), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.intermediate # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/activations.py:38:0\n",
       "  %186 : Float(1:51200, 100:512, 512:1) = aten::mul(%182, %185), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.intermediate # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/activations.py:38:0\n",
       "  %187 : Float(1:51200, 100:512, 512:1) = aten::tanh(%186), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.intermediate # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/activations.py:38:0\n",
       "  %188 : Float(1:51200, 100:512, 512:1) = aten::add(%187, %99, %87), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.intermediate # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/activations.py:38:0\n",
       "  %input.11 : Float(1:51200, 100:512, 512:1) = aten::mul(%181, %188), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.intermediate # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/activations.py:38:0\n",
       "  %190 : __torch__.torch.nn.modules.normalization.___torch_mangle_98.LayerNorm = prim::GetAttr[name=\"LayerNorm\"](%107)\n",
       "  %191 : __torch__.torch.nn.modules.linear.___torch_mangle_97.Linear = prim::GetAttr[name=\"dense\"](%107)\n",
       "  %192 : Tensor = prim::GetAttr[name=\"bias\"](%191)\n",
       "  %193 : Tensor = prim::GetAttr[name=\"weight\"](%191)\n",
       "  %194 : Float(512:1, 128:512) = aten::t(%193), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.output/__module.encoder.layer.0.output.dense # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1676:0\n",
       "  %output.6 : Float(1:12800, 100:128, 128:1) = aten::matmul(%input.11, %194), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.output/__module.encoder.layer.0.output.dense # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1676:0\n",
       "  %input.12 : Float(1:12800, 100:128, 128:1) = aten::add_(%output.6, %192, %87), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.output/__module.encoder.layer.0.output.dense # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1678:0\n",
       "  %hidden_states.2 : Float(1:12800, 100:128, 128:1) = aten::dropout(%input.12, %97, %96), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.output/__module.encoder.layer.0.output.dropout # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:973:0\n",
       "  %input.13 : Float(1:12800, 100:128, 128:1) = aten::add(%hidden_states.2, %input_tensor.1, %87), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.output # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_bert.py:371:0\n",
       "  %199 : Tensor = prim::GetAttr[name=\"bias\"](%190)\n",
       "  %200 : Tensor = prim::GetAttr[name=\"weight\"](%190)\n",
       "  %201 : int[] = prim::ListConstruct(%98), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.output/__module.encoder.layer.0.output.LayerNorm\n",
       "  %input.14 : Float(1:12800, 100:128, 128:1) = aten::layer_norm(%input.13, %201, %200, %199, %86, %85), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.output/__module.encoder.layer.0.output.LayerNorm # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:2048:0\n",
       "  %203 : __torch__.transformers.modeling_bert.___torch_mangle_117.BertOutput = prim::GetAttr[name=\"output\"](%104)\n",
       "  %204 : __torch__.transformers.modeling_bert.___torch_mangle_113.BertIntermediate = prim::GetAttr[name=\"intermediate\"](%104)\n",
       "  %205 : __torch__.transformers.modeling_bert.___torch_mangle_111.BertAttention = prim::GetAttr[name=\"attention\"](%104)\n",
       "  %206 : __torch__.transformers.modeling_bert.___torch_mangle_110.BertSelfOutput = prim::GetAttr[name=\"output\"](%205)\n",
       "  %207 : __torch__.transformers.modeling_bert.___torch_mangle_106.BertSelfAttention = prim::GetAttr[name=\"self\"](%205)\n",
       "  %208 : __torch__.torch.nn.modules.linear.___torch_mangle_104.Linear = prim::GetAttr[name=\"value\"](%207)\n",
       "  %209 : __torch__.torch.nn.modules.linear.___torch_mangle_103.Linear = prim::GetAttr[name=\"key\"](%207)\n",
       "  %210 : __torch__.torch.nn.modules.linear.___torch_mangle_102.Linear = prim::GetAttr[name=\"query\"](%207)\n",
       "  %211 : Tensor = prim::GetAttr[name=\"bias\"](%210)\n",
       "  %212 : Tensor = prim::GetAttr[name=\"weight\"](%210)\n",
       "  %213 : Float(128:1, 128:128) = aten::t(%212), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self/__module.encoder.layer.1.attention.self.query # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1676:0\n",
       "  %output.7 : Float(1:12800, 100:128, 128:1) = aten::matmul(%input.14, %213), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self/__module.encoder.layer.1.attention.self.query # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1676:0\n",
       "  %x.8 : Float(1:12800, 100:128, 128:1) = aten::add_(%output.7, %211, %87), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self/__module.encoder.layer.1.attention.self.query # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1678:0\n",
       "  %216 : Tensor = prim::GetAttr[name=\"bias\"](%209)\n",
       "  %217 : Tensor = prim::GetAttr[name=\"weight\"](%209)\n",
       "  %218 : Float(128:1, 128:128) = aten::t(%217), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self/__module.encoder.layer.1.attention.self.key # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1676:0\n",
       "  %output.8 : Float(1:12800, 100:128, 128:1) = aten::matmul(%input.14, %218), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self/__module.encoder.layer.1.attention.self.key # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1676:0\n",
       "  %x.10 : Float(1:12800, 100:128, 128:1) = aten::add_(%output.8, %216, %87), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self/__module.encoder.layer.1.attention.self.key # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1678:0\n",
       "  %221 : Tensor = prim::GetAttr[name=\"bias\"](%208)\n",
       "  %222 : Tensor = prim::GetAttr[name=\"weight\"](%208)\n",
       "  %223 : Float(128:1, 128:128) = aten::t(%222), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self/__module.encoder.layer.1.attention.self.value # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1676:0\n",
       "  %output.9 : Float(1:12800, 100:128, 128:1) = aten::matmul(%input.14, %223), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self/__module.encoder.layer.1.attention.self.value # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1676:0\n",
       "  %x.12 : Float(1:12800, 100:128, 128:1) = aten::add_(%output.9, %221, %87), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self/__module.encoder.layer.1.attention.self.value # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1678:0\n",
       "  %226 : int = aten::size(%x.8, %88), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_bert.py:227:0\n",
       "  %227 : int = aten::size(%x.8, %87), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_bert.py:227:0\n",
       "  %228 : int[] = prim::ListConstruct(%226, %227, %89, %90), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self\n",
       "  %x.9 : Float(1:12800, 100:128, 2:64, 64:1) = aten::view(%x.8, %228), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_bert.py:228:0\n",
       "  %230 : int[] = prim::ListConstruct(%88, %89, %87, %91), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self\n",
       "  %query_layer : Float(1:12800, 2:64, 100:128, 64:1) = aten::permute(%x.9, %230), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_bert.py:229:0\n",
       "  %232 : int = aten::size(%x.10, %88), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_bert.py:227:0\n",
       "  %233 : int = aten::size(%x.10, %87), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_bert.py:227:0\n",
       "  %234 : int[] = prim::ListConstruct(%232, %233, %89, %90), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self\n",
       "  %x.11 : Float(1:12800, 100:128, 2:64, 64:1) = aten::view(%x.10, %234), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_bert.py:228:0\n",
       "  %236 : int[] = prim::ListConstruct(%88, %89, %87, %91), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self\n",
       "  %key_layer : Float(1:12800, 2:64, 100:128, 64:1) = aten::permute(%x.11, %236), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_bert.py:229:0\n",
       "  %238 : int = aten::size(%x.12, %88), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_bert.py:227:0\n",
       "  %239 : int = aten::size(%x.12, %87), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_bert.py:227:0\n",
       "  %240 : int[] = prim::ListConstruct(%238, %239, %89, %90), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self\n",
       "  %x.13 : Float(1:12800, 100:128, 2:64, 64:1) = aten::view(%x.12, %240), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_bert.py:228:0\n",
       "  %242 : int[] = prim::ListConstruct(%88, %89, %87, %91), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self\n",
       "  %value_layer : Float(1:12800, 2:64, 100:128, 64:1) = aten::permute(%x.13, %242), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_bert.py:229:0\n",
       "  %244 : Float(1:12800, 2:64, 64:1, 100:128) = aten::transpose(%key_layer, %92, %93), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_bert.py:258:0\n",
       "  %attention_scores.3 : Float(1:20000, 2:10000, 100:100, 100:1) = aten::matmul(%query_layer, %244), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_bert.py:258:0\n",
       "  %attention_scores : Float(1:20000, 2:10000, 100:100, 100:1) = aten::div(%attention_scores.3, %94), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_bert.py:259:0\n",
       "  %input.15 : Float(1:20000, 2:10000, 100:100, 100:1) = aten::add(%attention_scores, %attention_mask, %87), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_bert.py:262:0\n",
       "  %input.16 : Float(1:20000, 2:10000, 100:100, 100:1) = aten::softmax(%input.15, %92, %95), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1498:0\n",
       "  %attention_probs : Float(1:20000, 2:10000, 100:100, 100:1) = aten::dropout(%input.16, %97, %96), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self/__module.encoder.layer.1.attention.self.dropout # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:973:0\n",
       "  %context_layer.3 : Float(1:12800, 2:6400, 100:64, 64:1) = aten::matmul(%attention_probs, %value_layer), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_bert.py:275:0\n",
       "  %251 : int[] = prim::ListConstruct(%88, %89, %87, %91), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self\n",
       "  %252 : Float(1:12800, 100:64, 2:6400, 64:1) = aten::permute(%context_layer.3, %251), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_bert.py:277:0\n",
       "  %context_layer : Float(1:12800, 100:128, 2:64, 64:1) = aten::contiguous(%252, %88), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_bert.py:277:0\n",
       "  %254 : int = aten::size(%context_layer, %88), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_bert.py:278:0\n",
       "  %255 : int = aten::size(%context_layer, %87), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_bert.py:278:0\n",
       "  %256 : int[] = prim::ListConstruct(%254, %255, %98), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self\n",
       "  %input.17 : Float(1:12800, 100:128, 128:1) = aten::view(%context_layer, %256), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_bert.py:279:0\n",
       "  %258 : __torch__.torch.nn.modules.normalization.___torch_mangle_108.LayerNorm = prim::GetAttr[name=\"LayerNorm\"](%206)\n",
       "  %259 : __torch__.torch.nn.modules.linear.___torch_mangle_107.Linear = prim::GetAttr[name=\"dense\"](%206)\n",
       "  %260 : Tensor = prim::GetAttr[name=\"bias\"](%259)\n",
       "  %261 : Tensor = prim::GetAttr[name=\"weight\"](%259)\n",
       "  %262 : Float(128:1, 128:128) = aten::t(%261), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.output/__module.encoder.layer.1.attention.output.dense # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1676:0\n",
       "  %output.10 : Float(1:12800, 100:128, 128:1) = aten::matmul(%input.17, %262), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.output/__module.encoder.layer.1.attention.output.dense # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1676:0\n",
       "  %input.18 : Float(1:12800, 100:128, 128:1) = aten::add_(%output.10, %260, %87), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.output/__module.encoder.layer.1.attention.output.dense # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1678:0\n",
       "  %hidden_states.3 : Float(1:12800, 100:128, 128:1) = aten::dropout(%input.18, %97, %96), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.output/__module.encoder.layer.1.attention.output.dropout # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:973:0\n",
       "  %input.19 : Float(1:12800, 100:128, 128:1) = aten::add(%hidden_states.3, %input.14, %87), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.output # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_bert.py:295:0\n",
       "  %267 : Tensor = prim::GetAttr[name=\"bias\"](%258)\n",
       "  %268 : Tensor = prim::GetAttr[name=\"weight\"](%258)\n",
       "  %269 : int[] = prim::ListConstruct(%98), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.output/__module.encoder.layer.1.attention.output.LayerNorm\n",
       "  %input_tensor : Float(1:12800, 100:128, 128:1) = aten::layer_norm(%input.19, %269, %268, %267, %86, %85), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.output/__module.encoder.layer.1.attention.output.LayerNorm # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:2048:0\n",
       "  %271 : __torch__.torch.nn.modules.linear.___torch_mangle_112.Linear = prim::GetAttr[name=\"dense\"](%204)\n",
       "  %272 : Tensor = prim::GetAttr[name=\"bias\"](%271)\n",
       "  %273 : Tensor = prim::GetAttr[name=\"weight\"](%271)\n",
       "  %274 : Float(128:1, 512:128) = aten::t(%273), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.intermediate/__module.encoder.layer.1.intermediate.dense # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1676:0\n",
       "  %output.11 : Float(1:51200, 100:512, 512:1) = aten::matmul(%input_tensor, %274), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.intermediate/__module.encoder.layer.1.intermediate.dense # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1676:0\n",
       "  %x : Float(1:51200, 100:512, 512:1) = aten::add_(%output.11, %272, %87), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.intermediate/__module.encoder.layer.1.intermediate.dense # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1678:0\n",
       "  %277 : Float(1:51200, 100:512, 512:1) = aten::mul(%x, %102), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.intermediate # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/activations.py:38:0\n",
       "  %278 : Float(1:51200, 100:512, 512:1) = aten::mul(%x, %101), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.intermediate # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/activations.py:38:0\n",
       "  %279 : Float(1:51200, 100:512, 512:1) = aten::mul(%x, %100), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.intermediate # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/activations.py:38:0\n",
       "  %280 : Float(1:51200, 100:512, 512:1) = aten::mul(%279, %x), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.intermediate # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/activations.py:38:0\n",
       "  %281 : Float(1:51200, 100:512, 512:1) = aten::add(%280, %99, %87), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.intermediate # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/activations.py:38:0\n",
       "  %282 : Float(1:51200, 100:512, 512:1) = aten::mul(%278, %281), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.intermediate # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/activations.py:38:0\n",
       "  %283 : Float(1:51200, 100:512, 512:1) = aten::tanh(%282), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.intermediate # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/activations.py:38:0\n",
       "  %284 : Float(1:51200, 100:512, 512:1) = aten::add(%283, %99, %87), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.intermediate # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/activations.py:38:0\n",
       "  %input.20 : Float(1:51200, 100:512, 512:1) = aten::mul(%277, %284), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.intermediate # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/activations.py:38:0\n",
       "  %286 : __torch__.torch.nn.modules.normalization.___torch_mangle_115.LayerNorm = prim::GetAttr[name=\"LayerNorm\"](%203)\n",
       "  %287 : __torch__.torch.nn.modules.linear.___torch_mangle_114.Linear = prim::GetAttr[name=\"dense\"](%203)\n",
       "  %288 : Tensor = prim::GetAttr[name=\"bias\"](%287)\n",
       "  %289 : Tensor = prim::GetAttr[name=\"weight\"](%287)\n",
       "  %290 : Float(512:1, 128:512) = aten::t(%289), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.output/__module.encoder.layer.1.output.dense # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1676:0\n",
       "  %output : Float(1:12800, 100:128, 128:1) = aten::matmul(%input.20, %290), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.output/__module.encoder.layer.1.output.dense # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1676:0\n",
       "  %input.21 : Float(1:12800, 100:128, 128:1) = aten::add_(%output, %288, %87), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.output/__module.encoder.layer.1.output.dense # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1678:0\n",
       "  %hidden_states.4 : Float(1:12800, 100:128, 128:1) = aten::dropout(%input.21, %97, %96), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.output/__module.encoder.layer.1.output.dropout # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:973:0\n",
       "  %input.22 : Float(1:12800, 100:128, 128:1) = aten::add(%hidden_states.4, %input_tensor, %87), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.output # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_bert.py:371:0\n",
       "  %295 : Tensor = prim::GetAttr[name=\"bias\"](%286)\n",
       "  %296 : Tensor = prim::GetAttr[name=\"weight\"](%286)\n",
       "  %297 : int[] = prim::ListConstruct(%98), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.output/__module.encoder.layer.1.output.LayerNorm\n",
       "  %hidden_states : Float(1:12800, 100:128, 128:1) = aten::layer_norm(%input.22, %297, %296, %295, %86, %85), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.output/__module.encoder.layer.1.output.LayerNorm # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:2048:0\n",
       "  %299 : int = prim::Constant[value=1](), scope: __module.pooler # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_bert.py:517:0\n",
       "  %300 : int = prim::Constant[value=9223372036854775807](), scope: __module.pooler # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_bert.py:517:0\n",
       "  %301 : int = prim::Constant[value=0](), scope: __module.pooler # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_bert.py:517:0\n",
       "  %302 : __torch__.torch.nn.modules.linear.___torch_mangle_121.Linear = prim::GetAttr[name=\"dense\"](%2)\n",
       "  %303 : Float(1:12800, 100:128, 128:1) = aten::slice(%hidden_states, %301, %301, %300, %299), scope: __module.pooler # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_bert.py:517:0\n",
       "  %input.23 : Float(1:12800, 128:1) = aten::select(%303, %299, %301), scope: __module.pooler # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_bert.py:517:0\n",
       "  %305 : Tensor = prim::GetAttr[name=\"bias\"](%302)\n",
       "  %306 : Tensor = prim::GetAttr[name=\"weight\"](%302)\n",
       "  %307 : Float(128:1, 128:128) = aten::t(%306), scope: __module.pooler/__module.pooler.dense # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1674:0\n",
       "  %input : Float(1:128, 128:1) = aten::addmm(%305, %input.23, %307, %299, %299), scope: __module.pooler/__module.pooler.dense # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1674:0\n",
       "  %309 : Float(1:128, 128:1) = aten::tanh(%input), scope: __module.pooler/__module.pooler.activation # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/modules/activation.py:350:0\n",
       "  %54 : (Float(1:12800, 100:128, 128:1), Float(1:128, 128:1)) = prim::TupleConstruct(%hidden_states, %309)\n",
       "  return (%54)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[%2 : __torch__.transformers.modeling_bert.BertPooler = prim::GetAttr[name=\"pooler\"](%self.1),\n",
       " %3 : __torch__.transformers.modeling_bert.BertEncoder = prim::GetAttr[name=\"encoder\"](%self.1),\n",
       " %4 : __torch__.transformers.modeling_bert.BertEmbeddings = prim::GetAttr[name=\"embeddings\"](%self.1),\n",
       " %5 : int = prim::Constant[value=0]() # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_bert.py:802:0,\n",
       " %6 : int = aten::size(%input_ids, %5) # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_bert.py:802:0,\n",
       " %7 : Long() = prim::NumToTensor(%6),\n",
       " %8 : int = aten::Int(%7),\n",
       " %9 : int = aten::Int(%7),\n",
       " %10 : int = prim::Constant[value=1]() # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_bert.py:802:0,\n",
       " %11 : int = aten::size(%input_ids, %10) # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_bert.py:802:0,\n",
       " %12 : Long() = prim::NumToTensor(%11),\n",
       " %13 : int = aten::Int(%12),\n",
       " %14 : int = aten::Int(%12),\n",
       " %15 : int[] = prim::ListConstruct(%9, %14),\n",
       " %16 : int = prim::Constant[value=6]() # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_bert.py:811:0,\n",
       " %17 : int = prim::Constant[value=0]() # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_bert.py:811:0,\n",
       " %18 : Device = prim::Constant[value=\"cpu\"]() # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_bert.py:811:0,\n",
       " %19 : bool = prim::Constant[value=0]() # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_bert.py:811:0,\n",
       " %attention_mask.1 : Float(1:100, 100:1) = aten::ones(%15, %16, %17, %18, %19) # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_bert.py:811:0,\n",
       " %21 : int[] = prim::ListConstruct(%8, %13),\n",
       " %22 : int = prim::Constant[value=4]() # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_bert.py:813:0,\n",
       " %23 : int = prim::Constant[value=0]() # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_bert.py:813:0,\n",
       " %24 : Device = prim::Constant[value=\"cpu\"]() # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_bert.py:813:0,\n",
       " %25 : bool = prim::Constant[value=0]() # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_bert.py:813:0,\n",
       " %input.2 : Long(1:100, 100:1) = aten::zeros(%21, %22, %23, %24, %25) # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_bert.py:813:0,\n",
       " %27 : int = prim::Constant[value=0]() # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_utils.py:258:0,\n",
       " %28 : int = prim::Constant[value=0]() # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_utils.py:258:0,\n",
       " %29 : int = prim::Constant[value=9223372036854775807]() # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_utils.py:258:0,\n",
       " %30 : int = prim::Constant[value=1]() # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_utils.py:258:0,\n",
       " %31 : Float(1:100, 100:1) = aten::slice(%attention_mask.1, %27, %28, %29, %30) # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_utils.py:258:0,\n",
       " %32 : int = prim::Constant[value=1]() # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_utils.py:258:0,\n",
       " %33 : Float(1:100, 1:100, 100:1) = aten::unsqueeze(%31, %32) # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_utils.py:258:0,\n",
       " %34 : int = prim::Constant[value=2]() # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_utils.py:258:0,\n",
       " %35 : Float(1:100, 1:100, 1:100, 100:1) = aten::unsqueeze(%33, %34) # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_utils.py:258:0,\n",
       " %36 : int = prim::Constant[value=3]() # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_utils.py:258:0,\n",
       " %37 : int = prim::Constant[value=0]() # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_utils.py:258:0,\n",
       " %38 : int = prim::Constant[value=9223372036854775807]() # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_utils.py:258:0,\n",
       " %39 : int = prim::Constant[value=1]() # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_utils.py:258:0,\n",
       " %extended_attention_mask : Float(1:100, 1:100, 1:100, 100:1) = aten::slice(%35, %36, %37, %38, %39) # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_utils.py:258:0,\n",
       " %41 : int = prim::Constant[value=6]() # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_utils.py:271:0,\n",
       " %42 : bool = prim::Constant[value=0]() # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_utils.py:271:0,\n",
       " %43 : bool = prim::Constant[value=0]() # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_utils.py:271:0,\n",
       " %44 : None = prim::Constant(),\n",
       " %45 : Float(1:100, 1:100, 1:100, 100:1) = aten::to(%extended_attention_mask, %41, %42, %43, %44) # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_utils.py:271:0,\n",
       " %46 : float = prim::Constant[value=1.]() # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/tensor.py:396:0,\n",
       " %47 : int = prim::Constant[value=1]() # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/tensor.py:396:0,\n",
       " %48 : Float(1:100, 1:100, 1:100, 100:1) = aten::rsub(%45, %46, %47) # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/tensor.py:396:0,\n",
       " %49 : Double() = prim::Constant[value={-10000}]() # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_utils.py:272:0,\n",
       " %attention_mask : Float(1:100, 1:100, 1:100, 100:1) = aten::mul(%48, %49) # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_utils.py:272:0,\n",
       " %55 : float = prim::Constant[value=0.10000000000000001](), scope: __module.embeddings/__module.embeddings.dropout # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:973:0,\n",
       " %56 : int = prim::Constant[value=128](), scope: __module.embeddings/__module.embeddings.LayerNorm # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:2048:0,\n",
       " %57 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.embeddings/__module.embeddings.LayerNorm # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:2048:0,\n",
       " %58 : bool = prim::Constant[value=1](), scope: __module.embeddings/__module.embeddings.LayerNorm # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:2048:0,\n",
       " %59 : int = prim::Constant[value=-1](), scope: __module.embeddings/__module.embeddings.position_embeddings # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1814:0,\n",
       " %60 : bool = prim::Constant[value=0](), scope: __module.embeddings/__module.embeddings.word_embeddings # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1814:0,\n",
       " %61 : int = prim::Constant[value=9223372036854775807](), scope: __module.embeddings # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_bert.py:191:0,\n",
       " %62 : int = prim::Constant[value=0](), scope: __module.embeddings # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_bert.py:191:0,\n",
       " %63 : int = prim::Constant[value=1](), scope: __module.embeddings # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_bert.py:184:0,\n",
       " %64 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name=\"LayerNorm\"](%4),\n",
       " %65 : __torch__.torch.nn.modules.sparse.___torch_mangle_3.Embedding = prim::GetAttr[name=\"token_type_embeddings\"](%4),\n",
       " %66 : __torch__.torch.nn.modules.sparse.___torch_mangle_2.Embedding = prim::GetAttr[name=\"position_embeddings\"](%4),\n",
       " %67 : __torch__.torch.nn.modules.sparse.Embedding = prim::GetAttr[name=\"word_embeddings\"](%4),\n",
       " %68 : Tensor = prim::GetAttr[name=\"position_ids\"](%4),\n",
       " %69 : int = aten::size(%input_ids, %63), scope: __module.embeddings # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_bert.py:184:0,\n",
       " %70 : Long(1:512, 512:1) = aten::slice(%68, %62, %62, %61, %63), scope: __module.embeddings # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_bert.py:191:0,\n",
       " %input.1 : Long(1:512, 100:1) = aten::slice(%70, %63, %62, %69, %63), scope: __module.embeddings # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_bert.py:191:0,\n",
       " %72 : Tensor = prim::GetAttr[name=\"weight\"](%67),\n",
       " %inputs_embeds : Float(1:12800, 100:128, 128:1) = aten::embedding(%72, %input_ids, %62, %60, %60), scope: __module.embeddings/__module.embeddings.word_embeddings # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1814:0,\n",
       " %74 : Tensor = prim::GetAttr[name=\"weight\"](%66),\n",
       " %position_embeddings : Float(1:12800, 100:128, 128:1) = aten::embedding(%74, %input.1, %59, %60, %60), scope: __module.embeddings/__module.embeddings.position_embeddings # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1814:0,\n",
       " %76 : Tensor = prim::GetAttr[name=\"weight\"](%65),\n",
       " %token_type_embeddings : Float(1:12800, 100:128, 128:1) = aten::embedding(%76, %input.2, %59, %60, %60), scope: __module.embeddings/__module.embeddings.token_type_embeddings # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1814:0,\n",
       " %78 : Float(1:12800, 100:128, 128:1) = aten::add(%inputs_embeds, %position_embeddings, %63), scope: __module.embeddings # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_bert.py:201:0,\n",
       " %input.3 : Float(1:12800, 100:128, 128:1) = aten::add(%78, %token_type_embeddings, %63), scope: __module.embeddings # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_bert.py:201:0,\n",
       " %80 : Tensor = prim::GetAttr[name=\"bias\"](%64),\n",
       " %81 : Tensor = prim::GetAttr[name=\"weight\"](%64),\n",
       " %82 : int[] = prim::ListConstruct(%56), scope: __module.embeddings/__module.embeddings.LayerNorm,\n",
       " %input.4 : Float(1:12800, 100:128, 128:1) = aten::layer_norm(%input.3, %82, %81, %80, %57, %58), scope: __module.embeddings/__module.embeddings.LayerNorm # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:2048:0,\n",
       " %input.5 : Float(1:12800, 100:128, 128:1) = aten::dropout(%input.4, %55, %60), scope: __module.embeddings/__module.embeddings.dropout # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:973:0,\n",
       " %85 : bool = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.output/__module.encoder.layer.0.attention.output.LayerNorm # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:2048:0,\n",
       " %86 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.output/__module.encoder.layer.0.attention.output.LayerNorm # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:2048:0,\n",
       " %87 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self/__module.encoder.layer.0.attention.self.query # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1678:0,\n",
       " %88 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_bert.py:227:0,\n",
       " %89 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_bert.py:228:0,\n",
       " %90 : int = prim::Constant[value=64](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_bert.py:228:0,\n",
       " %91 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_bert.py:229:0,\n",
       " %92 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_bert.py:258:0,\n",
       " %93 : int = prim::Constant[value=-2](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_bert.py:258:0,\n",
       " %94 : Double() = prim::Constant[value={8}](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_bert.py:259:0,\n",
       " %95 : None = prim::Constant(), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self,\n",
       " %96 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self/__module.encoder.layer.0.attention.self.dropout # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:973:0,\n",
       " %97 : float = prim::Constant[value=0.10000000000000001](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self/__module.encoder.layer.0.attention.self.dropout # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:973:0,\n",
       " %98 : int = prim::Constant[value=128](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_bert.py:279:0,\n",
       " %99 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name=\"layer\"](%3),\n",
       " %100 : __torch__.transformers.modeling_bert.___torch_mangle_31.BertLayer = prim::GetAttr[name=\"1\"](%99),\n",
       " %101 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name=\"layer\"](%3),\n",
       " %102 : __torch__.transformers.modeling_bert.BertLayer = prim::GetAttr[name=\"0\"](%101),\n",
       " %103 : __torch__.transformers.modeling_bert.BertOutput = prim::GetAttr[name=\"output\"](%102),\n",
       " %104 : __torch__.transformers.modeling_bert.BertIntermediate = prim::GetAttr[name=\"intermediate\"](%102),\n",
       " %105 : __torch__.transformers.modeling_bert.BertAttention = prim::GetAttr[name=\"attention\"](%102),\n",
       " %106 : __torch__.transformers.modeling_bert.BertSelfOutput = prim::GetAttr[name=\"output\"](%105),\n",
       " %107 : __torch__.transformers.modeling_bert.BertSelfAttention = prim::GetAttr[name=\"self\"](%105),\n",
       " %108 : __torch__.torch.nn.modules.linear.___torch_mangle_6.Linear = prim::GetAttr[name=\"value\"](%107),\n",
       " %109 : __torch__.torch.nn.modules.linear.___torch_mangle_5.Linear = prim::GetAttr[name=\"key\"](%107),\n",
       " %110 : __torch__.torch.nn.modules.linear.___torch_mangle_4.Linear = prim::GetAttr[name=\"query\"](%107),\n",
       " %111 : Tensor = prim::GetAttr[name=\"bias\"](%110),\n",
       " %112 : Tensor = prim::GetAttr[name=\"weight\"](%110),\n",
       " %113 : Float(128:1, 128:128) = aten::t(%112), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self/__module.encoder.layer.0.attention.self.query # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1676:0,\n",
       " %output.1 : Float(1:12800, 100:128, 128:1) = aten::matmul(%input.5, %113), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self/__module.encoder.layer.0.attention.self.query # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1676:0,\n",
       " %x.1 : Float(1:12800, 100:128, 128:1) = aten::add_(%output.1, %111, %87), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self/__module.encoder.layer.0.attention.self.query # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1678:0,\n",
       " %116 : Tensor = prim::GetAttr[name=\"bias\"](%109),\n",
       " %117 : Tensor = prim::GetAttr[name=\"weight\"](%109),\n",
       " %118 : Float(128:1, 128:128) = aten::t(%117), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self/__module.encoder.layer.0.attention.self.key # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1676:0,\n",
       " %output.2 : Float(1:12800, 100:128, 128:1) = aten::matmul(%input.5, %118), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self/__module.encoder.layer.0.attention.self.key # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1676:0,\n",
       " %x.3 : Float(1:12800, 100:128, 128:1) = aten::add_(%output.2, %116, %87), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self/__module.encoder.layer.0.attention.self.key # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1678:0,\n",
       " %121 : Tensor = prim::GetAttr[name=\"bias\"](%108),\n",
       " %122 : Tensor = prim::GetAttr[name=\"weight\"](%108),\n",
       " %123 : Float(128:1, 128:128) = aten::t(%122), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self/__module.encoder.layer.0.attention.self.value # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1676:0,\n",
       " %output.3 : Float(1:12800, 100:128, 128:1) = aten::matmul(%input.5, %123), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self/__module.encoder.layer.0.attention.self.value # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1676:0,\n",
       " %x.5 : Float(1:12800, 100:128, 128:1) = aten::add_(%output.3, %121, %87), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self/__module.encoder.layer.0.attention.self.value # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1678:0,\n",
       " %126 : int = aten::size(%x.1, %88), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_bert.py:227:0,\n",
       " %127 : int = aten::size(%x.1, %87), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_bert.py:227:0,\n",
       " %128 : int[] = prim::ListConstruct(%126, %127, %89, %90), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self,\n",
       " %x.2 : Float(1:12800, 100:128, 2:64, 64:1) = aten::view(%x.1, %128), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_bert.py:228:0,\n",
       " %130 : int[] = prim::ListConstruct(%88, %89, %87, %91), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self,\n",
       " %query_layer.1 : Float(1:12800, 2:64, 100:128, 64:1) = aten::permute(%x.2, %130), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_bert.py:229:0,\n",
       " %132 : int = aten::size(%x.3, %88), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_bert.py:227:0,\n",
       " %133 : int = aten::size(%x.3, %87), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_bert.py:227:0,\n",
       " %134 : int[] = prim::ListConstruct(%132, %133, %89, %90), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self,\n",
       " %x.4 : Float(1:12800, 100:128, 2:64, 64:1) = aten::view(%x.3, %134), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_bert.py:228:0,\n",
       " %136 : int[] = prim::ListConstruct(%88, %89, %87, %91), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self,\n",
       " %key_layer.1 : Float(1:12800, 2:64, 100:128, 64:1) = aten::permute(%x.4, %136), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_bert.py:229:0,\n",
       " %138 : int = aten::size(%x.5, %88), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_bert.py:227:0,\n",
       " %139 : int = aten::size(%x.5, %87), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_bert.py:227:0,\n",
       " %140 : int[] = prim::ListConstruct(%138, %139, %89, %90), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self,\n",
       " %x.6 : Float(1:12800, 100:128, 2:64, 64:1) = aten::view(%x.5, %140), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_bert.py:228:0,\n",
       " %142 : int[] = prim::ListConstruct(%88, %89, %87, %91), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self,\n",
       " %value_layer.1 : Float(1:12800, 2:64, 100:128, 64:1) = aten::permute(%x.6, %142), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_bert.py:229:0,\n",
       " %144 : Float(1:12800, 2:64, 64:1, 100:128) = aten::transpose(%key_layer.1, %92, %93), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_bert.py:258:0,\n",
       " %attention_scores.1 : Float(1:20000, 2:10000, 100:100, 100:1) = aten::matmul(%query_layer.1, %144), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_bert.py:258:0,\n",
       " %attention_scores.2 : Float(1:20000, 2:10000, 100:100, 100:1) = aten::div(%attention_scores.1, %94), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_bert.py:259:0,\n",
       " %input.6 : Float(1:20000, 2:10000, 100:100, 100:1) = aten::add(%attention_scores.2, %attention_mask, %87), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_bert.py:262:0,\n",
       " %input.7 : Float(1:20000, 2:10000, 100:100, 100:1) = aten::softmax(%input.6, %92, %95), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1498:0,\n",
       " %attention_probs.1 : Float(1:20000, 2:10000, 100:100, 100:1) = aten::dropout(%input.7, %97, %96), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self/__module.encoder.layer.0.attention.self.dropout # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:973:0,\n",
       " %context_layer.1 : Float(1:12800, 2:6400, 100:64, 64:1) = aten::matmul(%attention_probs.1, %value_layer.1), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_bert.py:275:0,\n",
       " %151 : int[] = prim::ListConstruct(%88, %89, %87, %91), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self,\n",
       " %152 : Float(1:12800, 100:64, 2:6400, 64:1) = aten::permute(%context_layer.1, %151), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_bert.py:277:0,\n",
       " %context_layer.2 : Float(1:12800, 100:128, 2:64, 64:1) = aten::contiguous(%152, %88), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_bert.py:277:0,\n",
       " %154 : int = aten::size(%context_layer.2, %88), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_bert.py:278:0,\n",
       " %155 : int = aten::size(%context_layer.2, %87), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_bert.py:278:0,\n",
       " %156 : int[] = prim::ListConstruct(%154, %155, %98), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self,\n",
       " %input.8 : Float(1:12800, 100:128, 128:1) = aten::view(%context_layer.2, %156), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_bert.py:279:0,\n",
       " %158 : __torch__.torch.nn.modules.normalization.___torch_mangle_9.LayerNorm = prim::GetAttr[name=\"LayerNorm\"](%106),\n",
       " %159 : __torch__.torch.nn.modules.linear.___torch_mangle_8.Linear = prim::GetAttr[name=\"dense\"](%106),\n",
       " %160 : Tensor = prim::GetAttr[name=\"bias\"](%159),\n",
       " %161 : Tensor = prim::GetAttr[name=\"weight\"](%159),\n",
       " %162 : Float(128:1, 128:128) = aten::t(%161), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.output/__module.encoder.layer.0.attention.output.dense # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1676:0,\n",
       " %output.4 : Float(1:12800, 100:128, 128:1) = aten::matmul(%input.8, %162), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.output/__module.encoder.layer.0.attention.output.dense # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1676:0,\n",
       " %input.9 : Float(1:12800, 100:128, 128:1) = aten::add_(%output.4, %160, %87), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.output/__module.encoder.layer.0.attention.output.dense # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1678:0,\n",
       " %hidden_states.1 : Float(1:12800, 100:128, 128:1) = aten::dropout(%input.9, %97, %96), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.output/__module.encoder.layer.0.attention.output.dropout # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:973:0,\n",
       " %input.10 : Float(1:12800, 100:128, 128:1) = aten::add(%hidden_states.1, %input.5, %87), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.output # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_bert.py:295:0,\n",
       " %167 : Tensor = prim::GetAttr[name=\"bias\"](%158),\n",
       " %168 : Tensor = prim::GetAttr[name=\"weight\"](%158),\n",
       " %169 : int[] = prim::ListConstruct(%98), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.output/__module.encoder.layer.0.attention.output.LayerNorm,\n",
       " %input_tensor.1 : Float(1:12800, 100:128, 128:1) = aten::layer_norm(%input.10, %169, %168, %167, %86, %85), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.output/__module.encoder.layer.0.attention.output.LayerNorm # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:2048:0,\n",
       " %171 : __torch__.torch.nn.modules.linear.___torch_mangle_11.Linear = prim::GetAttr[name=\"dense\"](%104),\n",
       " %172 : Tensor = prim::GetAttr[name=\"bias\"](%171),\n",
       " %173 : Tensor = prim::GetAttr[name=\"weight\"](%171),\n",
       " %174 : Float(128:1, 512:128) = aten::t(%173), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.intermediate/__module.encoder.layer.0.intermediate.dense # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1676:0,\n",
       " %output.5 : Float(1:51200, 100:512, 512:1) = aten::matmul(%input_tensor.1, %174), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.intermediate/__module.encoder.layer.0.intermediate.dense # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1676:0,\n",
       " %input.11 : Float(1:51200, 100:512, 512:1) = aten::add_(%output.5, %172, %87), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.intermediate/__module.encoder.layer.0.intermediate.dense # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1678:0,\n",
       " %input.12 : Float(1:51200, 100:512, 512:1) = aten::gelu(%input.11), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.intermediate # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1369:0,\n",
       " %178 : __torch__.torch.nn.modules.normalization.___torch_mangle_13.LayerNorm = prim::GetAttr[name=\"LayerNorm\"](%103),\n",
       " %179 : __torch__.torch.nn.modules.linear.___torch_mangle_12.Linear = prim::GetAttr[name=\"dense\"](%103),\n",
       " %180 : Tensor = prim::GetAttr[name=\"bias\"](%179),\n",
       " %181 : Tensor = prim::GetAttr[name=\"weight\"](%179),\n",
       " %182 : Float(512:1, 128:512) = aten::t(%181), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.output/__module.encoder.layer.0.output.dense # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1676:0,\n",
       " %output.6 : Float(1:12800, 100:128, 128:1) = aten::matmul(%input.12, %182), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.output/__module.encoder.layer.0.output.dense # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1676:0,\n",
       " %input.13 : Float(1:12800, 100:128, 128:1) = aten::add_(%output.6, %180, %87), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.output/__module.encoder.layer.0.output.dense # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1678:0,\n",
       " %hidden_states.2 : Float(1:12800, 100:128, 128:1) = aten::dropout(%input.13, %97, %96), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.output/__module.encoder.layer.0.output.dropout # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:973:0,\n",
       " %input.14 : Float(1:12800, 100:128, 128:1) = aten::add(%hidden_states.2, %input_tensor.1, %87), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.output # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_bert.py:371:0,\n",
       " %187 : Tensor = prim::GetAttr[name=\"bias\"](%178),\n",
       " %188 : Tensor = prim::GetAttr[name=\"weight\"](%178),\n",
       " %189 : int[] = prim::ListConstruct(%98), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.output/__module.encoder.layer.0.output.LayerNorm,\n",
       " %input.15 : Float(1:12800, 100:128, 128:1) = aten::layer_norm(%input.14, %189, %188, %187, %86, %85), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.output/__module.encoder.layer.0.output.LayerNorm # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:2048:0,\n",
       " %191 : __torch__.transformers.modeling_bert.___torch_mangle_30.BertOutput = prim::GetAttr[name=\"output\"](%100),\n",
       " %192 : __torch__.transformers.modeling_bert.___torch_mangle_26.BertIntermediate = prim::GetAttr[name=\"intermediate\"](%100),\n",
       " %193 : __torch__.transformers.modeling_bert.___torch_mangle_24.BertAttention = prim::GetAttr[name=\"attention\"](%100),\n",
       " %194 : __torch__.transformers.modeling_bert.___torch_mangle_23.BertSelfOutput = prim::GetAttr[name=\"output\"](%193),\n",
       " %195 : __torch__.transformers.modeling_bert.___torch_mangle_19.BertSelfAttention = prim::GetAttr[name=\"self\"](%193),\n",
       " %196 : __torch__.torch.nn.modules.linear.___torch_mangle_17.Linear = prim::GetAttr[name=\"value\"](%195),\n",
       " %197 : __torch__.torch.nn.modules.linear.___torch_mangle_16.Linear = prim::GetAttr[name=\"key\"](%195),\n",
       " %198 : __torch__.torch.nn.modules.linear.___torch_mangle_15.Linear = prim::GetAttr[name=\"query\"](%195),\n",
       " %199 : Tensor = prim::GetAttr[name=\"bias\"](%198),\n",
       " %200 : Tensor = prim::GetAttr[name=\"weight\"](%198),\n",
       " %201 : Float(128:1, 128:128) = aten::t(%200), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self/__module.encoder.layer.1.attention.self.query # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1676:0,\n",
       " %output.7 : Float(1:12800, 100:128, 128:1) = aten::matmul(%input.15, %201), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self/__module.encoder.layer.1.attention.self.query # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1676:0,\n",
       " %x.7 : Float(1:12800, 100:128, 128:1) = aten::add_(%output.7, %199, %87), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self/__module.encoder.layer.1.attention.self.query # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1678:0,\n",
       " %204 : Tensor = prim::GetAttr[name=\"bias\"](%197),\n",
       " %205 : Tensor = prim::GetAttr[name=\"weight\"](%197),\n",
       " %206 : Float(128:1, 128:128) = aten::t(%205), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self/__module.encoder.layer.1.attention.self.key # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1676:0,\n",
       " %output.8 : Float(1:12800, 100:128, 128:1) = aten::matmul(%input.15, %206), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self/__module.encoder.layer.1.attention.self.key # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1676:0,\n",
       " %x.9 : Float(1:12800, 100:128, 128:1) = aten::add_(%output.8, %204, %87), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self/__module.encoder.layer.1.attention.self.key # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1678:0,\n",
       " %209 : Tensor = prim::GetAttr[name=\"bias\"](%196),\n",
       " %210 : Tensor = prim::GetAttr[name=\"weight\"](%196),\n",
       " %211 : Float(128:1, 128:128) = aten::t(%210), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self/__module.encoder.layer.1.attention.self.value # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1676:0,\n",
       " %output.9 : Float(1:12800, 100:128, 128:1) = aten::matmul(%input.15, %211), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self/__module.encoder.layer.1.attention.self.value # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1676:0,\n",
       " %x.11 : Float(1:12800, 100:128, 128:1) = aten::add_(%output.9, %209, %87), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self/__module.encoder.layer.1.attention.self.value # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1678:0,\n",
       " %214 : int = aten::size(%x.7, %88), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_bert.py:227:0,\n",
       " %215 : int = aten::size(%x.7, %87), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_bert.py:227:0,\n",
       " %216 : int[] = prim::ListConstruct(%214, %215, %89, %90), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self,\n",
       " %x.8 : Float(1:12800, 100:128, 2:64, 64:1) = aten::view(%x.7, %216), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_bert.py:228:0,\n",
       " %218 : int[] = prim::ListConstruct(%88, %89, %87, %91), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self,\n",
       " %query_layer : Float(1:12800, 2:64, 100:128, 64:1) = aten::permute(%x.8, %218), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_bert.py:229:0,\n",
       " %220 : int = aten::size(%x.9, %88), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_bert.py:227:0,\n",
       " %221 : int = aten::size(%x.9, %87), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_bert.py:227:0,\n",
       " %222 : int[] = prim::ListConstruct(%220, %221, %89, %90), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self,\n",
       " %x.10 : Float(1:12800, 100:128, 2:64, 64:1) = aten::view(%x.9, %222), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_bert.py:228:0,\n",
       " %224 : int[] = prim::ListConstruct(%88, %89, %87, %91), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self,\n",
       " %key_layer : Float(1:12800, 2:64, 100:128, 64:1) = aten::permute(%x.10, %224), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_bert.py:229:0,\n",
       " %226 : int = aten::size(%x.11, %88), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_bert.py:227:0,\n",
       " %227 : int = aten::size(%x.11, %87), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_bert.py:227:0,\n",
       " %228 : int[] = prim::ListConstruct(%226, %227, %89, %90), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self,\n",
       " %x : Float(1:12800, 100:128, 2:64, 64:1) = aten::view(%x.11, %228), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_bert.py:228:0,\n",
       " %230 : int[] = prim::ListConstruct(%88, %89, %87, %91), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self,\n",
       " %value_layer : Float(1:12800, 2:64, 100:128, 64:1) = aten::permute(%x, %230), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_bert.py:229:0,\n",
       " %232 : Float(1:12800, 2:64, 64:1, 100:128) = aten::transpose(%key_layer, %92, %93), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_bert.py:258:0,\n",
       " %attention_scores.3 : Float(1:20000, 2:10000, 100:100, 100:1) = aten::matmul(%query_layer, %232), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_bert.py:258:0,\n",
       " %attention_scores : Float(1:20000, 2:10000, 100:100, 100:1) = aten::div(%attention_scores.3, %94), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_bert.py:259:0,\n",
       " %input.16 : Float(1:20000, 2:10000, 100:100, 100:1) = aten::add(%attention_scores, %attention_mask, %87), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_bert.py:262:0,\n",
       " %input.17 : Float(1:20000, 2:10000, 100:100, 100:1) = aten::softmax(%input.16, %92, %95), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1498:0,\n",
       " %attention_probs : Float(1:20000, 2:10000, 100:100, 100:1) = aten::dropout(%input.17, %97, %96), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self/__module.encoder.layer.1.attention.self.dropout # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:973:0,\n",
       " %context_layer.3 : Float(1:12800, 2:6400, 100:64, 64:1) = aten::matmul(%attention_probs, %value_layer), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_bert.py:275:0,\n",
       " %239 : int[] = prim::ListConstruct(%88, %89, %87, %91), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self,\n",
       " %240 : Float(1:12800, 100:64, 2:6400, 64:1) = aten::permute(%context_layer.3, %239), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_bert.py:277:0,\n",
       " %context_layer : Float(1:12800, 100:128, 2:64, 64:1) = aten::contiguous(%240, %88), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_bert.py:277:0,\n",
       " %242 : int = aten::size(%context_layer, %88), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_bert.py:278:0,\n",
       " %243 : int = aten::size(%context_layer, %87), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_bert.py:278:0,\n",
       " %244 : int[] = prim::ListConstruct(%242, %243, %98), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self,\n",
       " %input.18 : Float(1:12800, 100:128, 128:1) = aten::view(%context_layer, %244), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_bert.py:279:0,\n",
       " %246 : __torch__.torch.nn.modules.normalization.___torch_mangle_21.LayerNorm = prim::GetAttr[name=\"LayerNorm\"](%194),\n",
       " %247 : __torch__.torch.nn.modules.linear.___torch_mangle_20.Linear = prim::GetAttr[name=\"dense\"](%194),\n",
       " %248 : Tensor = prim::GetAttr[name=\"bias\"](%247),\n",
       " %249 : Tensor = prim::GetAttr[name=\"weight\"](%247),\n",
       " %250 : Float(128:1, 128:128) = aten::t(%249), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.output/__module.encoder.layer.1.attention.output.dense # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1676:0,\n",
       " %output.10 : Float(1:12800, 100:128, 128:1) = aten::matmul(%input.18, %250), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.output/__module.encoder.layer.1.attention.output.dense # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1676:0,\n",
       " %input.19 : Float(1:12800, 100:128, 128:1) = aten::add_(%output.10, %248, %87), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.output/__module.encoder.layer.1.attention.output.dense # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1678:0,\n",
       " %hidden_states.3 : Float(1:12800, 100:128, 128:1) = aten::dropout(%input.19, %97, %96), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.output/__module.encoder.layer.1.attention.output.dropout # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:973:0,\n",
       " %input.20 : Float(1:12800, 100:128, 128:1) = aten::add(%hidden_states.3, %input.15, %87), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.output # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_bert.py:295:0,\n",
       " %255 : Tensor = prim::GetAttr[name=\"bias\"](%246),\n",
       " %256 : Tensor = prim::GetAttr[name=\"weight\"](%246),\n",
       " %257 : int[] = prim::ListConstruct(%98), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.output/__module.encoder.layer.1.attention.output.LayerNorm,\n",
       " %input_tensor : Float(1:12800, 100:128, 128:1) = aten::layer_norm(%input.20, %257, %256, %255, %86, %85), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.output/__module.encoder.layer.1.attention.output.LayerNorm # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:2048:0,\n",
       " %259 : __torch__.torch.nn.modules.linear.___torch_mangle_25.Linear = prim::GetAttr[name=\"dense\"](%192),\n",
       " %260 : Tensor = prim::GetAttr[name=\"bias\"](%259),\n",
       " %261 : Tensor = prim::GetAttr[name=\"weight\"](%259),\n",
       " %262 : Float(128:1, 512:128) = aten::t(%261), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.intermediate/__module.encoder.layer.1.intermediate.dense # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1676:0,\n",
       " %output.11 : Float(1:51200, 100:512, 512:1) = aten::matmul(%input_tensor, %262), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.intermediate/__module.encoder.layer.1.intermediate.dense # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1676:0,\n",
       " %input.21 : Float(1:51200, 100:512, 512:1) = aten::add_(%output.11, %260, %87), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.intermediate/__module.encoder.layer.1.intermediate.dense # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1678:0,\n",
       " %input.22 : Float(1:51200, 100:512, 512:1) = aten::gelu(%input.21), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.intermediate # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1369:0,\n",
       " %266 : __torch__.torch.nn.modules.normalization.___torch_mangle_28.LayerNorm = prim::GetAttr[name=\"LayerNorm\"](%191),\n",
       " %267 : __torch__.torch.nn.modules.linear.___torch_mangle_27.Linear = prim::GetAttr[name=\"dense\"](%191),\n",
       " %268 : Tensor = prim::GetAttr[name=\"bias\"](%267),\n",
       " %269 : Tensor = prim::GetAttr[name=\"weight\"](%267),\n",
       " %270 : Float(512:1, 128:512) = aten::t(%269), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.output/__module.encoder.layer.1.output.dense # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1676:0,\n",
       " %output : Float(1:12800, 100:128, 128:1) = aten::matmul(%input.22, %270), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.output/__module.encoder.layer.1.output.dense # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1676:0,\n",
       " %input.23 : Float(1:12800, 100:128, 128:1) = aten::add_(%output, %268, %87), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.output/__module.encoder.layer.1.output.dense # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1678:0,\n",
       " %hidden_states.4 : Float(1:12800, 100:128, 128:1) = aten::dropout(%input.23, %97, %96), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.output/__module.encoder.layer.1.output.dropout # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:973:0,\n",
       " %input.24 : Float(1:12800, 100:128, 128:1) = aten::add(%hidden_states.4, %input_tensor, %87), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.output # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_bert.py:371:0,\n",
       " %275 : Tensor = prim::GetAttr[name=\"bias\"](%266),\n",
       " %276 : Tensor = prim::GetAttr[name=\"weight\"](%266),\n",
       " %277 : int[] = prim::ListConstruct(%98), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.output/__module.encoder.layer.1.output.LayerNorm,\n",
       " %hidden_states : Float(1:12800, 100:128, 128:1) = aten::layer_norm(%input.24, %277, %276, %275, %86, %85), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.output/__module.encoder.layer.1.output.LayerNorm # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:2048:0,\n",
       " %279 : int = prim::Constant[value=1](), scope: __module.pooler # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_bert.py:517:0,\n",
       " %280 : int = prim::Constant[value=9223372036854775807](), scope: __module.pooler # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_bert.py:517:0,\n",
       " %281 : int = prim::Constant[value=0](), scope: __module.pooler # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_bert.py:517:0,\n",
       " %282 : __torch__.torch.nn.modules.linear.___torch_mangle_32.Linear = prim::GetAttr[name=\"dense\"](%2),\n",
       " %283 : Float(1:12800, 100:128, 128:1) = aten::slice(%hidden_states, %281, %281, %280, %279), scope: __module.pooler # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_bert.py:517:0,\n",
       " %input.25 : Float(1:12800, 128:1) = aten::select(%283, %279, %281), scope: __module.pooler # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_bert.py:517:0,\n",
       " %285 : Tensor = prim::GetAttr[name=\"bias\"](%282),\n",
       " %286 : Tensor = prim::GetAttr[name=\"weight\"](%282),\n",
       " %287 : Float(128:1, 128:128) = aten::t(%286), scope: __module.pooler/__module.pooler.dense # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1674:0,\n",
       " %input : Float(1:128, 128:1) = aten::addmm(%285, %input.25, %287, %279, %279), scope: __module.pooler/__module.pooler.dense # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1674:0,\n",
       " %289 : Float(1:128, 128:1) = aten::tanh(%input), scope: __module.pooler/__module.pooler.activation # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/modules/activation.py:350:0,\n",
       " %54 : (Float(1:12800, 100:128, 128:1), Float(1:128, 128:1)) = prim::TupleConstruct(%hidden_states, %289)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ni=list(graph.nodes())\n",
    "ni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "%2 : __torch__.transformers.modeling_bert.BertPooler = prim::GetAttr[name=\"pooler\"](%self.1)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ni[0].output().node()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2 defined in (%2 : __torch__.transformers.modeling_bert.BertPooler = prim::GetAttr[name=\"pooler\"](%self.1)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ni[0].output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78 defined in (%78 : Float(1:12800, 100:128, 128:1) = aten::add(%inputs_embeds, %position_embeddings, %63), scope: __module.embeddings # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_bert.py:201:0\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ni[72].output() # build all input leaf data nodes, then construct the opnode with input and output nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "%2 : __torch__.transformers.modeling_bert.BertPooler = prim::GetAttr[name=\"pooler\"](%self.1)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ni[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2 defined in (%2 : __torch__.transformers.modeling_bert.BertPooler = prim::GetAttr[name=\"pooler\"](%self.1)\n",
       " )]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(ni[0].outputs())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[self.1 defined in (%self.1 : __torch__.transformers.modeling_bert.BertModel, %input_ids : Long(1:100, 100:1) = prim::Param()\n",
       " )]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(ni[0].inputs())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "%self.1 : __torch__.transformers.modeling_bert.BertModel, %input_ids : Long(1:100, 100:1) = prim::Param()"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pn=ni[0].input().node()\n",
    "pn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[self.1 defined in (%self.1 : __torch__.transformers.modeling_bert.BertModel, %input_ids : Long(1:100, 100:1) = prim::Param()\n",
       " ),\n",
       " input_ids defined in (%self.1 : __torch__.transformers.modeling_bert.BertModel, %input_ids : Long(1:100, 100:1) = prim::Param()\n",
       " )]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gi=list(graph.inputs())\n",
    "gi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "__torch__.transformers.modeling_bert.BertModel"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(gi[0].node().outputs())[0].type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 100]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(gi[1].node().outputs())[1].type().sizes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'encoder.layer.0.attention.output'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'encoder/encoder.layer.0/encoder.layer.0.attention/encoder.layer.0.attention.output'.split('/')[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%self.1 : __torch__.transformers.modeling_bert.BertModel, %input_ids : Long(1:100, 100:1) = prim::Param()\n",
      "\n",
      "%self.1 : __torch__.transformers.modeling_bert.BertModel, %input_ids : Long(1:100, 100:1) = prim::Param()\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(pn)\n",
    "print(gi[1].node())\n",
    "pn==gi[0].node()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[%2 : __torch__.transformers.modeling_bert.BertPooler = prim::GetAttr[name=\"pooler\"](%self.1),\n",
       " %3 : __torch__.transformers.modeling_bert.BertEncoder = prim::GetAttr[name=\"encoder\"](%self.1),\n",
       " %4 : __torch__.transformers.modeling_bert.BertEmbeddings = prim::GetAttr[name=\"embeddings\"](%self.1)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i.user for i in gi[0].uses()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[54 defined in (%54 : (Float(1:12800, 100:128, 128:1), Float(1:128, 128:1)) = prim::TupleConstruct(%hidden_states, %289)\n",
       " )]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "go=list(graph.outputs())\n",
    "go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.44.1 (20200629.0846)\n",
       " -->\n",
       "<!-- Title: G Pages: 1 -->\n",
       "<svg width=\"296pt\" height=\"438pt\"\n",
       " viewBox=\"0.00 0.00 296.00 438.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 434)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-434 292,-434 292,4 -4,4\"/>\n",
       "<g id=\"clust1\" class=\"cluster\">\n",
       "<title>cluster_0</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"lightgrey\" points=\"8,-56 8,-386 180,-386 180,-56 8,-56\"/>\n",
       "<text text-anchor=\"middle\" x=\"94\" y=\"-370.8\" font-family=\"Times,serif\" font-size=\"14.00\">process #1</text>\n",
       "</g>\n",
       "<g id=\"clust2\" class=\"cluster\">\n",
       "<title>cluster_0</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"lightblue\" points=\"98,-64 98,-355 172,-355 172,-64 98,-64\"/>\n",
       "<text text-anchor=\"middle\" x=\"135\" y=\"-339.8\" font-family=\"Times,serif\" font-size=\"14.00\">process #3</text>\n",
       "</g>\n",
       "<g id=\"clust3\" class=\"cluster\">\n",
       "<title>cluster_1</title>\n",
       "<polygon fill=\"none\" stroke=\"blue\" points=\"206,-64 206,-355 280,-355 280,-64 206,-64\"/>\n",
       "<text text-anchor=\"middle\" x=\"243\" y=\"-339.8\" font-family=\"Times,serif\" font-size=\"14.00\">process #2</text>\n",
       "</g>\n",
       "<!-- a0 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>a0</title>\n",
       "<ellipse fill=\"white\" stroke=\"white\" cx=\"63\" cy=\"-306\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"63\" y=\"-302.3\" font-family=\"Times,serif\" font-size=\"14.00\">a0</text>\n",
       "</g>\n",
       "<!-- a1 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>a1</title>\n",
       "<ellipse fill=\"white\" stroke=\"white\" cx=\"63\" cy=\"-234\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"63\" y=\"-230.3\" font-family=\"Times,serif\" font-size=\"14.00\">a1</text>\n",
       "</g>\n",
       "<!-- a0&#45;&gt;a1 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>a0&#45;&gt;a1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M63,-287.7C63,-279.98 63,-270.71 63,-262.11\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"66.5,-262.1 63,-252.1 59.5,-262.1 66.5,-262.1\"/>\n",
       "</g>\n",
       "<!-- a2 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>a2</title>\n",
       "<ellipse fill=\"white\" stroke=\"white\" cx=\"63\" cy=\"-162\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"63\" y=\"-158.3\" font-family=\"Times,serif\" font-size=\"14.00\">a2</text>\n",
       "</g>\n",
       "<!-- a1&#45;&gt;a2 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>a1&#45;&gt;a2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M63,-215.7C63,-207.98 63,-198.71 63,-190.11\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"66.5,-190.1 63,-180.1 59.5,-190.1 66.5,-190.1\"/>\n",
       "</g>\n",
       "<!-- b3 -->\n",
       "<g id=\"node12\" class=\"node\">\n",
       "<title>b3</title>\n",
       "<ellipse fill=\"lightgrey\" stroke=\"black\" cx=\"241\" cy=\"-90\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"241\" y=\"-86.3\" font-family=\"Times,serif\" font-size=\"14.00\">b3</text>\n",
       "</g>\n",
       "<!-- a1&#45;&gt;b3 -->\n",
       "<g id=\"edge12\" class=\"edge\">\n",
       "<title>a1&#45;&gt;b3</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M82.88,-221.77C86.54,-219.79 90.37,-217.79 94,-216 129.71,-198.41 144.97,-204.93 176,-180 187.85,-170.48 210.03,-138.49 225.16,-115.59\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"228.21,-117.31 230.76,-107.02 222.36,-113.47 228.21,-117.31\"/>\n",
       "</g>\n",
       "<!-- a3 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>a3</title>\n",
       "<ellipse fill=\"white\" stroke=\"white\" cx=\"63\" cy=\"-90\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"63\" y=\"-86.3\" font-family=\"Times,serif\" font-size=\"14.00\">a3</text>\n",
       "</g>\n",
       "<!-- a2&#45;&gt;a3 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>a2&#45;&gt;a3</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M63,-143.7C63,-135.98 63,-126.71 63,-118.11\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"66.5,-118.1 63,-108.1 59.5,-118.1 66.5,-118.1\"/>\n",
       "</g>\n",
       "<!-- a3&#45;&gt;a0 -->\n",
       "<g id=\"edge14\" class=\"edge\">\n",
       "<title>a3&#45;&gt;a0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M49.25,-105.93C41.04,-115.9 31.38,-129.75 27,-144 12.89,-189.88 12.89,-206.12 27,-252 30.29,-262.69 36.54,-273.15 42.93,-281.92\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"40.35,-284.31 49.25,-290.07 45.88,-280.02 40.35,-284.31\"/>\n",
       "</g>\n",
       "<!-- end -->\n",
       "<g id=\"node14\" class=\"node\">\n",
       "<title>end</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"170,-36 134,-36 134,0 170,0 170,-36\"/>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"146,-36 134,-24 \"/>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"134,-12 146,0 \"/>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"158,0 170,-12 \"/>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"170,-24 158,-36 \"/>\n",
       "<text text-anchor=\"middle\" x=\"152\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">end</text>\n",
       "</g>\n",
       "<!-- a3&#45;&gt;end -->\n",
       "<g id=\"edge15\" class=\"edge\">\n",
       "<title>a3&#45;&gt;end</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M75.85,-73.77C81.17,-67.89 87.59,-61.32 94,-56 103.57,-48.06 114.95,-40.46 125.09,-34.23\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"127.17,-37.07 133.96,-28.93 123.58,-31.06 127.17,-37.07\"/>\n",
       "</g>\n",
       "<!-- c0 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>c0</title>\n",
       "<ellipse fill=\"white\" stroke=\"white\" cx=\"135\" cy=\"-306\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"135\" y=\"-302.3\" font-family=\"Times,serif\" font-size=\"14.00\">c0</text>\n",
       "</g>\n",
       "<!-- c1 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>c1</title>\n",
       "<ellipse fill=\"white\" stroke=\"white\" cx=\"135\" cy=\"-234\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"135\" y=\"-230.3\" font-family=\"Times,serif\" font-size=\"14.00\">c1</text>\n",
       "</g>\n",
       "<!-- c0&#45;&gt;c1 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>c0&#45;&gt;c1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M135,-287.7C135,-279.98 135,-270.71 135,-262.11\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"138.5,-262.1 135,-252.1 131.5,-262.1 138.5,-262.1\"/>\n",
       "</g>\n",
       "<!-- c2 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>c2</title>\n",
       "<ellipse fill=\"white\" stroke=\"white\" cx=\"135\" cy=\"-162\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"135\" y=\"-158.3\" font-family=\"Times,serif\" font-size=\"14.00\">c2</text>\n",
       "</g>\n",
       "<!-- c1&#45;&gt;c2 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>c1&#45;&gt;c2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M135,-215.7C135,-207.98 135,-198.71 135,-190.11\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"138.5,-190.1 135,-180.1 131.5,-190.1 138.5,-190.1\"/>\n",
       "</g>\n",
       "<!-- c3 -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>c3</title>\n",
       "<ellipse fill=\"white\" stroke=\"white\" cx=\"135\" cy=\"-90\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"135\" y=\"-86.3\" font-family=\"Times,serif\" font-size=\"14.00\">c3</text>\n",
       "</g>\n",
       "<!-- c2&#45;&gt;c3 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>c2&#45;&gt;c3</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M135,-143.7C135,-135.98 135,-126.71 135,-118.11\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"138.5,-118.1 135,-108.1 131.5,-118.1 138.5,-118.1\"/>\n",
       "</g>\n",
       "<!-- b0 -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>b0</title>\n",
       "<ellipse fill=\"lightgrey\" stroke=\"black\" cx=\"241\" cy=\"-306\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"241\" y=\"-302.3\" font-family=\"Times,serif\" font-size=\"14.00\">b0</text>\n",
       "</g>\n",
       "<!-- b1 -->\n",
       "<g id=\"node10\" class=\"node\">\n",
       "<title>b1</title>\n",
       "<ellipse fill=\"lightgrey\" stroke=\"black\" cx=\"243\" cy=\"-234\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"243\" y=\"-230.3\" font-family=\"Times,serif\" font-size=\"14.00\">b1</text>\n",
       "</g>\n",
       "<!-- b0&#45;&gt;b1 -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>b0&#45;&gt;b1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M241.49,-287.7C241.71,-279.98 241.98,-270.71 242.23,-262.11\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"245.72,-262.2 242.51,-252.1 238.73,-262 245.72,-262.2\"/>\n",
       "</g>\n",
       "<!-- b2 -->\n",
       "<g id=\"node11\" class=\"node\">\n",
       "<title>b2</title>\n",
       "<ellipse fill=\"lightgrey\" stroke=\"black\" cx=\"245\" cy=\"-162\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"245\" y=\"-158.3\" font-family=\"Times,serif\" font-size=\"14.00\">b2</text>\n",
       "</g>\n",
       "<!-- b1&#45;&gt;b2 -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>b1&#45;&gt;b2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M243.49,-215.7C243.71,-207.98 243.98,-198.71 244.23,-190.11\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"247.72,-190.2 244.51,-180.1 240.73,-190 247.72,-190.2\"/>\n",
       "</g>\n",
       "<!-- b2&#45;&gt;a3 -->\n",
       "<g id=\"edge13\" class=\"edge\">\n",
       "<title>b2&#45;&gt;a3</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M220.73,-153.83C190.49,-144.6 137.52,-127.41 94,-108 93.31,-107.69 92.61,-107.37 91.9,-107.04\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"93.28,-103.82 82.77,-102.47 90.14,-110.07 93.28,-103.82\"/>\n",
       "</g>\n",
       "<!-- b2&#45;&gt;b3 -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>b2&#45;&gt;b3</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M244.01,-143.7C243.57,-135.98 243.04,-126.71 242.55,-118.11\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"246.04,-117.89 241.98,-108.1 239.05,-118.29 246.04,-117.89\"/>\n",
       "</g>\n",
       "<!-- b3&#45;&gt;end -->\n",
       "<g id=\"edge16\" class=\"edge\">\n",
       "<title>b3&#45;&gt;end</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M224.26,-75.83C211.26,-65.61 193,-51.24 178.08,-39.51\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"180.22,-36.74 170.2,-33.31 175.89,-42.25 180.22,-36.74\"/>\n",
       "</g>\n",
       "<!-- start -->\n",
       "<g id=\"node13\" class=\"node\">\n",
       "<title>start</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"152,-430 115.47,-412 152,-394 188.53,-412 152,-430\"/>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"126.23,-417.3 126.23,-406.7 \"/>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"141.24,-399.3 162.76,-399.3 \"/>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"177.77,-406.7 177.77,-417.3 \"/>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"162.76,-424.7 141.24,-424.7 \"/>\n",
       "<text text-anchor=\"middle\" x=\"152\" y=\"-408.3\" font-family=\"Times,serif\" font-size=\"14.00\">start</text>\n",
       "</g>\n",
       "<!-- start&#45;&gt;a0 -->\n",
       "<g id=\"edge10\" class=\"edge\">\n",
       "<title>start&#45;&gt;a0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M128.61,-405.52C116.91,-401.74 103.3,-395.61 94,-386 80.26,-371.79 72.4,-350.78 68.04,-334\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"71.44,-333.14 65.76,-324.19 64.62,-334.73 71.44,-333.14\"/>\n",
       "</g>\n",
       "<!-- start&#45;&gt;b0 -->\n",
       "<g id=\"edge11\" class=\"edge\">\n",
       "<title>start&#45;&gt;b0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M163.42,-399.47C167.47,-395.29 172,-390.5 176,-386 192.13,-367.86 209.57,-346.57 222.32,-330.66\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"225.38,-332.43 228.88,-322.43 219.91,-328.07 225.38,-332.43\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.dot.Digraph at 0x1393f1700>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from graphviz import Digraph\n",
    "\n",
    "g = Digraph('G')\n",
    "\n",
    "c0 = Digraph('cluster_0')\n",
    "c0.body.append('style=filled')\n",
    "c0.body.append('color=lightgrey')\n",
    "c0.node_attr.update(style='filled', color='white')\n",
    "c0.edges([('a0', 'a1'), ('a1', 'a2'), ('a2', 'a3')])\n",
    "c0.body.append('label =\"process #1\"')\n",
    "\n",
    "c2 = Digraph('cluster_0')\n",
    "c2.body.append('style=filled')\n",
    "c2.body.append('color=lightgrey')\n",
    "c2.node_attr.update(style='filled', color='white')\n",
    "c2.edges([('c0', 'c1'), ('c1', 'c2'), ('c2', 'c3')])\n",
    "c2.body.append('label =\"process #3\"')\n",
    "c2.body.append('color=lightblue')\n",
    "c0.subgraph(c2)\n",
    "\n",
    "c1 = Digraph('cluster_1')\n",
    "c1.node_attr.update(style='filled')\n",
    "c1.edges([('b0', 'b1'), ('b1', 'b2'), ('b2', 'b3')])\n",
    "c1.body.append('label =\"process #2\"')\n",
    "c1.body.append('color=blue')\n",
    "\n",
    "g.subgraph(c0)\n",
    "g.subgraph(c1)\n",
    "\n",
    "g.edge('start', 'a0')\n",
    "g.edge('start', 'b0')\n",
    "g.edge('a1', 'b3')\n",
    "g.edge('b2', 'a3')\n",
    "g.edge('a3', 'a0')\n",
    "g.edge('a3', 'end')\n",
    "g.edge('b3', 'end')\n",
    "\n",
    "g.node('start', shape='Mdiamond')\n",
    "g.node('end', shape='Msquare')\n",
    "\n",
    "g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sub_scopes='encoder.layer.0.attention.output.LayerNorm'.split('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'false'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'true' if '' and len(''.split('.')) else 'false'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 encoder.layer.0.attention.output encoder.layer.0.attention.output.LayerNorm\n",
      "5 encoder.layer.0.attention encoder.layer.0.attention.output\n",
      "4 encoder.layer.0 encoder.layer.0.attention\n",
      "3 encoder.layer encoder.layer.0\n",
      "2 encoder encoder.layer\n",
      "1  encoder\n"
     ]
    }
   ],
   "source": [
    "sub_scopes='encoder.layer.0.attention.output.LayerNorm'.split('.')\n",
    "for si in range(len(sub_scopes), 0, -1):\n",
    "    p_scope = '.'.join(sub_scopes[0:si-1])\n",
    "    child_scope = '.'.join(sub_scopes[0:si])\n",
    "    print(si, p_scope, child_scope)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
