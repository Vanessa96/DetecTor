{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import autoreload\n",
    "# ?autoreload\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import torch\n",
    "from transformers import BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 128, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 128)\n",
       "    (token_type_embeddings): Embedding(2, 128)\n",
       "    (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=128, out_features=512, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=512, out_features=128, bias=True)\n",
       "          (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=128, out_features=512, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=512, out_features=128, bias=True)\n",
       "          (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BertModel.from_pretrained(\"prajjwal1/bert-tiny\",\n",
    "                                  # add_pooling_layer=False,\n",
    "                                  torchscript=True)\n",
    "\n",
    "inputs = torch.randint(1000, size=(1, 100)).long()\n",
    "# model()\n",
    "# with torch.onnx.select_model_mode_for_export(model, False):\n",
    "  # trace, _ = torch.jit._get_trace_graph(model, args=(inputs,))\n",
    "#     trace = torch.jit.trace(model, (inputs, ))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"_name_or_path\": \"prajjwal1/bert-tiny\",\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"gradient_checkpointing\": false,\n",
       "  \"hidden_act\": \"gelu_fast\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 128,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 512,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 2,\n",
       "  \"num_hidden_layers\": 2,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"torchscript\": true,\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"vocab_size\": 30522\n",
       "}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config=model.config\n",
    "config.hidden_act = 'gelu_fast'\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from transformers.activations import ACT2FN\n",
    "\n",
    "class BertIntermediate(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)\n",
    "        if isinstance(config.hidden_act, str):\n",
    "            self.intermediate_act_fn = ACT2FN[config.hidden_act]\n",
    "        else:\n",
    "            self.intermediate_act_fn = config.hidden_act\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "#         hidden_states = self.intermediate_act_fn(hidden_states)\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertIntermediate(\n",
       "  (dense): Linear(in_features=128, out_features=512, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc_model = BertIntermediate(config)\n",
    "input_len = 32\n",
    "input_states = torch.rand((input_len, config.hidden_size))\n",
    "fc_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_trace = torch.jit.trace(fc_model, input_states)\n",
    "fc_graph = fc_trace.inlined_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "graph(%self.1 : __torch__.BertIntermediate,\n",
       "      %input : Float(32:128, 128:1)):\n",
       "  %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name=\"dense\"](%self.1)\n",
       "  %4 : int = prim::Constant[value=1](), scope: __module.dense # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1674:0\n",
       "  %5 : Tensor = prim::GetAttr[name=\"bias\"](%2)\n",
       "  %6 : Tensor = prim::GetAttr[name=\"weight\"](%2)\n",
       "  %7 : Float(128:1, 512:128) = aten::t(%6), scope: __module.dense # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1674:0\n",
       "  %8 : Float(32:512, 512:1) = aten::addmm(%5, %input, %7, %4, %4), scope: __module.dense # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1674:0\n",
       "  return (%8)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[%2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name=\"dense\"](%self.1),\n",
       " %4 : int = prim::Constant[value=1](), scope: __module.dense # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1674:0,\n",
       " %5 : Tensor = prim::GetAttr[name=\"bias\"](%2),\n",
       " %6 : Tensor = prim::GetAttr[name=\"weight\"](%2),\n",
       " %7 : Float(128:1, 512:128) = aten::t(%6), scope: __module.dense # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1674:0,\n",
       " %8 : Float(32:512, 512:1) = aten::addmm(%5, %input, %7, %4, %4), scope: __module.dense # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1674:0]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(fc_graph.nodes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['copyMetadata',\n",
       " 'debugName',\n",
       " 'inferTypeFrom',\n",
       " 'isCompleteTensor',\n",
       " 'node',\n",
       " 'offset',\n",
       " 'replaceAllUsesWith',\n",
       " 'requiresGrad',\n",
       " 'requires_grad',\n",
       " 'setDebugName',\n",
       " 'setType',\n",
       " 'setTypeAs',\n",
       " 'toIValue',\n",
       " 'type',\n",
       " 'unique',\n",
       " 'uses']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc_in_nodes=list(fc_graph.inputs())\n",
    "[i for i in dir(fc_in_nodes[0]) if not i.startswith('__')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'input'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc_in_nodes[1].debugName()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8 defined in (%8 : Float(32:512, 512:1) = aten::addmm(%5, %input, %7, %4, %4), scope: __module.dense # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1674:0\n",
       " )]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(fc_graph.outputs())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/qqcao/work/transformers/src/transformers/modeling_utils.py:1669: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  input_tensor.shape == tensor_shape for input_tensor in input_tensors\n"
     ]
    }
   ],
   "source": [
    "trace = torch.jit.trace(model, inputs)\n",
    "graph = trace.inlined_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "graph"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "graph(%self.1 : __torch__.transformers.modeling_bert.___torch_mangle_78.BertModel,\n",
       "      %input_ids : Long(1:100, 100:1)):\n",
       "  %2 : __torch__.transformers.modeling_bert.___torch_mangle_77.BertPooler = prim::GetAttr[name=\"pooler\"](%self.1)\n",
       "  %3 : __torch__.transformers.modeling_bert.___torch_mangle_74.BertEncoder = prim::GetAttr[name=\"encoder\"](%self.1)\n",
       "  %4 : __torch__.transformers.modeling_bert.___torch_mangle_38.BertEmbeddings = prim::GetAttr[name=\"embeddings\"](%self.1)\n",
       "  %5 : int = prim::Constant[value=0]() # /Users/qqcao/work/transformers/src/transformers/modeling_bert.py:795:0\n",
       "  %6 : int = aten::size(%input_ids, %5) # /Users/qqcao/work/transformers/src/transformers/modeling_bert.py:795:0\n",
       "  %7 : Long() = prim::NumToTensor(%6)\n",
       "  %8 : int = aten::Int(%7)\n",
       "  %9 : int = aten::Int(%7)\n",
       "  %10 : int = prim::Constant[value=1]() # /Users/qqcao/work/transformers/src/transformers/modeling_bert.py:795:0\n",
       "  %11 : int = aten::size(%input_ids, %10) # /Users/qqcao/work/transformers/src/transformers/modeling_bert.py:795:0\n",
       "  %12 : Long() = prim::NumToTensor(%11)\n",
       "  %13 : int = aten::Int(%12)\n",
       "  %14 : int = aten::Int(%12)\n",
       "  %15 : int[] = prim::ListConstruct(%9, %14)\n",
       "  %16 : int = prim::Constant[value=6]() # /Users/qqcao/work/transformers/src/transformers/modeling_bert.py:804:0\n",
       "  %17 : int = prim::Constant[value=0]() # /Users/qqcao/work/transformers/src/transformers/modeling_bert.py:804:0\n",
       "  %18 : Device = prim::Constant[value=\"cpu\"]() # /Users/qqcao/work/transformers/src/transformers/modeling_bert.py:804:0\n",
       "  %19 : bool = prim::Constant[value=0]() # /Users/qqcao/work/transformers/src/transformers/modeling_bert.py:804:0\n",
       "  %attention_mask.1 : Float(1:100, 100:1) = aten::ones(%15, %16, %17, %18, %19) # /Users/qqcao/work/transformers/src/transformers/modeling_bert.py:804:0\n",
       "  %21 : int[] = prim::ListConstruct(%8, %13)\n",
       "  %22 : int = prim::Constant[value=4]() # /Users/qqcao/work/transformers/src/transformers/modeling_bert.py:806:0\n",
       "  %23 : int = prim::Constant[value=0]() # /Users/qqcao/work/transformers/src/transformers/modeling_bert.py:806:0\n",
       "  %24 : Device = prim::Constant[value=\"cpu\"]() # /Users/qqcao/work/transformers/src/transformers/modeling_bert.py:806:0\n",
       "  %25 : bool = prim::Constant[value=0]() # /Users/qqcao/work/transformers/src/transformers/modeling_bert.py:806:0\n",
       "  %input.2 : Long(1:100, 100:1) = aten::zeros(%21, %22, %23, %24, %25) # /Users/qqcao/work/transformers/src/transformers/modeling_bert.py:806:0\n",
       "  %27 : int = prim::Constant[value=0]() # /Users/qqcao/work/transformers/src/transformers/modeling_utils.py:258:0\n",
       "  %28 : int = prim::Constant[value=0]() # /Users/qqcao/work/transformers/src/transformers/modeling_utils.py:258:0\n",
       "  %29 : int = prim::Constant[value=9223372036854775807]() # /Users/qqcao/work/transformers/src/transformers/modeling_utils.py:258:0\n",
       "  %30 : int = prim::Constant[value=1]() # /Users/qqcao/work/transformers/src/transformers/modeling_utils.py:258:0\n",
       "  %31 : Float(1:100, 100:1) = aten::slice(%attention_mask.1, %27, %28, %29, %30) # /Users/qqcao/work/transformers/src/transformers/modeling_utils.py:258:0\n",
       "  %32 : int = prim::Constant[value=1]() # /Users/qqcao/work/transformers/src/transformers/modeling_utils.py:258:0\n",
       "  %33 : Float(1:100, 1:100, 100:1) = aten::unsqueeze(%31, %32) # /Users/qqcao/work/transformers/src/transformers/modeling_utils.py:258:0\n",
       "  %34 : int = prim::Constant[value=2]() # /Users/qqcao/work/transformers/src/transformers/modeling_utils.py:258:0\n",
       "  %35 : Float(1:100, 1:100, 1:100, 100:1) = aten::unsqueeze(%33, %34) # /Users/qqcao/work/transformers/src/transformers/modeling_utils.py:258:0\n",
       "  %36 : int = prim::Constant[value=3]() # /Users/qqcao/work/transformers/src/transformers/modeling_utils.py:258:0\n",
       "  %37 : int = prim::Constant[value=0]() # /Users/qqcao/work/transformers/src/transformers/modeling_utils.py:258:0\n",
       "  %38 : int = prim::Constant[value=9223372036854775807]() # /Users/qqcao/work/transformers/src/transformers/modeling_utils.py:258:0\n",
       "  %39 : int = prim::Constant[value=1]() # /Users/qqcao/work/transformers/src/transformers/modeling_utils.py:258:0\n",
       "  %extended_attention_mask : Float(1:100, 1:100, 1:100, 100:1) = aten::slice(%35, %36, %37, %38, %39) # /Users/qqcao/work/transformers/src/transformers/modeling_utils.py:258:0\n",
       "  %41 : int = prim::Constant[value=6]() # /Users/qqcao/work/transformers/src/transformers/modeling_utils.py:271:0\n",
       "  %42 : bool = prim::Constant[value=0]() # /Users/qqcao/work/transformers/src/transformers/modeling_utils.py:271:0\n",
       "  %43 : bool = prim::Constant[value=0]() # /Users/qqcao/work/transformers/src/transformers/modeling_utils.py:271:0\n",
       "  %44 : None = prim::Constant()\n",
       "  %45 : Float(1:100, 1:100, 1:100, 100:1) = aten::to(%extended_attention_mask, %41, %42, %43, %44) # /Users/qqcao/work/transformers/src/transformers/modeling_utils.py:271:0\n",
       "  %46 : float = prim::Constant[value=1.]() # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/tensor.py:396:0\n",
       "  %47 : int = prim::Constant[value=1]() # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/tensor.py:396:0\n",
       "  %48 : Float(1:100, 1:100, 1:100, 100:1) = aten::rsub(%45, %46, %47) # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/tensor.py:396:0\n",
       "  %49 : Double() = prim::Constant[value={-10000}]() # /Users/qqcao/work/transformers/src/transformers/modeling_utils.py:272:0\n",
       "  %attention_mask : Float(1:100, 1:100, 1:100, 100:1) = aten::mul(%48, %49) # /Users/qqcao/work/transformers/src/transformers/modeling_utils.py:272:0\n",
       "  %55 : float = prim::Constant[value=0.10000000000000001](), scope: __module.embeddings/__module.embeddings.dropout # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:973:0\n",
       "  %56 : int = prim::Constant[value=128](), scope: __module.embeddings/__module.embeddings.LayerNorm # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:2048:0\n",
       "  %57 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.embeddings/__module.embeddings.LayerNorm # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:2048:0\n",
       "  %58 : bool = prim::Constant[value=1](), scope: __module.embeddings/__module.embeddings.LayerNorm # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:2048:0\n",
       "  %59 : int = prim::Constant[value=-1](), scope: __module.embeddings/__module.embeddings.position_embeddings # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1814:0\n",
       "  %60 : bool = prim::Constant[value=0](), scope: __module.embeddings/__module.embeddings.word_embeddings # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1814:0\n",
       "  %61 : int = prim::Constant[value=9223372036854775807](), scope: __module.embeddings # /Users/qqcao/work/transformers/src/transformers/modeling_bert.py:191:0\n",
       "  %62 : int = prim::Constant[value=0](), scope: __module.embeddings # /Users/qqcao/work/transformers/src/transformers/modeling_bert.py:191:0\n",
       "  %63 : int = prim::Constant[value=1](), scope: __module.embeddings # /Users/qqcao/work/transformers/src/transformers/modeling_bert.py:184:0\n",
       "  %64 : __torch__.torch.nn.modules.normalization.___torch_mangle_36.LayerNorm = prim::GetAttr[name=\"LayerNorm\"](%4)\n",
       "  %65 : __torch__.torch.nn.modules.sparse.___torch_mangle_35.Embedding = prim::GetAttr[name=\"token_type_embeddings\"](%4)\n",
       "  %66 : __torch__.torch.nn.modules.sparse.___torch_mangle_34.Embedding = prim::GetAttr[name=\"position_embeddings\"](%4)\n",
       "  %67 : __torch__.torch.nn.modules.sparse.___torch_mangle_33.Embedding = prim::GetAttr[name=\"word_embeddings\"](%4)\n",
       "  %68 : Tensor = prim::GetAttr[name=\"position_ids\"](%4)\n",
       "  %69 : int = aten::size(%input_ids, %63), scope: __module.embeddings # /Users/qqcao/work/transformers/src/transformers/modeling_bert.py:184:0\n",
       "  %70 : Long(1:512, 512:1) = aten::slice(%68, %62, %62, %61, %63), scope: __module.embeddings # /Users/qqcao/work/transformers/src/transformers/modeling_bert.py:191:0\n",
       "  %input.1 : Long(1:512, 100:1) = aten::slice(%70, %63, %62, %69, %63), scope: __module.embeddings # /Users/qqcao/work/transformers/src/transformers/modeling_bert.py:191:0\n",
       "  %72 : Tensor = prim::GetAttr[name=\"weight\"](%67)\n",
       "  %inputs_embeds : Float(1:12800, 100:128, 128:1) = aten::embedding(%72, %input_ids, %62, %60, %60), scope: __module.embeddings/__module.embeddings.word_embeddings # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1814:0\n",
       "  %74 : Tensor = prim::GetAttr[name=\"weight\"](%66)\n",
       "  %position_embeddings : Float(1:12800, 100:128, 128:1) = aten::embedding(%74, %input.1, %59, %60, %60), scope: __module.embeddings/__module.embeddings.position_embeddings # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1814:0\n",
       "  %76 : Tensor = prim::GetAttr[name=\"weight\"](%65)\n",
       "  %token_type_embeddings : Float(1:12800, 100:128, 128:1) = aten::embedding(%76, %input.2, %59, %60, %60), scope: __module.embeddings/__module.embeddings.token_type_embeddings # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1814:0\n",
       "  %78 : Float(1:12800, 100:128, 128:1) = aten::add(%inputs_embeds, %position_embeddings, %63), scope: __module.embeddings # /Users/qqcao/work/transformers/src/transformers/modeling_bert.py:201:0\n",
       "  %input.3 : Float(1:12800, 100:128, 128:1) = aten::add(%78, %token_type_embeddings, %63), scope: __module.embeddings # /Users/qqcao/work/transformers/src/transformers/modeling_bert.py:201:0\n",
       "  %80 : Tensor = prim::GetAttr[name=\"bias\"](%64)\n",
       "  %81 : Tensor = prim::GetAttr[name=\"weight\"](%64)\n",
       "  %82 : int[] = prim::ListConstruct(%56), scope: __module.embeddings/__module.embeddings.LayerNorm\n",
       "  %input.4 : Float(1:12800, 100:128, 128:1) = aten::layer_norm(%input.3, %82, %81, %80, %57, %58), scope: __module.embeddings/__module.embeddings.LayerNorm # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:2048:0\n",
       "  %input.5 : Float(1:12800, 100:128, 128:1) = aten::dropout(%input.4, %55, %60), scope: __module.embeddings/__module.embeddings.dropout # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:973:0\n",
       "  %85 : bool = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.output/__module.encoder.layer.0.attention.output.LayerNorm # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:2048:0\n",
       "  %86 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.output/__module.encoder.layer.0.attention.output.LayerNorm # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:2048:0\n",
       "  %87 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self/__module.encoder.layer.0.attention.self.query # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1678:0\n",
       "  %88 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # /Users/qqcao/work/transformers/src/transformers/modeling_bert.py:227:0\n",
       "  %89 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # /Users/qqcao/work/transformers/src/transformers/modeling_bert.py:228:0\n",
       "  %90 : int = prim::Constant[value=64](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # /Users/qqcao/work/transformers/src/transformers/modeling_bert.py:228:0\n",
       "  %91 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # /Users/qqcao/work/transformers/src/transformers/modeling_bert.py:229:0\n",
       "  %92 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # /Users/qqcao/work/transformers/src/transformers/modeling_bert.py:258:0\n",
       "  %93 : int = prim::Constant[value=-2](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # /Users/qqcao/work/transformers/src/transformers/modeling_bert.py:258:0\n",
       "  %94 : Double() = prim::Constant[value={8}](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # /Users/qqcao/work/transformers/src/transformers/modeling_bert.py:259:0\n",
       "  %95 : None = prim::Constant(), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self\n",
       "  %96 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self/__module.encoder.layer.0.attention.self.dropout # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:973:0\n",
       "  %97 : float = prim::Constant[value=0.10000000000000001](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self/__module.encoder.layer.0.attention.self.dropout # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:973:0\n",
       "  %98 : int = prim::Constant[value=128](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # /Users/qqcao/work/transformers/src/transformers/modeling_bert.py:279:0\n",
       "  %99 : __torch__.torch.nn.modules.container.___torch_mangle_73.ModuleList = prim::GetAttr[name=\"layer\"](%3)\n",
       "  %100 : __torch__.transformers.modeling_bert.___torch_mangle_72.BertLayer = prim::GetAttr[name=\"1\"](%99)\n",
       "  %101 : __torch__.torch.nn.modules.container.___torch_mangle_73.ModuleList = prim::GetAttr[name=\"layer\"](%3)\n",
       "  %102 : __torch__.transformers.modeling_bert.___torch_mangle_55.BertLayer = prim::GetAttr[name=\"0\"](%101)\n",
       "  %103 : __torch__.transformers.modeling_bert.___torch_mangle_54.BertOutput = prim::GetAttr[name=\"output\"](%102)\n",
       "  %104 : __torch__.transformers.modeling_bert.___torch_mangle_50.BertIntermediate = prim::GetAttr[name=\"intermediate\"](%102)\n",
       "  %105 : __torch__.transformers.modeling_bert.___torch_mangle_48.BertAttention = prim::GetAttr[name=\"attention\"](%102)\n",
       "  %106 : __torch__.transformers.modeling_bert.___torch_mangle_47.BertSelfOutput = prim::GetAttr[name=\"output\"](%105)\n",
       "  %107 : __torch__.transformers.modeling_bert.___torch_mangle_43.BertSelfAttention = prim::GetAttr[name=\"self\"](%105)\n",
       "  %108 : __torch__.torch.nn.modules.linear.___torch_mangle_41.Linear = prim::GetAttr[name=\"value\"](%107)\n",
       "  %109 : __torch__.torch.nn.modules.linear.___torch_mangle_40.Linear = prim::GetAttr[name=\"key\"](%107)\n",
       "  %110 : __torch__.torch.nn.modules.linear.___torch_mangle_39.Linear = prim::GetAttr[name=\"query\"](%107)\n",
       "  %111 : Tensor = prim::GetAttr[name=\"bias\"](%110)\n",
       "  %112 : Tensor = prim::GetAttr[name=\"weight\"](%110)\n",
       "  %113 : Float(128:1, 128:128) = aten::t(%112), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self/__module.encoder.layer.0.attention.self.query # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1676:0\n",
       "  %output.1 : Float(1:12800, 100:128, 128:1) = aten::matmul(%input.5, %113), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self/__module.encoder.layer.0.attention.self.query # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1676:0\n",
       "  %x.1 : Float(1:12800, 100:128, 128:1) = aten::add_(%output.1, %111, %87), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self/__module.encoder.layer.0.attention.self.query # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1678:0\n",
       "  %116 : Tensor = prim::GetAttr[name=\"bias\"](%109)\n",
       "  %117 : Tensor = prim::GetAttr[name=\"weight\"](%109)\n",
       "  %118 : Float(128:1, 128:128) = aten::t(%117), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self/__module.encoder.layer.0.attention.self.key # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1676:0\n",
       "  %output.2 : Float(1:12800, 100:128, 128:1) = aten::matmul(%input.5, %118), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self/__module.encoder.layer.0.attention.self.key # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1676:0\n",
       "  %x.3 : Float(1:12800, 100:128, 128:1) = aten::add_(%output.2, %116, %87), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self/__module.encoder.layer.0.attention.self.key # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1678:0\n",
       "  %121 : Tensor = prim::GetAttr[name=\"bias\"](%108)\n",
       "  %122 : Tensor = prim::GetAttr[name=\"weight\"](%108)\n",
       "  %123 : Float(128:1, 128:128) = aten::t(%122), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self/__module.encoder.layer.0.attention.self.value # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1676:0\n",
       "  %output.3 : Float(1:12800, 100:128, 128:1) = aten::matmul(%input.5, %123), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self/__module.encoder.layer.0.attention.self.value # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1676:0\n",
       "  %x.5 : Float(1:12800, 100:128, 128:1) = aten::add_(%output.3, %121, %87), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self/__module.encoder.layer.0.attention.self.value # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1678:0\n",
       "  %126 : int = aten::size(%x.1, %88), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # /Users/qqcao/work/transformers/src/transformers/modeling_bert.py:227:0\n",
       "  %127 : int = aten::size(%x.1, %87), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # /Users/qqcao/work/transformers/src/transformers/modeling_bert.py:227:0\n",
       "  %128 : int[] = prim::ListConstruct(%126, %127, %89, %90), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self\n",
       "  %x.2 : Float(1:12800, 100:128, 2:64, 64:1) = aten::view(%x.1, %128), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # /Users/qqcao/work/transformers/src/transformers/modeling_bert.py:228:0\n",
       "  %130 : int[] = prim::ListConstruct(%88, %89, %87, %91), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self\n",
       "  %query_layer.1 : Float(1:12800, 2:64, 100:128, 64:1) = aten::permute(%x.2, %130), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # /Users/qqcao/work/transformers/src/transformers/modeling_bert.py:229:0\n",
       "  %132 : int = aten::size(%x.3, %88), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # /Users/qqcao/work/transformers/src/transformers/modeling_bert.py:227:0\n",
       "  %133 : int = aten::size(%x.3, %87), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # /Users/qqcao/work/transformers/src/transformers/modeling_bert.py:227:0\n",
       "  %134 : int[] = prim::ListConstruct(%132, %133, %89, %90), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self\n",
       "  %x.4 : Float(1:12800, 100:128, 2:64, 64:1) = aten::view(%x.3, %134), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # /Users/qqcao/work/transformers/src/transformers/modeling_bert.py:228:0\n",
       "  %136 : int[] = prim::ListConstruct(%88, %89, %87, %91), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self\n",
       "  %key_layer.1 : Float(1:12800, 2:64, 100:128, 64:1) = aten::permute(%x.4, %136), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # /Users/qqcao/work/transformers/src/transformers/modeling_bert.py:229:0\n",
       "  %138 : int = aten::size(%x.5, %88), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # /Users/qqcao/work/transformers/src/transformers/modeling_bert.py:227:0\n",
       "  %139 : int = aten::size(%x.5, %87), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # /Users/qqcao/work/transformers/src/transformers/modeling_bert.py:227:0\n",
       "  %140 : int[] = prim::ListConstruct(%138, %139, %89, %90), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self\n",
       "  %x.6 : Float(1:12800, 100:128, 2:64, 64:1) = aten::view(%x.5, %140), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # /Users/qqcao/work/transformers/src/transformers/modeling_bert.py:228:0\n",
       "  %142 : int[] = prim::ListConstruct(%88, %89, %87, %91), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self\n",
       "  %value_layer.1 : Float(1:12800, 2:64, 100:128, 64:1) = aten::permute(%x.6, %142), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # /Users/qqcao/work/transformers/src/transformers/modeling_bert.py:229:0\n",
       "  %144 : Float(1:12800, 2:64, 64:1, 100:128) = aten::transpose(%key_layer.1, %92, %93), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # /Users/qqcao/work/transformers/src/transformers/modeling_bert.py:258:0\n",
       "  %attention_scores.1 : Float(1:20000, 2:10000, 100:100, 100:1) = aten::matmul(%query_layer.1, %144), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # /Users/qqcao/work/transformers/src/transformers/modeling_bert.py:258:0\n",
       "  %attention_scores.2 : Float(1:20000, 2:10000, 100:100, 100:1) = aten::div(%attention_scores.1, %94), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # /Users/qqcao/work/transformers/src/transformers/modeling_bert.py:259:0\n",
       "  %input.6 : Float(1:20000, 2:10000, 100:100, 100:1) = aten::add(%attention_scores.2, %attention_mask, %87), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # /Users/qqcao/work/transformers/src/transformers/modeling_bert.py:262:0\n",
       "  %input.7 : Float(1:20000, 2:10000, 100:100, 100:1) = aten::softmax(%input.6, %92, %95), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1498:0\n",
       "  %attention_probs.1 : Float(1:20000, 2:10000, 100:100, 100:1) = aten::dropout(%input.7, %97, %96), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self/__module.encoder.layer.0.attention.self.dropout # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:973:0\n",
       "  %context_layer.1 : Float(1:12800, 2:6400, 100:64, 64:1) = aten::matmul(%attention_probs.1, %value_layer.1), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # /Users/qqcao/work/transformers/src/transformers/modeling_bert.py:275:0\n",
       "  %151 : int[] = prim::ListConstruct(%88, %89, %87, %91), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self\n",
       "  %152 : Float(1:12800, 100:64, 2:6400, 64:1) = aten::permute(%context_layer.1, %151), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # /Users/qqcao/work/transformers/src/transformers/modeling_bert.py:277:0\n",
       "  %context_layer.2 : Float(1:12800, 100:128, 2:64, 64:1) = aten::contiguous(%152, %88), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # /Users/qqcao/work/transformers/src/transformers/modeling_bert.py:277:0\n",
       "  %154 : int = aten::size(%context_layer.2, %88), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # /Users/qqcao/work/transformers/src/transformers/modeling_bert.py:278:0\n",
       "  %155 : int = aten::size(%context_layer.2, %87), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # /Users/qqcao/work/transformers/src/transformers/modeling_bert.py:278:0\n",
       "  %156 : int[] = prim::ListConstruct(%154, %155, %98), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self\n",
       "  %input.8 : Float(1:12800, 100:128, 128:1) = aten::view(%context_layer.2, %156), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # /Users/qqcao/work/transformers/src/transformers/modeling_bert.py:279:0\n",
       "  %158 : __torch__.torch.nn.modules.normalization.___torch_mangle_45.LayerNorm = prim::GetAttr[name=\"LayerNorm\"](%106)\n",
       "  %159 : __torch__.torch.nn.modules.linear.___torch_mangle_44.Linear = prim::GetAttr[name=\"dense\"](%106)\n",
       "  %160 : Tensor = prim::GetAttr[name=\"bias\"](%159)\n",
       "  %161 : Tensor = prim::GetAttr[name=\"weight\"](%159)\n",
       "  %162 : Float(128:1, 128:128) = aten::t(%161), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.output/__module.encoder.layer.0.attention.output.dense # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1676:0\n",
       "  %output.4 : Float(1:12800, 100:128, 128:1) = aten::matmul(%input.8, %162), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.output/__module.encoder.layer.0.attention.output.dense # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1676:0\n",
       "  %input.9 : Float(1:12800, 100:128, 128:1) = aten::add_(%output.4, %160, %87), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.output/__module.encoder.layer.0.attention.output.dense # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1678:0\n",
       "  %hidden_states.1 : Float(1:12800, 100:128, 128:1) = aten::dropout(%input.9, %97, %96), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.output/__module.encoder.layer.0.attention.output.dropout # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:973:0\n",
       "  %input.10 : Float(1:12800, 100:128, 128:1) = aten::add(%hidden_states.1, %input.5, %87), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.output # /Users/qqcao/work/transformers/src/transformers/modeling_bert.py:295:0\n",
       "  %167 : Tensor = prim::GetAttr[name=\"bias\"](%158)\n",
       "  %168 : Tensor = prim::GetAttr[name=\"weight\"](%158)\n",
       "  %169 : int[] = prim::ListConstruct(%98), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.output/__module.encoder.layer.0.attention.output.LayerNorm\n",
       "  %input_tensor.1 : Float(1:12800, 100:128, 128:1) = aten::layer_norm(%input.10, %169, %168, %167, %86, %85), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.output/__module.encoder.layer.0.attention.output.LayerNorm # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:2048:0\n",
       "  %171 : __torch__.torch.nn.modules.linear.___torch_mangle_49.Linear = prim::GetAttr[name=\"dense\"](%104)\n",
       "  %172 : Tensor = prim::GetAttr[name=\"bias\"](%171)\n",
       "  %173 : Tensor = prim::GetAttr[name=\"weight\"](%171)\n",
       "  %174 : Float(128:1, 512:128) = aten::t(%173), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.intermediate/__module.encoder.layer.0.intermediate.dense # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1676:0\n",
       "  %output.5 : Float(1:51200, 100:512, 512:1) = aten::matmul(%input_tensor.1, %174), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.intermediate/__module.encoder.layer.0.intermediate.dense # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1676:0\n",
       "  %input.11 : Float(1:51200, 100:512, 512:1) = aten::add_(%output.5, %172, %87), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.intermediate/__module.encoder.layer.0.intermediate.dense # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1678:0\n",
       "  %input.12 : Float(1:51200, 100:512, 512:1) = aten::gelu(%input.11), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.intermediate # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1369:0\n",
       "  %178 : __torch__.torch.nn.modules.normalization.___torch_mangle_52.LayerNorm = prim::GetAttr[name=\"LayerNorm\"](%103)\n",
       "  %179 : __torch__.torch.nn.modules.linear.___torch_mangle_51.Linear = prim::GetAttr[name=\"dense\"](%103)\n",
       "  %180 : Tensor = prim::GetAttr[name=\"bias\"](%179)\n",
       "  %181 : Tensor = prim::GetAttr[name=\"weight\"](%179)\n",
       "  %182 : Float(512:1, 128:512) = aten::t(%181), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.output/__module.encoder.layer.0.output.dense # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1676:0\n",
       "  %output.6 : Float(1:12800, 100:128, 128:1) = aten::matmul(%input.12, %182), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.output/__module.encoder.layer.0.output.dense # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1676:0\n",
       "  %input.13 : Float(1:12800, 100:128, 128:1) = aten::add_(%output.6, %180, %87), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.output/__module.encoder.layer.0.output.dense # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1678:0\n",
       "  %hidden_states.2 : Float(1:12800, 100:128, 128:1) = aten::dropout(%input.13, %97, %96), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.output/__module.encoder.layer.0.output.dropout # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:973:0\n",
       "  %input.14 : Float(1:12800, 100:128, 128:1) = aten::add(%hidden_states.2, %input_tensor.1, %87), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.output # /Users/qqcao/work/transformers/src/transformers/modeling_bert.py:371:0\n",
       "  %187 : Tensor = prim::GetAttr[name=\"bias\"](%178)\n",
       "  %188 : Tensor = prim::GetAttr[name=\"weight\"](%178)\n",
       "  %189 : int[] = prim::ListConstruct(%98), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.output/__module.encoder.layer.0.output.LayerNorm\n",
       "  %input.15 : Float(1:12800, 100:128, 128:1) = aten::layer_norm(%input.14, %189, %188, %187, %86, %85), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.output/__module.encoder.layer.0.output.LayerNorm # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:2048:0\n",
       "  %191 : __torch__.transformers.modeling_bert.___torch_mangle_71.BertOutput = prim::GetAttr[name=\"output\"](%100)\n",
       "  %192 : __torch__.transformers.modeling_bert.___torch_mangle_67.BertIntermediate = prim::GetAttr[name=\"intermediate\"](%100)\n",
       "  %193 : __torch__.transformers.modeling_bert.___torch_mangle_65.BertAttention = prim::GetAttr[name=\"attention\"](%100)\n",
       "  %194 : __torch__.transformers.modeling_bert.___torch_mangle_64.BertSelfOutput = prim::GetAttr[name=\"output\"](%193)\n",
       "  %195 : __torch__.transformers.modeling_bert.___torch_mangle_60.BertSelfAttention = prim::GetAttr[name=\"self\"](%193)\n",
       "  %196 : __torch__.torch.nn.modules.linear.___torch_mangle_58.Linear = prim::GetAttr[name=\"value\"](%195)\n",
       "  %197 : __torch__.torch.nn.modules.linear.___torch_mangle_57.Linear = prim::GetAttr[name=\"key\"](%195)\n",
       "  %198 : __torch__.torch.nn.modules.linear.___torch_mangle_56.Linear = prim::GetAttr[name=\"query\"](%195)\n",
       "  %199 : Tensor = prim::GetAttr[name=\"bias\"](%198)\n",
       "  %200 : Tensor = prim::GetAttr[name=\"weight\"](%198)\n",
       "  %201 : Float(128:1, 128:128) = aten::t(%200), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self/__module.encoder.layer.1.attention.self.query # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1676:0\n",
       "  %output.7 : Float(1:12800, 100:128, 128:1) = aten::matmul(%input.15, %201), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self/__module.encoder.layer.1.attention.self.query # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1676:0\n",
       "  %x.7 : Float(1:12800, 100:128, 128:1) = aten::add_(%output.7, %199, %87), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self/__module.encoder.layer.1.attention.self.query # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1678:0\n",
       "  %204 : Tensor = prim::GetAttr[name=\"bias\"](%197)\n",
       "  %205 : Tensor = prim::GetAttr[name=\"weight\"](%197)\n",
       "  %206 : Float(128:1, 128:128) = aten::t(%205), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self/__module.encoder.layer.1.attention.self.key # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1676:0\n",
       "  %output.8 : Float(1:12800, 100:128, 128:1) = aten::matmul(%input.15, %206), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self/__module.encoder.layer.1.attention.self.key # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1676:0\n",
       "  %x.9 : Float(1:12800, 100:128, 128:1) = aten::add_(%output.8, %204, %87), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self/__module.encoder.layer.1.attention.self.key # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1678:0\n",
       "  %209 : Tensor = prim::GetAttr[name=\"bias\"](%196)\n",
       "  %210 : Tensor = prim::GetAttr[name=\"weight\"](%196)\n",
       "  %211 : Float(128:1, 128:128) = aten::t(%210), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self/__module.encoder.layer.1.attention.self.value # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1676:0\n",
       "  %output.9 : Float(1:12800, 100:128, 128:1) = aten::matmul(%input.15, %211), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self/__module.encoder.layer.1.attention.self.value # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1676:0\n",
       "  %x.11 : Float(1:12800, 100:128, 128:1) = aten::add_(%output.9, %209, %87), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self/__module.encoder.layer.1.attention.self.value # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1678:0\n",
       "  %214 : int = aten::size(%x.7, %88), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # /Users/qqcao/work/transformers/src/transformers/modeling_bert.py:227:0\n",
       "  %215 : int = aten::size(%x.7, %87), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # /Users/qqcao/work/transformers/src/transformers/modeling_bert.py:227:0\n",
       "  %216 : int[] = prim::ListConstruct(%214, %215, %89, %90), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self\n",
       "  %x.8 : Float(1:12800, 100:128, 2:64, 64:1) = aten::view(%x.7, %216), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # /Users/qqcao/work/transformers/src/transformers/modeling_bert.py:228:0\n",
       "  %218 : int[] = prim::ListConstruct(%88, %89, %87, %91), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self\n",
       "  %query_layer : Float(1:12800, 2:64, 100:128, 64:1) = aten::permute(%x.8, %218), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # /Users/qqcao/work/transformers/src/transformers/modeling_bert.py:229:0\n",
       "  %220 : int = aten::size(%x.9, %88), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # /Users/qqcao/work/transformers/src/transformers/modeling_bert.py:227:0\n",
       "  %221 : int = aten::size(%x.9, %87), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # /Users/qqcao/work/transformers/src/transformers/modeling_bert.py:227:0\n",
       "  %222 : int[] = prim::ListConstruct(%220, %221, %89, %90), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self\n",
       "  %x.10 : Float(1:12800, 100:128, 2:64, 64:1) = aten::view(%x.9, %222), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # /Users/qqcao/work/transformers/src/transformers/modeling_bert.py:228:0\n",
       "  %224 : int[] = prim::ListConstruct(%88, %89, %87, %91), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self\n",
       "  %key_layer : Float(1:12800, 2:64, 100:128, 64:1) = aten::permute(%x.10, %224), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # /Users/qqcao/work/transformers/src/transformers/modeling_bert.py:229:0\n",
       "  %226 : int = aten::size(%x.11, %88), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # /Users/qqcao/work/transformers/src/transformers/modeling_bert.py:227:0\n",
       "  %227 : int = aten::size(%x.11, %87), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # /Users/qqcao/work/transformers/src/transformers/modeling_bert.py:227:0\n",
       "  %228 : int[] = prim::ListConstruct(%226, %227, %89, %90), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self\n",
       "  %x : Float(1:12800, 100:128, 2:64, 64:1) = aten::view(%x.11, %228), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # /Users/qqcao/work/transformers/src/transformers/modeling_bert.py:228:0\n",
       "  %230 : int[] = prim::ListConstruct(%88, %89, %87, %91), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self\n",
       "  %value_layer : Float(1:12800, 2:64, 100:128, 64:1) = aten::permute(%x, %230), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # /Users/qqcao/work/transformers/src/transformers/modeling_bert.py:229:0\n",
       "  %232 : Float(1:12800, 2:64, 64:1, 100:128) = aten::transpose(%key_layer, %92, %93), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # /Users/qqcao/work/transformers/src/transformers/modeling_bert.py:258:0\n",
       "  %attention_scores.3 : Float(1:20000, 2:10000, 100:100, 100:1) = aten::matmul(%query_layer, %232), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # /Users/qqcao/work/transformers/src/transformers/modeling_bert.py:258:0\n",
       "  %attention_scores : Float(1:20000, 2:10000, 100:100, 100:1) = aten::div(%attention_scores.3, %94), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # /Users/qqcao/work/transformers/src/transformers/modeling_bert.py:259:0\n",
       "  %input.16 : Float(1:20000, 2:10000, 100:100, 100:1) = aten::add(%attention_scores, %attention_mask, %87), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # /Users/qqcao/work/transformers/src/transformers/modeling_bert.py:262:0\n",
       "  %input.17 : Float(1:20000, 2:10000, 100:100, 100:1) = aten::softmax(%input.16, %92, %95), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1498:0\n",
       "  %attention_probs : Float(1:20000, 2:10000, 100:100, 100:1) = aten::dropout(%input.17, %97, %96), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self/__module.encoder.layer.1.attention.self.dropout # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:973:0\n",
       "  %context_layer.3 : Float(1:12800, 2:6400, 100:64, 64:1) = aten::matmul(%attention_probs, %value_layer), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # /Users/qqcao/work/transformers/src/transformers/modeling_bert.py:275:0\n",
       "  %239 : int[] = prim::ListConstruct(%88, %89, %87, %91), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self\n",
       "  %240 : Float(1:12800, 100:64, 2:6400, 64:1) = aten::permute(%context_layer.3, %239), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # /Users/qqcao/work/transformers/src/transformers/modeling_bert.py:277:0\n",
       "  %context_layer : Float(1:12800, 100:128, 2:64, 64:1) = aten::contiguous(%240, %88), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # /Users/qqcao/work/transformers/src/transformers/modeling_bert.py:277:0\n",
       "  %242 : int = aten::size(%context_layer, %88), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # /Users/qqcao/work/transformers/src/transformers/modeling_bert.py:278:0\n",
       "  %243 : int = aten::size(%context_layer, %87), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # /Users/qqcao/work/transformers/src/transformers/modeling_bert.py:278:0\n",
       "  %244 : int[] = prim::ListConstruct(%242, %243, %98), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self\n",
       "  %input.18 : Float(1:12800, 100:128, 128:1) = aten::view(%context_layer, %244), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # /Users/qqcao/work/transformers/src/transformers/modeling_bert.py:279:0\n",
       "  %246 : __torch__.torch.nn.modules.normalization.___torch_mangle_62.LayerNorm = prim::GetAttr[name=\"LayerNorm\"](%194)\n",
       "  %247 : __torch__.torch.nn.modules.linear.___torch_mangle_61.Linear = prim::GetAttr[name=\"dense\"](%194)\n",
       "  %248 : Tensor = prim::GetAttr[name=\"bias\"](%247)\n",
       "  %249 : Tensor = prim::GetAttr[name=\"weight\"](%247)\n",
       "  %250 : Float(128:1, 128:128) = aten::t(%249), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.output/__module.encoder.layer.1.attention.output.dense # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1676:0\n",
       "  %output.10 : Float(1:12800, 100:128, 128:1) = aten::matmul(%input.18, %250), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.output/__module.encoder.layer.1.attention.output.dense # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1676:0\n",
       "  %input.19 : Float(1:12800, 100:128, 128:1) = aten::add_(%output.10, %248, %87), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.output/__module.encoder.layer.1.attention.output.dense # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1678:0\n",
       "  %hidden_states.3 : Float(1:12800, 100:128, 128:1) = aten::dropout(%input.19, %97, %96), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.output/__module.encoder.layer.1.attention.output.dropout # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:973:0\n",
       "  %input.20 : Float(1:12800, 100:128, 128:1) = aten::add(%hidden_states.3, %input.15, %87), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.output # /Users/qqcao/work/transformers/src/transformers/modeling_bert.py:295:0\n",
       "  %255 : Tensor = prim::GetAttr[name=\"bias\"](%246)\n",
       "  %256 : Tensor = prim::GetAttr[name=\"weight\"](%246)\n",
       "  %257 : int[] = prim::ListConstruct(%98), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.output/__module.encoder.layer.1.attention.output.LayerNorm\n",
       "  %input_tensor : Float(1:12800, 100:128, 128:1) = aten::layer_norm(%input.20, %257, %256, %255, %86, %85), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.output/__module.encoder.layer.1.attention.output.LayerNorm # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:2048:0\n",
       "  %259 : __torch__.torch.nn.modules.linear.___torch_mangle_66.Linear = prim::GetAttr[name=\"dense\"](%192)\n",
       "  %260 : Tensor = prim::GetAttr[name=\"bias\"](%259)\n",
       "  %261 : Tensor = prim::GetAttr[name=\"weight\"](%259)\n",
       "  %262 : Float(128:1, 512:128) = aten::t(%261), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.intermediate/__module.encoder.layer.1.intermediate.dense # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1676:0\n",
       "  %output.11 : Float(1:51200, 100:512, 512:1) = aten::matmul(%input_tensor, %262), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.intermediate/__module.encoder.layer.1.intermediate.dense # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1676:0\n",
       "  %input.21 : Float(1:51200, 100:512, 512:1) = aten::add_(%output.11, %260, %87), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.intermediate/__module.encoder.layer.1.intermediate.dense # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1678:0\n",
       "  %input.22 : Float(1:51200, 100:512, 512:1) = aten::gelu(%input.21), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.intermediate # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1369:0\n",
       "  %266 : __torch__.torch.nn.modules.normalization.___torch_mangle_69.LayerNorm = prim::GetAttr[name=\"LayerNorm\"](%191)\n",
       "  %267 : __torch__.torch.nn.modules.linear.___torch_mangle_68.Linear = prim::GetAttr[name=\"dense\"](%191)\n",
       "  %268 : Tensor = prim::GetAttr[name=\"bias\"](%267)\n",
       "  %269 : Tensor = prim::GetAttr[name=\"weight\"](%267)\n",
       "  %270 : Float(512:1, 128:512) = aten::t(%269), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.output/__module.encoder.layer.1.output.dense # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1676:0\n",
       "  %output : Float(1:12800, 100:128, 128:1) = aten::matmul(%input.22, %270), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.output/__module.encoder.layer.1.output.dense # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1676:0\n",
       "  %input.23 : Float(1:12800, 100:128, 128:1) = aten::add_(%output, %268, %87), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.output/__module.encoder.layer.1.output.dense # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1678:0\n",
       "  %hidden_states.4 : Float(1:12800, 100:128, 128:1) = aten::dropout(%input.23, %97, %96), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.output/__module.encoder.layer.1.output.dropout # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:973:0\n",
       "  %input.24 : Float(1:12800, 100:128, 128:1) = aten::add(%hidden_states.4, %input_tensor, %87), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.output # /Users/qqcao/work/transformers/src/transformers/modeling_bert.py:371:0\n",
       "  %275 : Tensor = prim::GetAttr[name=\"bias\"](%266)\n",
       "  %276 : Tensor = prim::GetAttr[name=\"weight\"](%266)\n",
       "  %277 : int[] = prim::ListConstruct(%98), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.output/__module.encoder.layer.1.output.LayerNorm\n",
       "  %hidden_states : Float(1:12800, 100:128, 128:1) = aten::layer_norm(%input.24, %277, %276, %275, %86, %85), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.output/__module.encoder.layer.1.output.LayerNorm # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:2048:0\n",
       "  %279 : int = prim::Constant[value=1](), scope: __module.pooler # /Users/qqcao/work/transformers/src/transformers/modeling_bert.py:507:0\n",
       "  %280 : int = prim::Constant[value=9223372036854775807](), scope: __module.pooler # /Users/qqcao/work/transformers/src/transformers/modeling_bert.py:507:0\n",
       "  %281 : int = prim::Constant[value=0](), scope: __module.pooler # /Users/qqcao/work/transformers/src/transformers/modeling_bert.py:507:0\n",
       "  %282 : __torch__.torch.nn.modules.linear.___torch_mangle_75.Linear = prim::GetAttr[name=\"dense\"](%2)\n",
       "  %283 : Float(1:12800, 100:128, 128:1) = aten::slice(%hidden_states, %281, %281, %280, %279), scope: __module.pooler # /Users/qqcao/work/transformers/src/transformers/modeling_bert.py:507:0\n",
       "  %input.25 : Float(1:12800, 128:1) = aten::select(%283, %279, %281), scope: __module.pooler # /Users/qqcao/work/transformers/src/transformers/modeling_bert.py:507:0\n",
       "  %285 : Tensor = prim::GetAttr[name=\"bias\"](%282)\n",
       "  %286 : Tensor = prim::GetAttr[name=\"weight\"](%282)\n",
       "  %287 : Float(128:1, 128:128) = aten::t(%286), scope: __module.pooler/__module.pooler.dense # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1674:0\n",
       "  %input : Float(1:128, 128:1) = aten::addmm(%285, %input.25, %287, %279, %279), scope: __module.pooler/__module.pooler.dense # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1674:0\n",
       "  %289 : Float(1:128, 128:1) = aten::tanh(%input), scope: __module.pooler/__module.pooler.activation # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/modules/activation.py:350:0\n",
       "  %54 : (Float(1:12800, 100:128, 128:1), Float(1:128, 128:1)) = prim::TupleConstruct(%hidden_states, %289)\n",
       "  return (%54)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ni=list(n.inputs())\n",
    "ni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5 defined in (%5 : Tensor = prim::GetAttr[name=\"bias\"](%2)\n",
       " ),\n",
       " input defined in (%self.1 : __torch__.___torch_mangle_3.BertIntermediate, %input : Float(32:128, 128:1) = prim::Param()\n",
       " ),\n",
       " 7 defined in (%7 : Float(128:1, 512:128) = aten::t(%6), scope: __module.dense # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1674:0\n",
       " ),\n",
       " 4 defined in (%4 : int = prim::Constant[value=1](), scope: __module.dense # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1674:0\n",
       " ),\n",
       " 4 defined in (%4 : int = prim::Constant[value=1](), scope: __module.dense # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1674:0\n",
       " )]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ni=list(n.inputs())\n",
    "ni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch._C.IntType"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(ni[4].type())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[32, 128]"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ni[1].type().sizes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a is None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isinstance(ni[1].type(), torch.TensorType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'aten::addmm'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n.kind()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'8'"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n.output().debugName()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n.output().type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'__module.dense'"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n.scopeName()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'prim::GetAttr'"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes[0].kind()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5 defined in (%5 : Tensor = prim::GetAttr[name=\"bias\"](%2)\n",
       " ),\n",
       " input defined in (%self.1 : __torch__.___torch_mangle_3.BertIntermediate, %input : Float(32:128, 128:1) = prim::Param()\n",
       " ),\n",
       " 7 defined in (%7 : Float(128:1, 512:128) = aten::t(%6), scope: __module.dense # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1674:0\n",
       " ),\n",
       " 4 defined in (%4 : int = prim::Constant[value=1](), scope: __module.dense # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1674:0\n",
       " ),\n",
       " 4 defined in (%4 : int = prim::Constant[value=1](), scope: __module.dense # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1674:0\n",
       " )]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(n.inputs())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['addBlock',\n",
       " 'addInput',\n",
       " 'addOutput',\n",
       " 'attributeNames',\n",
       " 'blocks',\n",
       " 'cconv',\n",
       " 'copyAttributes',\n",
       " 'destroy',\n",
       " 'eraseOutput',\n",
       " 'f',\n",
       " 'f_',\n",
       " 'findAllNodes',\n",
       " 'findNode',\n",
       " 'fs',\n",
       " 'fs_',\n",
       " 'g',\n",
       " 'g_',\n",
       " 'gs',\n",
       " 'gs_',\n",
       " 'hasAttribute',\n",
       " 'hasAttributes',\n",
       " 'hasMultipleOutputs',\n",
       " 'hasUses',\n",
       " 'i',\n",
       " 'i_',\n",
       " 'input',\n",
       " 'inputs',\n",
       " 'inputsAt',\n",
       " 'insertAfter',\n",
       " 'insertBefore',\n",
       " 'is',\n",
       " 'isNondeterministic',\n",
       " 'is_',\n",
       " 'kind',\n",
       " 'kindOf',\n",
       " 'moveAfter',\n",
       " 'moveBefore',\n",
       " 'mustBeNone',\n",
       " 'output',\n",
       " 'outputs',\n",
       " 'outputsAt',\n",
       " 'outputsSize',\n",
       " 'pyname',\n",
       " 'pyobj',\n",
       " 'removeAllInputs',\n",
       " 'removeAttribute',\n",
       " 'removeInput',\n",
       " 'replaceAllUsesWith',\n",
       " 'replaceInput',\n",
       " 'replaceInputWith',\n",
       " 's',\n",
       " 's_',\n",
       " 'scalar_args',\n",
       " 'scopeName',\n",
       " 'sourceRange',\n",
       " 'ss',\n",
       " 'ss_',\n",
       " 't',\n",
       " 't_',\n",
       " 'ts',\n",
       " 'ts_',\n",
       " 'z',\n",
       " 'z_',\n",
       " 'zs',\n",
       " 'zs_']"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i for i in dir(n) if not i.startswith('__')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "required keyword attribute 'name' is undefined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-82-e1eb6659b021>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mn\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0ms\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'name'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m: required keyword attribute 'name' is undefined"
     ]
    }
   ],
   "source": [
    "n.s('name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "expected a prim::PythonOp but found a aten::addmm",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-85-71a448463cb4>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mn\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpyobj\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m: expected a prim::PythonOp but found a aten::addmm"
     ]
    }
   ],
   "source": [
    "n.pyobj()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{i:getattr(n, n.kindOf(i))(i) for i in n.attributeNames()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[self.1 defined in (%self.1 : __torch__.BertIntermediate, %input : Float(32:128, 128:1) = prim::Param()\n",
       " )]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(n.inputs())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "%7 : Float(128:1, 512:128) = aten::t(%6), scope: __module.dense # /Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/torch/nn/functional.py:1674:0"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# {i.debugName(): i for i in fc_graph.nodes()}\n",
    "# [[n for i in dir(n) if not i.startswith('__')] for n in fc_graph.nodes()]\n",
    "nodes = list(fc_graph.nodes())\n",
    "# [i for i in dir(n) if not i.startswith('__')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=nodes[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'19'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(fc_graph.outputs()).debugName()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchviz import make_dot, make_dot_from_trace\n",
    "# fc_model.eval()\n",
    "# with torch.onnx.select_model_mode_for_export(fc_model, False):\n",
    "#     fc_viz_trace, _ = torch.jit._get_trace_graph(fc_model, args=(input_states,))\n",
    "# # make_dot(fc_model(input_states), params=dict(fc_model.named_parameters()))\n",
    "# make_dot_from_trace(fc_viz_trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch.autograd.profiler as profiler\n",
    "\n",
    "with profiler.profile() as prof:\n",
    "    with profiler.record_function(\"model_inference\"):\n",
    "        model(inputs)\n",
    "\n",
    "prof.export_chrome_trace(\"trace.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trace.graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# trace.graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# trace.inlined_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# for in_node in trace.graph.inputs():\n",
    "#     print(in_node)\n",
    "# print('start output nodes')\n",
    "# for in_node in trace.inlined_graph.nodes():\n",
    "#     print(in_node)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# from torchviz import make_dot, make_dot_from_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# make_dot(model(inputs), params=dict(model.named_parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "from collections import namedtuple\n",
    "from distutils.version import LooseVersion\n",
    "from graphviz import Digraph\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "Node = namedtuple('Node', ('name', 'inputs', 'attr', 'op'))\n",
    "\n",
    "def resize_graph(dot, size_per_element=0.15, min_size=12):\n",
    "    \"\"\"Resize the graph according to how much content it contains.\n",
    "    Modify the graph in place.\n",
    "    \"\"\"\n",
    "    # Get the approximate number of nodes and edges\n",
    "    num_rows = len(dot.body)\n",
    "    content_size = num_rows * size_per_element\n",
    "    size = max(min_size, content_size)\n",
    "    size_str = str(size) + \",\" + str(size)\n",
    "    dot.graph_attr.update(size=size_str)\n",
    "\n",
    "def replace(name, scope):\n",
    "    return '/'.join([scope[name], name])\n",
    "\n",
    "def parse(graph):\n",
    "    scope = {}\n",
    "    for n in graph.nodes():\n",
    "        inputs = [i.debugName() for i in n.inputs()]\n",
    "        for i in range(1, len(inputs)):\n",
    "            scope[inputs[i]] = n.scopeName()\n",
    "\n",
    "        uname = next(n.outputs()).debugName()\n",
    "        if not n.scopeName():\n",
    "            warnings.warn('{} has empty scope name'.format(n))\n",
    "        scope[uname] = n.scopeName()\n",
    "    scope['0'] = 'input'\n",
    "    print('scope:', scope)\n",
    "    nodes = []\n",
    "    for n in graph.nodes():\n",
    "        attrs = {k: n[k] for k in n.attributeNames()}\n",
    "        attrs = str(attrs).replace(\"'\", ' ')\n",
    "        inputs = [replace(i.debugName(), scope) for i in n.inputs()]\n",
    "        uname = next(n.outputs()).debugName()\n",
    "        nodes.append(Node(**{'name': replace(uname, scope),\n",
    "                             'op': n.kind(),\n",
    "                             'inputs': inputs,\n",
    "                             'attr': attrs}))\n",
    "\n",
    "    for n in graph.inputs():\n",
    "        uname = n.debugName()\n",
    "        if uname not in scope.keys():\n",
    "            scope[uname] = 'unused'\n",
    "        nodes.append(Node(**{'name': replace(uname, scope),\n",
    "                             'op': 'Parameter',\n",
    "                             'inputs': [],\n",
    "                             'attr': str(n.type())}))\n",
    "\n",
    "    return nodes\n",
    "\n",
    "def dot_from_graph(graph):\n",
    "    # # from tensorboardX\n",
    "    # if LooseVersion(torch.__version__) >= LooseVersion(\"0.4.1\"):\n",
    "    #     torch.onnx._optimize_trace(trace, torch._C._onnx.OperatorExportTypes.ONNX_ATEN_FALLBACK)\n",
    "    # elif LooseVersion(torch.__version__) >= LooseVersion(\"0.4\"):\n",
    "    #     torch.onnx._optimize_trace(trace, False)\n",
    "    # else:\n",
    "    #     torch.onnx._optimize_trace(trace)\n",
    "    # graph = trace.inlined_graph\n",
    "    # list_of_nodes = parse(graph)\n",
    "\n",
    "    node_attr = dict(style='filled',\n",
    "                     shape='box',\n",
    "                     align='left',\n",
    "                     fontsize='12',\n",
    "                     ranksep='0.1',\n",
    "                     height='0.2')\n",
    "\n",
    "    dot = Digraph(node_attr=node_attr, graph_attr=dict(size=\"12,12\"))\n",
    "\n",
    "    for node in graph.nodes:\n",
    "        dot.node(node.name, label=node.name.replace('/', '\\n'))\n",
    "        if node.inputs:\n",
    "            for inp in node.inputs:\n",
    "                dot.edge(inp, node.name)\n",
    "\n",
    "    resize_graph(dot)\n",
    "\n",
    "    return dot\n",
    "\n",
    "# make_dot_from_trace(trace)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# from torchprofile.utils import trace\n",
    "# graph = trace(model, inputs)\n",
    "# dot_from_graph(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "from collections import namedtuple\n",
    "from distutils.version import LooseVersion\n",
    "from graphviz import Digraph\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "Node = namedtuple('Node', ('name', 'inputs', 'attr', 'op'))\n",
    "\n",
    "def resize_graph(dot, size_per_element=0.15, min_size=12):\n",
    "    \"\"\"Resize the graph according to how much content it contains.\n",
    "    Modify the graph in place.\n",
    "    \"\"\"\n",
    "    # Get the approximate number of nodes and edges\n",
    "    num_rows = len(dot.body)\n",
    "    content_size = num_rows * size_per_element\n",
    "    size = max(min_size, content_size)\n",
    "    size_str = str(size) + \",\" + str(size)\n",
    "    dot.graph_attr.update(size=size_str)\n",
    "\n",
    "def replace(name, scope):\n",
    "    return '/'.join([scope[name], name])\n",
    "\n",
    "def parse(graph):\n",
    "    scope = {}\n",
    "    for n in graph.nodes():\n",
    "        inputs = [i.debugName() for i in n.inputs()]\n",
    "        for i in range(1, len(inputs)):\n",
    "            scope[inputs[i]] = n.scopeName()\n",
    "\n",
    "        uname = next(n.outputs()).debugName()\n",
    "        if not n.scopeName():\n",
    "            warnings.warn('{} has empty scope name'.format(n))\n",
    "        scope[uname] = n.scopeName()\n",
    "    scope['0'] = 'input'\n",
    "    print('scope:', scope)\n",
    "    nodes = []\n",
    "    for n in graph.nodes():\n",
    "        attrs = {k: n[k] for k in n.attributeNames()}\n",
    "        attrs = str(attrs).replace(\"'\", ' ')\n",
    "        inputs = [replace(i.debugName(), scope) for i in n.inputs()]\n",
    "        uname = next(n.outputs()).debugName()\n",
    "        nodes.append(Node(**{'name': replace(uname, scope),\n",
    "                             'op': n.kind(),\n",
    "                             'inputs': inputs,\n",
    "                             'attr': attrs}))\n",
    "\n",
    "    for n in graph.inputs():\n",
    "        uname = n.debugName()\n",
    "        if uname not in scope.keys():\n",
    "            scope[uname] = 'unused'\n",
    "        nodes.append(Node(**{'name': replace(uname, scope),\n",
    "                             'op': 'Parameter',\n",
    "                             'inputs': [],\n",
    "                             'attr': str(n.type())}))\n",
    "\n",
    "    return nodes\n",
    "\n",
    "def make_dot_from_trace(trace):\n",
    "    \"\"\" Produces graphs of torch.jit.trace outputs\n",
    "    Example:\n",
    "    >>> trace, = torch.jit.trace(model, args=(x,))\n",
    "    >>> dot = make_dot_from_trace(trace)\n",
    "    \"\"\"\n",
    "    # # from tensorboardX\n",
    "    # if LooseVersion(torch.__version__) >= LooseVersion(\"0.4.1\"):\n",
    "    #     torch.onnx._optimize_trace(trace, torch._C._onnx.OperatorExportTypes.ONNX_ATEN_FALLBACK)\n",
    "    # elif LooseVersion(torch.__version__) >= LooseVersion(\"0.4\"):\n",
    "    #     torch.onnx._optimize_trace(trace, False)\n",
    "    # else:\n",
    "    #     torch.onnx._optimize_trace(trace)\n",
    "    graph = trace.inlined_graph\n",
    "    list_of_nodes = parse(graph)\n",
    "\n",
    "    node_attr = dict(style='filled',\n",
    "                     shape='box',\n",
    "                     align='left',\n",
    "                     fontsize='12',\n",
    "                     ranksep='0.1',\n",
    "                     height='0.2')\n",
    "\n",
    "    dot = Digraph(node_attr=node_attr, graph_attr=dict(size=\"12,12\"))\n",
    "\n",
    "    for node in list_of_nodes:\n",
    "        dot.node(node.name, label=node.name.replace('/', '\\n'))\n",
    "        if node.inputs:\n",
    "            for inp in node.inputs:\n",
    "                dot.edge(inp, node.name)\n",
    "\n",
    "    resize_graph(dot)\n",
    "\n",
    "    return dot\n",
    "\n",
    "make_dot_from_trace(trace)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}