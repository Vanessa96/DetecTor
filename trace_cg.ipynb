{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import autoreload\n",
    "# ?autoreload\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import torch\n",
    "from common import sanitize\n",
    "# import torch.nn as nn\n",
    "# torch.nn.modules.module.Module._call_impl = _call_impl2\n",
    "# torch.nn.Module._call_impl=_call_impl2\n",
    "# nn.Module._call_impl=_call_impl2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x125c03130>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import time\n",
    "from transformers import AutoModel\n",
    "from transformers import AutoConfig\n",
    "import torch\n",
    "from torch import nn\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from calibrate_e_ml import get_module_info\n",
    "from calibrate_e_ml import print_info\n",
    "\n",
    "from cg.node import construct_aggregation_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 100\n",
    "bs = 4\n",
    "input_ids = torch.randint(1000, size=(bs, seq_len), dtype=torch.long, device='cpu')\n",
    "cuda_exist = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if cuda_exist else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "221e1cf7f7f3437598b0e061e0a2d906",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=285.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BertEmbeddings 1\n",
      "<class 'dict'> embeddings:BertEmbeddings {'input_ids': torch.Size([4, 100]), 'position_ids': None, 'token_type_ids': torch.Size([4, 100]), 'inputs_embeds': None, 'past_key_values_length': 0}\n",
      "\n",
      "BertSelfAttention 2\n",
      "<class 'tuple'> encoder.layer.0.attention.self:BertSelfAttention [torch.Size([4, 100, 128]), torch.Size([4, 1, 1, 100]), None, None, None, None, False]\n",
      "\n",
      "BertSelfOutput 2\n",
      "<class 'tuple'> encoder.layer.0.attention.output:BertSelfOutput [torch.Size([4, 100, 128]), torch.Size([4, 100, 128])]\n",
      "\n",
      "BertAttention 2\n",
      "<class 'dict'> encoder.layer.0.attention:BertAttention {'output_attentions': False, 'past_key_value': None}\n",
      "<class 'tuple'> encoder.layer.0.attention:BertAttention [torch.Size([4, 100, 128]), torch.Size([4, 1, 1, 100]), None]\n",
      "\n",
      "BertIntermediate 2\n",
      "<class 'tuple'> encoder.layer.0.intermediate:BertIntermediate [torch.Size([4, 100, 128])]\n",
      "\n",
      "BertOutput 2\n",
      "<class 'tuple'> encoder.layer.0.output:BertOutput [torch.Size([4, 100, 512]), torch.Size([4, 100, 128])]\n",
      "\n",
      "BertLayer 2\n",
      "<class 'tuple'> encoder.layer.0:BertLayer [torch.Size([4, 100, 128]), torch.Size([4, 1, 1, 100]), None, None, None, None, False]\n",
      "\n",
      "BertEncoder 1\n",
      "<class 'dict'> encoder:BertEncoder {'attention_mask': torch.Size([4, 1, 1, 100]), 'head_mask': [None, None], 'encoder_hidden_states': None, 'encoder_attention_mask': None, 'past_key_values': None, 'use_cache': False, 'output_attentions': False, 'output_hidden_states': False, 'return_dict': False}\n",
      "<class 'tuple'> encoder:BertEncoder [torch.Size([4, 100, 128])]\n",
      "\n",
      "BertPooler 1\n",
      "<class 'tuple'> pooler:BertPooler [torch.Size([4, 100, 128])]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_name = \"prajjwal1/bert-tiny\"\n",
    "information = get_module_info(model_name, bs, seq_len, device, 'module')\n",
    "print_info(information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "from run_level_exp import get_model_flops_mem_bytes, wrapped_partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sel_attn = information['BertSelfAttention'][0]\n",
    "fn = sel_attn['module']\n",
    "fi = sel_attn['inputs']\n",
    "fi_kwargs = sel_attn['in_kwargs']\n",
    "\n",
    "fn_fwd = fn.forward\n",
    "#  unify fi and fi_kwargs\n",
    "fn_args = inspect.getfullargspec(fn_fwd).args\n",
    "fill_args = dict()\n",
    "fi_args = fn_args[1:1 + len(fi)]\n",
    "ti = []\n",
    "for fi_k, fi_v in zip(fi_args, fi):\n",
    "    if isinstance(fi_v, torch.Tensor):\n",
    "        ti.append(fi_v)\n",
    "    else:\n",
    "        fill_args[fi_k] = fi_v\n",
    "for k, v in fi_kwargs.items():\n",
    "    if isinstance(v, torch.Tensor):\n",
    "        ti.append(v)\n",
    "    else:\n",
    "        fill_args[k] = v\n",
    "# wrap forward into traceable fn (only tensor args)\n",
    "# https://github.com/pytorch/pytorch/issues/14455#issuecomment-445962680\n",
    "fn.forward = wrapped_partial(fn.forward, **fill_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60.5152 6.62158203125\n"
     ]
    }
   ],
   "source": [
    "flops, mem_bytes = get_model_flops_mem_bytes(fn, ti, 'BertEncoder')\n",
    "print(flops/1e6, mem_bytes/1024/1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings 1\n",
      "<class 'tuple'> embeddings:Embeddings [torch.Size([4, 100])]\n",
      "\n",
      "MultiHeadSelfAttention 6\n",
      "<class 'dict'> transformer.layer.0.attention:MultiHeadSelfAttention {'query': torch.Size([4, 100, 768]), 'key': torch.Size([4, 100, 768]), 'value': torch.Size([4, 100, 768]), 'mask': torch.Size([4, 100]), 'head_mask': None, 'output_attentions': False}\n",
      "\n",
      "FFN 6\n",
      "<class 'tuple'> transformer.layer.0.ffn:FFN [torch.Size([4, 100, 768])]\n",
      "\n",
      "TransformerBlock 6\n",
      "<class 'dict'> transformer.layer.0:TransformerBlock {'x': torch.Size([4, 100, 768]), 'attn_mask': torch.Size([4, 100]), 'head_mask': None, 'output_attentions': False}\n",
      "\n",
      "Transformer 1\n",
      "<class 'dict'> transformer:Transformer {'x': torch.Size([4, 100, 768]), 'attn_mask': torch.Size([4, 100]), 'head_mask': [None, None, None, None, None, None], 'output_attentions': False, 'output_hidden_states': False, 'return_dict': False}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_name = \"distilbert-base-uncased\"\n",
    "distilbert_information = get_module_info(model_name, bs, seq_len, device, 'module')\n",
    "print_info(distilbert_information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings,embeddings:Embeddings\n",
      "\n",
      "MultiHeadSelfAttention,transformer.layer.0.attention:MultiHeadSelfAttention\n",
      "\t\tOpNode(id=scores, op=aten::matmul, flops=61440000, mem_bytes=4377600, \n",
      "\t\t\tinputs=[\n",
      "\t\t\t\tDataNode(id=q, dtype=Float, shape=[4, 12, 100, 64])\n",
      "\t\t\t\tDataNode(id=53, dtype=Float, shape=[4, 12, 64, 100])],\n",
      "\t\t\toutputs=[\n",
      "\t\t\t\tDataNode(id=scores, dtype=Float, shape=[4, 12, 100, 100])],)\n",
      "\t\tOpNode(id=input.3, op=aten::softmax, flops=2400000, mem_bytes=5760000, \n",
      "\t\t\tinputs=[\n",
      "\t\t\t\tDataNode(id=input.2, dtype=Float, shape=[4, 12, 100, 100])\n",
      "\t\t\t\tDataNode(id=64, dtype=int)\n",
      "\t\t\t\tDataNode(id=65, dtype=None)],\n",
      "\t\t\toutputs=[\n",
      "\t\t\t\tDataNode(id=input.3, dtype=Float, shape=[4, 12, 100, 100])],)\n",
      "\t\tOpNode(id=x, op=aten::matmul, flops=61440000, mem_bytes=4377600, \n",
      "\t\t\tinputs=[\n",
      "\t\t\t\tDataNode(id=weights, dtype=Float, shape=[4, 12, 100, 100])\n",
      "\t\t\t\tDataNode(id=v, dtype=Float, shape=[4, 12, 100, 64])],\n",
      "\t\t\toutputs=[\n",
      "\t\t\t\tDataNode(id=x, dtype=Float, shape=[4, 12, 100, 64])],)\n",
      "\n",
      "MultiHeadSelfAttention,transformer.layer.1.attention:MultiHeadSelfAttention\n",
      "\t\tOpNode(id=scores, op=aten::matmul, flops=61440000, mem_bytes=4377600, \n",
      "\t\t\tinputs=[\n",
      "\t\t\t\tDataNode(id=q, dtype=Float, shape=[4, 12, 100, 64])\n",
      "\t\t\t\tDataNode(id=53, dtype=Float, shape=[4, 12, 64, 100])],\n",
      "\t\t\toutputs=[\n",
      "\t\t\t\tDataNode(id=scores, dtype=Float, shape=[4, 12, 100, 100])],)\n",
      "\t\tOpNode(id=input.3, op=aten::softmax, flops=2400000, mem_bytes=5760000, \n",
      "\t\t\tinputs=[\n",
      "\t\t\t\tDataNode(id=input.2, dtype=Float, shape=[4, 12, 100, 100])\n",
      "\t\t\t\tDataNode(id=64, dtype=int)\n",
      "\t\t\t\tDataNode(id=65, dtype=None)],\n",
      "\t\t\toutputs=[\n",
      "\t\t\t\tDataNode(id=input.3, dtype=Float, shape=[4, 12, 100, 100])],)\n",
      "\t\tOpNode(id=x, op=aten::matmul, flops=61440000, mem_bytes=4377600, \n",
      "\t\t\tinputs=[\n",
      "\t\t\t\tDataNode(id=weights, dtype=Float, shape=[4, 12, 100, 100])\n",
      "\t\t\t\tDataNode(id=v, dtype=Float, shape=[4, 12, 100, 64])],\n",
      "\t\t\toutputs=[\n",
      "\t\t\t\tDataNode(id=x, dtype=Float, shape=[4, 12, 100, 64])],)\n",
      "\n",
      "MultiHeadSelfAttention,transformer.layer.2.attention:MultiHeadSelfAttention\n",
      "\t\tOpNode(id=scores, op=aten::matmul, flops=61440000, mem_bytes=4377600, \n",
      "\t\t\tinputs=[\n",
      "\t\t\t\tDataNode(id=q, dtype=Float, shape=[4, 12, 100, 64])\n",
      "\t\t\t\tDataNode(id=53, dtype=Float, shape=[4, 12, 64, 100])],\n",
      "\t\t\toutputs=[\n",
      "\t\t\t\tDataNode(id=scores, dtype=Float, shape=[4, 12, 100, 100])],)\n",
      "\t\tOpNode(id=input.3, op=aten::softmax, flops=2400000, mem_bytes=5760000, \n",
      "\t\t\tinputs=[\n",
      "\t\t\t\tDataNode(id=input.2, dtype=Float, shape=[4, 12, 100, 100])\n",
      "\t\t\t\tDataNode(id=64, dtype=int)\n",
      "\t\t\t\tDataNode(id=65, dtype=None)],\n",
      "\t\t\toutputs=[\n",
      "\t\t\t\tDataNode(id=input.3, dtype=Float, shape=[4, 12, 100, 100])],)\n",
      "\t\tOpNode(id=x, op=aten::matmul, flops=61440000, mem_bytes=4377600, \n",
      "\t\t\tinputs=[\n",
      "\t\t\t\tDataNode(id=weights, dtype=Float, shape=[4, 12, 100, 100])\n",
      "\t\t\t\tDataNode(id=v, dtype=Float, shape=[4, 12, 100, 64])],\n",
      "\t\t\toutputs=[\n",
      "\t\t\t\tDataNode(id=x, dtype=Float, shape=[4, 12, 100, 64])],)\n",
      "\n",
      "MultiHeadSelfAttention,transformer.layer.3.attention:MultiHeadSelfAttention\n",
      "\t\tOpNode(id=scores, op=aten::matmul, flops=61440000, mem_bytes=4377600, \n",
      "\t\t\tinputs=[\n",
      "\t\t\t\tDataNode(id=q, dtype=Float, shape=[4, 12, 100, 64])\n",
      "\t\t\t\tDataNode(id=53, dtype=Float, shape=[4, 12, 64, 100])],\n",
      "\t\t\toutputs=[\n",
      "\t\t\t\tDataNode(id=scores, dtype=Float, shape=[4, 12, 100, 100])],)\n",
      "\t\tOpNode(id=input.3, op=aten::softmax, flops=2400000, mem_bytes=5760000, \n",
      "\t\t\tinputs=[\n",
      "\t\t\t\tDataNode(id=input.2, dtype=Float, shape=[4, 12, 100, 100])\n",
      "\t\t\t\tDataNode(id=64, dtype=int)\n",
      "\t\t\t\tDataNode(id=65, dtype=None)],\n",
      "\t\t\toutputs=[\n",
      "\t\t\t\tDataNode(id=input.3, dtype=Float, shape=[4, 12, 100, 100])],)\n",
      "\t\tOpNode(id=x, op=aten::matmul, flops=61440000, mem_bytes=4377600, \n",
      "\t\t\tinputs=[\n",
      "\t\t\t\tDataNode(id=weights, dtype=Float, shape=[4, 12, 100, 100])\n",
      "\t\t\t\tDataNode(id=v, dtype=Float, shape=[4, 12, 100, 64])],\n",
      "\t\t\toutputs=[\n",
      "\t\t\t\tDataNode(id=x, dtype=Float, shape=[4, 12, 100, 64])],)\n",
      "\n",
      "MultiHeadSelfAttention,transformer.layer.4.attention:MultiHeadSelfAttention\n",
      "\t\tOpNode(id=scores, op=aten::matmul, flops=61440000, mem_bytes=4377600, \n",
      "\t\t\tinputs=[\n",
      "\t\t\t\tDataNode(id=q, dtype=Float, shape=[4, 12, 100, 64])\n",
      "\t\t\t\tDataNode(id=53, dtype=Float, shape=[4, 12, 64, 100])],\n",
      "\t\t\toutputs=[\n",
      "\t\t\t\tDataNode(id=scores, dtype=Float, shape=[4, 12, 100, 100])],)\n",
      "\t\tOpNode(id=input.3, op=aten::softmax, flops=2400000, mem_bytes=5760000, \n",
      "\t\t\tinputs=[\n",
      "\t\t\t\tDataNode(id=input.2, dtype=Float, shape=[4, 12, 100, 100])\n",
      "\t\t\t\tDataNode(id=64, dtype=int)\n",
      "\t\t\t\tDataNode(id=65, dtype=None)],\n",
      "\t\t\toutputs=[\n",
      "\t\t\t\tDataNode(id=input.3, dtype=Float, shape=[4, 12, 100, 100])],)\n",
      "\t\tOpNode(id=x, op=aten::matmul, flops=61440000, mem_bytes=4377600, \n",
      "\t\t\tinputs=[\n",
      "\t\t\t\tDataNode(id=weights, dtype=Float, shape=[4, 12, 100, 100])\n",
      "\t\t\t\tDataNode(id=v, dtype=Float, shape=[4, 12, 100, 64])],\n",
      "\t\t\toutputs=[\n",
      "\t\t\t\tDataNode(id=x, dtype=Float, shape=[4, 12, 100, 64])],)\n",
      "\n",
      "MultiHeadSelfAttention,transformer.layer.5.attention:MultiHeadSelfAttention\n",
      "\t\tOpNode(id=scores, op=aten::matmul, flops=61440000, mem_bytes=4377600, \n",
      "\t\t\tinputs=[\n",
      "\t\t\t\tDataNode(id=q, dtype=Float, shape=[4, 12, 100, 64])\n",
      "\t\t\t\tDataNode(id=53, dtype=Float, shape=[4, 12, 64, 100])],\n",
      "\t\t\toutputs=[\n",
      "\t\t\t\tDataNode(id=scores, dtype=Float, shape=[4, 12, 100, 100])],)\n",
      "\t\tOpNode(id=input.3, op=aten::softmax, flops=2400000, mem_bytes=5760000, \n",
      "\t\t\tinputs=[\n",
      "\t\t\t\tDataNode(id=input.2, dtype=Float, shape=[4, 12, 100, 100])\n",
      "\t\t\t\tDataNode(id=64, dtype=int)\n",
      "\t\t\t\tDataNode(id=65, dtype=None)],\n",
      "\t\t\toutputs=[\n",
      "\t\t\t\tDataNode(id=input.3, dtype=Float, shape=[4, 12, 100, 100])],)\n",
      "\t\tOpNode(id=x, op=aten::matmul, flops=61440000, mem_bytes=4377600, \n",
      "\t\t\tinputs=[\n",
      "\t\t\t\tDataNode(id=weights, dtype=Float, shape=[4, 12, 100, 100])\n",
      "\t\t\t\tDataNode(id=v, dtype=Float, shape=[4, 12, 100, 64])],\n",
      "\t\t\toutputs=[\n",
      "\t\t\t\tDataNode(id=x, dtype=Float, shape=[4, 12, 100, 64])],)\n",
      "\n",
      "FFN,transformer.layer.0.ffn:FFN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/modeling_utils.py:1759: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert all(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FFN,transformer.layer.1.ffn:FFN\n",
      "\n",
      "FFN,transformer.layer.2.ffn:FFN\n",
      "\n",
      "FFN,transformer.layer.3.ffn:FFN\n",
      "\n",
      "FFN,transformer.layer.4.ffn:FFN\n",
      "\n",
      "FFN,transformer.layer.5.ffn:FFN\n",
      "\n",
      "TransformerBlock,transformer.layer.0:TransformerBlock\n",
      "\t\tOpNode(id=input.2, op=aten::softmax, flops=2400000, mem_bytes=5760000, scope=attention, \n",
      "\t\t\tinputs=[\n",
      "\t\t\t\tDataNode(id=input.1, dtype=Float, shape=[4, 12, 100, 100])\n",
      "\t\t\t\tDataNode(id=26, dtype=int)\n",
      "\t\t\t\tDataNode(id=19, dtype=None)],\n",
      "\t\t\toutputs=[\n",
      "\t\t\t\tDataNode(id=input.2, dtype=Float, shape=[4, 12, 100, 100])],)\n",
      "\n",
      "TransformerBlock,transformer.layer.1:TransformerBlock\n",
      "\t\tOpNode(id=input.2, op=aten::softmax, flops=2400000, mem_bytes=5760000, scope=attention, \n",
      "\t\t\tinputs=[\n",
      "\t\t\t\tDataNode(id=input.1, dtype=Float, shape=[4, 12, 100, 100])\n",
      "\t\t\t\tDataNode(id=26, dtype=int)\n",
      "\t\t\t\tDataNode(id=19, dtype=None)],\n",
      "\t\t\toutputs=[\n",
      "\t\t\t\tDataNode(id=input.2, dtype=Float, shape=[4, 12, 100, 100])],)\n",
      "\n",
      "TransformerBlock,transformer.layer.2:TransformerBlock\n",
      "\t\tOpNode(id=input.2, op=aten::softmax, flops=2400000, mem_bytes=5760000, scope=attention, \n",
      "\t\t\tinputs=[\n",
      "\t\t\t\tDataNode(id=input.1, dtype=Float, shape=[4, 12, 100, 100])\n",
      "\t\t\t\tDataNode(id=26, dtype=int)\n",
      "\t\t\t\tDataNode(id=19, dtype=None)],\n",
      "\t\t\toutputs=[\n",
      "\t\t\t\tDataNode(id=input.2, dtype=Float, shape=[4, 12, 100, 100])],)\n",
      "\n",
      "TransformerBlock,transformer.layer.3:TransformerBlock\n",
      "\t\tOpNode(id=input.2, op=aten::softmax, flops=2400000, mem_bytes=5760000, scope=attention, \n",
      "\t\t\tinputs=[\n",
      "\t\t\t\tDataNode(id=input.1, dtype=Float, shape=[4, 12, 100, 100])\n",
      "\t\t\t\tDataNode(id=26, dtype=int)\n",
      "\t\t\t\tDataNode(id=19, dtype=None)],\n",
      "\t\t\toutputs=[\n",
      "\t\t\t\tDataNode(id=input.2, dtype=Float, shape=[4, 12, 100, 100])],)\n",
      "\n",
      "TransformerBlock,transformer.layer.4:TransformerBlock\n",
      "\t\tOpNode(id=input.2, op=aten::softmax, flops=2400000, mem_bytes=5760000, scope=attention, \n",
      "\t\t\tinputs=[\n",
      "\t\t\t\tDataNode(id=input.1, dtype=Float, shape=[4, 12, 100, 100])\n",
      "\t\t\t\tDataNode(id=26, dtype=int)\n",
      "\t\t\t\tDataNode(id=19, dtype=None)],\n",
      "\t\t\toutputs=[\n",
      "\t\t\t\tDataNode(id=input.2, dtype=Float, shape=[4, 12, 100, 100])],)\n",
      "\n",
      "TransformerBlock,transformer.layer.5:TransformerBlock\n",
      "\t\tOpNode(id=input.2, op=aten::softmax, flops=2400000, mem_bytes=5760000, scope=attention, \n",
      "\t\t\tinputs=[\n",
      "\t\t\t\tDataNode(id=input.1, dtype=Float, shape=[4, 12, 100, 100])\n",
      "\t\t\t\tDataNode(id=26, dtype=int)\n",
      "\t\t\t\tDataNode(id=19, dtype=None)],\n",
      "\t\t\toutputs=[\n",
      "\t\t\t\tDataNode(id=input.2, dtype=Float, shape=[4, 12, 100, 100])],)\n",
      "\n",
      "Transformer,transformer:Transformer\n",
      "\t\tOpNode(id=input.2, op=aten::softmax, flops=2400000, mem_bytes=5760000, scope=layer.0.attention, \n",
      "\t\t\tinputs=[\n",
      "\t\t\t\tDataNode(id=input.1, dtype=Float, shape=[4, 12, 100, 100])\n",
      "\t\t\t\tDataNode(id=26, dtype=int)\n",
      "\t\t\t\tDataNode(id=33, dtype=None)],\n",
      "\t\t\toutputs=[\n",
      "\t\t\t\tDataNode(id=input.2, dtype=Float, shape=[4, 12, 100, 100])],)\n",
      "\t\tOpNode(id=input.10, op=aten::softmax, flops=2400000, mem_bytes=5760000, scope=layer.1.attention, \n",
      "\t\t\tinputs=[\n",
      "\t\t\t\tDataNode(id=input.9, dtype=Float, shape=[4, 12, 100, 100])\n",
      "\t\t\t\tDataNode(id=119, dtype=int)\n",
      "\t\t\t\tDataNode(id=126, dtype=None)],\n",
      "\t\t\toutputs=[\n",
      "\t\t\t\tDataNode(id=input.10, dtype=Float, shape=[4, 12, 100, 100])],)\n",
      "\t\tOpNode(id=input.18, op=aten::softmax, flops=2400000, mem_bytes=5760000, scope=layer.2.attention, \n",
      "\t\t\tinputs=[\n",
      "\t\t\t\tDataNode(id=input.17, dtype=Float, shape=[4, 12, 100, 100])\n",
      "\t\t\t\tDataNode(id=212, dtype=int)\n",
      "\t\t\t\tDataNode(id=219, dtype=None)],\n",
      "\t\t\toutputs=[\n",
      "\t\t\t\tDataNode(id=input.18, dtype=Float, shape=[4, 12, 100, 100])],)\n",
      "\t\tOpNode(id=input.26, op=aten::softmax, flops=2400000, mem_bytes=5760000, scope=layer.3.attention, \n",
      "\t\t\tinputs=[\n",
      "\t\t\t\tDataNode(id=input.25, dtype=Float, shape=[4, 12, 100, 100])\n",
      "\t\t\t\tDataNode(id=305, dtype=int)\n",
      "\t\t\t\tDataNode(id=312, dtype=None)],\n",
      "\t\t\toutputs=[\n",
      "\t\t\t\tDataNode(id=input.26, dtype=Float, shape=[4, 12, 100, 100])],)\n",
      "\t\tOpNode(id=input.34, op=aten::softmax, flops=2400000, mem_bytes=5760000, scope=layer.4.attention, \n",
      "\t\t\tinputs=[\n",
      "\t\t\t\tDataNode(id=input.33, dtype=Float, shape=[4, 12, 100, 100])\n",
      "\t\t\t\tDataNode(id=398, dtype=int)\n",
      "\t\t\t\tDataNode(id=405, dtype=None)],\n",
      "\t\t\toutputs=[\n",
      "\t\t\t\tDataNode(id=input.34, dtype=Float, shape=[4, 12, 100, 100])],)\n",
      "\t\tOpNode(id=input.42, op=aten::softmax, flops=2400000, mem_bytes=5760000, scope=layer.5.attention, \n",
      "\t\t\tinputs=[\n",
      "\t\t\t\tDataNode(id=input.41, dtype=Float, shape=[4, 12, 100, 100])\n",
      "\t\t\t\tDataNode(id=491, dtype=int)\n",
      "\t\t\t\tDataNode(id=498, dtype=None)],\n",
      "\t\t\toutputs=[\n",
      "\t\t\t\tDataNode(id=input.42, dtype=Float, shape=[4, 12, 100, 100])],)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "get_non_parametric_ml_op(distilbert_information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertEmbeddings 1\n",
      "<class 'dict'> embeddings:BertEmbeddings {'input_ids': torch.Size([4, 100]), 'position_ids': None, 'token_type_ids': torch.Size([4, 100]), 'inputs_embeds': None, 'past_key_values_length': 0}\n",
      "\n",
      "BertSelfAttention 12\n",
      "<class 'tuple'> encoder.layer.0.attention.self:BertSelfAttention [torch.Size([4, 100, 768]), torch.Size([4, 1, 1, 100]), None, None, None, None, False]\n",
      "\n",
      "BertSelfOutput 12\n",
      "<class 'tuple'> encoder.layer.0.attention.output:BertSelfOutput [torch.Size([4, 100, 768]), torch.Size([4, 100, 768])]\n",
      "\n",
      "BertAttention 12\n",
      "<class 'dict'> encoder.layer.0.attention:BertAttention {'output_attentions': False, 'past_key_value': None}\n",
      "<class 'tuple'> encoder.layer.0.attention:BertAttention [torch.Size([4, 100, 768]), torch.Size([4, 1, 1, 100]), None]\n",
      "\n",
      "BertIntermediate 12\n",
      "<class 'tuple'> encoder.layer.0.intermediate:BertIntermediate [torch.Size([4, 100, 768])]\n",
      "\n",
      "BertOutput 12\n",
      "<class 'tuple'> encoder.layer.0.output:BertOutput [torch.Size([4, 100, 3072]), torch.Size([4, 100, 768])]\n",
      "\n",
      "BertLayer 12\n",
      "<class 'tuple'> encoder.layer.0:BertLayer [torch.Size([4, 100, 768]), torch.Size([4, 1, 1, 100]), None, None, None, None, False]\n",
      "\n",
      "BertEncoder 1\n",
      "<class 'dict'> encoder:BertEncoder {'attention_mask': torch.Size([4, 1, 1, 100]), 'head_mask': [None, None, None, None, None, None, None, None, None, None, None, None], 'encoder_hidden_states': None, 'encoder_attention_mask': None, 'past_key_values': None, 'use_cache': False, 'output_attentions': False, 'output_hidden_states': False, 'return_dict': False}\n",
      "<class 'tuple'> encoder:BertEncoder [torch.Size([4, 100, 768])]\n",
      "\n",
      "BertPooler 1\n",
      "<class 'tuple'> pooler:BertPooler [torch.Size([4, 100, 768])]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_name = \"bert-base-uncased\"\n",
    "bert_information = get_module_info(model_name, bs, seq_len, device, 'module')\n",
    "print_info(bert_information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tOpNode(id=attention_scores.1, op=aten::matmul, flops=61440000, mem_bytes=4377600, \n",
      "\t\t\tinputs=[\n",
      "\t\t\t\tDataNode(id=query_layer, dtype=Float, shape=[4, 12, 100, 64])\n",
      "\t\t\t\tDataNode(id=66, dtype=Float, shape=[4, 12, 64, 100])],\n",
      "\t\t\toutputs=[\n",
      "\t\t\t\tDataNode(id=attention_scores.1, dtype=Float, shape=[4, 12, 100, 100])],)\n",
      "\t\tOpNode(id=input, op=aten::softmax, flops=2400000, mem_bytes=5760000, \n",
      "\t\t\tinputs=[\n",
      "\t\t\t\tDataNode(id=input.2, dtype=Float, shape=[4, 12, 100, 100])\n",
      "\t\t\t\tDataNode(id=72, dtype=int)\n",
      "\t\t\t\tDataNode(id=73, dtype=None)],\n",
      "\t\t\toutputs=[\n",
      "\t\t\t\tDataNode(id=input, dtype=Float, shape=[4, 12, 100, 100])],)\n",
      "\t\tOpNode(id=context_layer.1, op=aten::matmul, flops=61440000, mem_bytes=4377600, \n",
      "\t\t\tinputs=[\n",
      "\t\t\t\tDataNode(id=attention_probs, dtype=Float, shape=[4, 12, 100, 100])\n",
      "\t\t\t\tDataNode(id=value_layer, dtype=Float, shape=[4, 12, 100, 64])],\n",
      "\t\t\toutputs=[\n",
      "\t\t\t\tDataNode(id=context_layer.1, dtype=Float, shape=[4, 12, 100, 64])],)\n"
     ]
    }
   ],
   "source": [
    "get_non_parametric_ml_op(bert_information['BertSelfAttention'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobertaEmbeddings 1\n",
      "<class 'dict'> embeddings:RobertaEmbeddings {'input_ids': torch.Size([4, 100]), 'position_ids': None, 'token_type_ids': torch.Size([4, 100]), 'inputs_embeds': None, 'past_key_values_length': 0}\n",
      "\n",
      "RobertaSelfAttention 12\n",
      "<class 'tuple'> encoder.layer.0.attention.self:RobertaSelfAttention [torch.Size([4, 100, 768]), torch.Size([4, 1, 1, 100]), None, None, None, None, False]\n",
      "\n",
      "RobertaSelfOutput 12\n",
      "<class 'tuple'> encoder.layer.0.attention.output:RobertaSelfOutput [torch.Size([4, 100, 768]), torch.Size([4, 100, 768])]\n",
      "\n",
      "RobertaAttention 12\n",
      "<class 'dict'> encoder.layer.0.attention:RobertaAttention {'output_attentions': False, 'past_key_value': None}\n",
      "<class 'tuple'> encoder.layer.0.attention:RobertaAttention [torch.Size([4, 100, 768]), torch.Size([4, 1, 1, 100]), None]\n",
      "\n",
      "RobertaIntermediate 12\n",
      "<class 'tuple'> encoder.layer.0.intermediate:RobertaIntermediate [torch.Size([4, 100, 768])]\n",
      "\n",
      "RobertaOutput 12\n",
      "<class 'tuple'> encoder.layer.0.output:RobertaOutput [torch.Size([4, 100, 3072]), torch.Size([4, 100, 768])]\n",
      "\n",
      "RobertaLayer 12\n",
      "<class 'tuple'> encoder.layer.0:RobertaLayer [torch.Size([4, 100, 768]), torch.Size([4, 1, 1, 100]), None, None, None, None, False]\n",
      "\n",
      "RobertaEncoder 1\n",
      "<class 'dict'> encoder:RobertaEncoder {'attention_mask': torch.Size([4, 1, 1, 100]), 'head_mask': [None, None, None, None, None, None, None, None, None, None, None, None], 'encoder_hidden_states': None, 'encoder_attention_mask': None, 'past_key_values': None, 'use_cache': False, 'output_attentions': False, 'output_hidden_states': False, 'return_dict': False}\n",
      "<class 'tuple'> encoder:RobertaEncoder [torch.Size([4, 100, 768])]\n",
      "\n",
      "RobertaPooler 1\n",
      "<class 'tuple'> pooler:RobertaPooler [torch.Size([4, 100, 768])]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_name = \"roberta-base\"\n",
    "roberta_information = get_module_info(model_name, bs, seq_len, device, 'module')\n",
    "print_info(roberta_information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tOpNode(id=attention_scores.1, op=aten::matmul, flops=61440000, mem_bytes=4377600, \n",
      "\t\t\tinputs=[\n",
      "\t\t\t\tDataNode(id=query_layer, dtype=Float, shape=[4, 12, 100, 64])\n",
      "\t\t\t\tDataNode(id=66, dtype=Float, shape=[4, 12, 64, 100])],\n",
      "\t\t\toutputs=[\n",
      "\t\t\t\tDataNode(id=attention_scores.1, dtype=Float, shape=[4, 12, 100, 100])],)\n",
      "\t\tOpNode(id=input, op=aten::softmax, flops=2400000, mem_bytes=5760000, \n",
      "\t\t\tinputs=[\n",
      "\t\t\t\tDataNode(id=input.2, dtype=Float, shape=[4, 12, 100, 100])\n",
      "\t\t\t\tDataNode(id=72, dtype=int)\n",
      "\t\t\t\tDataNode(id=73, dtype=None)],\n",
      "\t\t\toutputs=[\n",
      "\t\t\t\tDataNode(id=input, dtype=Float, shape=[4, 12, 100, 100])],)\n",
      "\t\tOpNode(id=context_layer.1, op=aten::matmul, flops=61440000, mem_bytes=4377600, \n",
      "\t\t\tinputs=[\n",
      "\t\t\t\tDataNode(id=attention_probs, dtype=Float, shape=[4, 12, 100, 100])\n",
      "\t\t\t\tDataNode(id=value_layer, dtype=Float, shape=[4, 12, 100, 64])],\n",
      "\t\t\toutputs=[\n",
      "\t\t\t\tDataNode(id=context_layer.1, dtype=Float, shape=[4, 12, 100, 64])],)\n"
     ]
    }
   ],
   "source": [
    "get_non_parametric_ml_op(roberta_information['RobertaSelfAttention'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NoNorm 193\n",
      "<class 'tuple'> embeddings.LayerNorm:NoNorm [torch.Size([4, 100, 512])]\n",
      "\n",
      "MobileBertEmbeddings 1\n",
      "<class 'dict'> embeddings:MobileBertEmbeddings {'input_ids': torch.Size([4, 100]), 'position_ids': None, 'token_type_ids': torch.Size([4, 100]), 'inputs_embeds': None}\n",
      "\n",
      "BottleneckLayer 48\n",
      "<class 'tuple'> encoder.layer.0.bottleneck.input:BottleneckLayer [torch.Size([4, 100, 512])]\n",
      "\n",
      "Bottleneck 24\n",
      "<class 'tuple'> encoder.layer.0.bottleneck:Bottleneck [torch.Size([4, 100, 512])]\n",
      "\n",
      "MobileBertSelfAttention 24\n",
      "<class 'tuple'> encoder.layer.0.attention.self:MobileBertSelfAttention [torch.Size([4, 100, 128]), torch.Size([4, 100, 128]), torch.Size([4, 100, 512]), torch.Size([4, 1, 1, 100]), None, False]\n",
      "\n",
      "MobileBertSelfOutput 24\n",
      "<class 'tuple'> encoder.layer.0.attention.output:MobileBertSelfOutput [torch.Size([4, 100, 128]), torch.Size([4, 100, 128])]\n",
      "\n",
      "MobileBertAttention 24\n",
      "<class 'dict'> encoder.layer.0.attention:MobileBertAttention {'output_attentions': False}\n",
      "<class 'tuple'> encoder.layer.0.attention:MobileBertAttention [torch.Size([4, 100, 128]), torch.Size([4, 100, 128]), torch.Size([4, 100, 512]), torch.Size([4, 100, 128]), torch.Size([4, 1, 1, 100]), None]\n",
      "\n",
      "MobileBertIntermediate 96\n",
      "<class 'tuple'> encoder.layer.0.ffn.0.intermediate:MobileBertIntermediate [torch.Size([4, 100, 128])]\n",
      "\n",
      "FFNOutput 72\n",
      "<class 'tuple'> encoder.layer.0.ffn.0.output:FFNOutput [torch.Size([4, 100, 512]), torch.Size([4, 100, 128])]\n",
      "\n",
      "FFNLayer 72\n",
      "<class 'tuple'> encoder.layer.0.ffn.0:FFNLayer [torch.Size([4, 100, 128])]\n",
      "\n",
      "OutputBottleneck 24\n",
      "<class 'tuple'> encoder.layer.0.output.bottleneck:OutputBottleneck [torch.Size([4, 100, 128]), torch.Size([4, 100, 512])]\n",
      "\n",
      "MobileBertOutput 24\n",
      "<class 'tuple'> encoder.layer.0.output:MobileBertOutput [torch.Size([4, 100, 512]), torch.Size([4, 100, 128]), torch.Size([4, 100, 512])]\n",
      "\n",
      "MobileBertLayer 24\n",
      "<class 'tuple'> encoder.layer.0:MobileBertLayer [torch.Size([4, 100, 512]), torch.Size([4, 1, 1, 100]), None, False]\n",
      "\n",
      "MobileBertEncoder 1\n",
      "<class 'dict'> encoder:MobileBertEncoder {'attention_mask': torch.Size([4, 1, 1, 100]), 'head_mask': [None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None], 'output_attentions': False, 'output_hidden_states': False, 'return_dict': False}\n",
      "<class 'tuple'> encoder:MobileBertEncoder [torch.Size([4, 100, 512])]\n",
      "\n",
      "MobileBertPooler 1\n",
      "<class 'tuple'> pooler:MobileBertPooler [torch.Size([4, 100, 512])]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# for k, info in information.items():\n",
    "#     for infoi in info:\n",
    "#         ii = infoi['inputs']\n",
    "#         if isinstance(ii, dict):\n",
    "#             print(type(ii), infoi['name'],  {iik:v.shape if isinstance(v, torch.Tensor) else v for iik, v in ii.items()})\n",
    "#         elif isinstance(ii, tuple):\n",
    "#             print(type(ii), infoi['name'],  [v.shape if isinstance(v, torch.Tensor) else v for v in ii])\n",
    "#         else:\n",
    "#             print(type(ii), 'todo', k, infoi)\n",
    "#         print()\n",
    "model_name = \"google/mobilebert-uncased\"\n",
    "mobilebert_information = get_module_info(model_name, bs, seq_len, device, 'module')\n",
    "print_info(mobilebert_information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tOpNode(id=attention_scores.1, op=aten::matmul, flops=10240000, mem_bytes=1049600, \n",
      "\t\t\tinputs=[\n",
      "\t\t\t\tDataNode(id=query_layer, dtype=Float, shape=[4, 4, 100, 32])\n",
      "\t\t\t\tDataNode(id=68, dtype=Float, shape=[4, 4, 32, 100])],\n",
      "\t\t\toutputs=[\n",
      "\t\t\t\tDataNode(id=attention_scores.1, dtype=Float, shape=[4, 4, 100, 100])],)\n",
      "\t\tOpNode(id=input, op=aten::softmax, flops=800000, mem_bytes=1920000, \n",
      "\t\t\tinputs=[\n",
      "\t\t\t\tDataNode(id=input.4, dtype=Float, shape=[4, 4, 100, 100])\n",
      "\t\t\t\tDataNode(id=74, dtype=int)\n",
      "\t\t\t\tDataNode(id=75, dtype=None)],\n",
      "\t\t\toutputs=[\n",
      "\t\t\t\tDataNode(id=input, dtype=Float, shape=[4, 4, 100, 100])],)\n",
      "\t\tOpNode(id=context_layer.1, op=aten::matmul, flops=10240000, mem_bytes=1049600, \n",
      "\t\t\tinputs=[\n",
      "\t\t\t\tDataNode(id=attention_probs, dtype=Float, shape=[4, 4, 100, 100])\n",
      "\t\t\t\tDataNode(id=value_layer, dtype=Float, shape=[4, 4, 100, 32])],\n",
      "\t\t\toutputs=[\n",
      "\t\t\t\tDataNode(id=context_layer.1, dtype=Float, shape=[4, 4, 100, 32])],)\n"
     ]
    }
   ],
   "source": [
    "get_non_parametric_ml_op(mobilebert_information['MobileBertSelfAttention'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertEmbeddings 1\n",
      "<class 'dict'> embeddings:BertEmbeddings {'input_ids': torch.Size([4, 100]), 'position_ids': None, 'token_type_ids': torch.Size([4, 100]), 'inputs_embeds': None, 'past_key_values_length': 0}\n",
      "\n",
      "BertSelfAttention 2\n",
      "<class 'tuple'> encoder.layer.0.attention.self:BertSelfAttention [torch.Size([4, 100, 128]), torch.Size([4, 1, 1, 100]), None, None, None, None, False]\n",
      "\n",
      "BertSelfOutput 2\n",
      "<class 'tuple'> encoder.layer.0.attention.output:BertSelfOutput [torch.Size([4, 100, 128]), torch.Size([4, 100, 128])]\n",
      "\n",
      "BertAttention 2\n",
      "<class 'dict'> encoder.layer.0.attention:BertAttention {'output_attentions': False, 'past_key_value': None}\n",
      "<class 'tuple'> encoder.layer.0.attention:BertAttention [torch.Size([4, 100, 128]), torch.Size([4, 1, 1, 100]), None]\n",
      "\n",
      "BertIntermediate 2\n",
      "<class 'tuple'> encoder.layer.0.intermediate:BertIntermediate [torch.Size([4, 100, 128])]\n",
      "\n",
      "BertOutput 2\n",
      "<class 'tuple'> encoder.layer.0.output:BertOutput [torch.Size([4, 100, 512]), torch.Size([4, 100, 128])]\n",
      "\n",
      "BertLayer 2\n",
      "<class 'tuple'> encoder.layer.0:BertLayer [torch.Size([4, 100, 128]), torch.Size([4, 1, 1, 100]), None, None, None, None, False]\n",
      "\n",
      "BertEncoder 1\n",
      "<class 'dict'> encoder:BertEncoder {'attention_mask': torch.Size([4, 1, 1, 100]), 'head_mask': [None, None], 'encoder_hidden_states': None, 'encoder_attention_mask': None, 'past_key_values': None, 'use_cache': False, 'output_attentions': False, 'output_hidden_states': False, 'return_dict': False}\n",
      "<class 'tuple'> encoder:BertEncoder [torch.Size([4, 100, 128])]\n",
      "\n",
      "BertPooler 1\n",
      "<class 'tuple'> pooler:BertPooler [torch.Size([4, 100, 128])]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_name = \"prajjwal1/bert-tiny\"\n",
    "information = get_module_info(model_name, bs, seq_len, device, 'module')\n",
    "print_info(information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from transformers import AutoConfig\n",
    "from transformers import AutoModel\n",
    "from transformers import modeling_utils\n",
    "\n",
    "def is_ml_operation(module):\n",
    "    \"\"\"\n",
    "    This function checks if any given module is of a type that\n",
    "    we want to analyse for E_ML operations\n",
    "    \"\"\"\n",
    "\n",
    "    e_ml_operations = {nn.Linear, nn.LayerNorm, nn.Embedding, nn.BatchNorm1d,\n",
    "                       nn.Conv1d, nn.MaxPool1d, nn.AvgPool1d, nn.LSTM, nn.Tanh,\n",
    "                       modeling_utils.Conv1D}\n",
    "\n",
    "    for e_ml_op in e_ml_operations:\n",
    "        if isinstance(module, e_ml_op):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "config = AutoConfig.from_pretrained(model_name)\n",
    "config.torchscript = True\n",
    "model=AutoModel.from_config(config).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"gradient_checkpointing\": false,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 128,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 512,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 2,\n",
       "  \"num_hidden_layers\": 2,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"torchscript\": true,\n",
       "  \"transformers_version\": \"4.2.2\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 30522\n",
       "}"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings:BertEmbeddings\n",
      "embeddings.dropout:Dropout\n",
      "encoder:BertEncoder\n",
      "encoder.layer:ModuleList\n",
      "encoder.layer.0:BertLayer\n",
      "encoder.layer.0.attention:BertAttention\n",
      "encoder.layer.0.attention.self:BertSelfAttention\n",
      "encoder.layer.0.attention.self.dropout:Dropout\n",
      "encoder.layer.0.attention.output:BertSelfOutput\n",
      "encoder.layer.0.attention.output.dropout:Dropout\n",
      "encoder.layer.0.intermediate:BertIntermediate\n",
      "encoder.layer.0.output:BertOutput\n",
      "encoder.layer.0.output.dropout:Dropout\n",
      "encoder.layer.1:BertLayer\n",
      "encoder.layer.1.attention:BertAttention\n",
      "encoder.layer.1.attention.self:BertSelfAttention\n",
      "encoder.layer.1.attention.self.dropout:Dropout\n",
      "encoder.layer.1.attention.output:BertSelfOutput\n",
      "encoder.layer.1.attention.output.dropout:Dropout\n",
      "encoder.layer.1.intermediate:BertIntermediate\n",
      "encoder.layer.1.output:BertOutput\n",
      "encoder.layer.1.output.dropout:Dropout\n",
      "pooler:BertPooler\n"
     ]
    }
   ],
   "source": [
    "for name, module in model.named_modules():\n",
    "    if not name:\n",
    "        continue\n",
    "    if not is_ml_operation(module):\n",
    "        print(f'{name}:{module.__class__.__name__}')\n",
    "#         print(f'\\t{module}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_non_parametric_ml_ops(model, input_ids):\n",
    "    non_param_scopes = {name for name, module in model.named_modules() if not is_ml_operation(module)}\n",
    "    trace = torch.jit.trace(model, input_ids)\n",
    "    trace_graph = trace.inlined_graph\n",
    "    graph, op_data_types = construct_aggregation_graph(trace_graph, fname)\n",
    "    sigs = set()\n",
    "    for node in graph.nodes:\n",
    "        if node.op == 'aten::softmax':\n",
    "            sig = f'op=softmax,flops={node.flops},mem_bytes={node.mem_bytes}'\n",
    "            if sig not in sigs:\n",
    "                print(node)\n",
    "                sigs.add(sig)\n",
    "        if node.op == 'aten::matmul' and node.scope in non_param_scopes:\n",
    "            sig = f'op=matmul,flops={node.flops},mem_bytes={node.mem_bytes}'\n",
    "            if sig not in sigs:\n",
    "                print(node)\n",
    "                sigs.add(sig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cg.node import construct_aggregation_graph\n",
    "from calibrate_e_ml import get_non_parametric_ml_ops\n",
    "model_name = \"prajjwal1/bert-tiny\"\n",
    "# \"google/mobilebert-uncased\" \"bert-base-uncased\" \"distilbert-base-uncased\" \"roberta-base\" \"prajjwal1/bert-tiny\"\n",
    "config = AutoConfig.from_pretrained(model_name)\n",
    "config.torchscript = True\n",
    "model=AutoModel.from_config(config).eval()\n",
    "np_info = get_non_parametric_ml_ops(model, input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matmul 4\n",
      "<class 'tuple'> encoder.layer.0.attention.self [torch.Size([4, 2, 100, 64]), torch.Size([4, 2, 64, 100])]\n",
      "\n",
      "softmax 2\n",
      "<class 'tuple'> encoder.layer.0.attention.self [torch.Size([4, 2, 100, 100])]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_info(np_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "tensor(0)\n",
      "1\n",
      "6\n",
      "None\n",
      "cpu\n",
      "False\n",
      "4\n",
      "None\n",
      "cpu\n",
      "False\n",
      "0\n",
      "0\n",
      "9223372036854775807\n",
      "1\n",
      "1\n",
      "2\n",
      "3\n",
      "0\n",
      "9223372036854775807\n",
      "1\n",
      "6\n",
      "False\n",
      "False\n",
      "None\n",
      "1.0\n",
      "1\n",
      "tensor(-10000., dtype=torch.float64)\n",
      "0.1\n",
      "128\n",
      "1e-12\n",
      "True\n",
      "-1\n",
      "False\n",
      "9223372036854775807\n",
      "0\n",
      "tensor(0)\n",
      "1\n",
      "True\n",
      "1e-12\n",
      "1\n",
      "0\n",
      "2\n",
      "64\n",
      "3\n",
      "-1\n",
      "-2\n",
      "tensor(8., dtype=torch.float64)\n",
      "None\n",
      "False\n",
      "0.1\n",
      "128\n",
      "1\n",
      "9223372036854775807\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "for node in trace_graph.nodes():\n",
    "    if node.kind() == 'prim::Constant':\n",
    "#         print(node)\n",
    "        print(node.outputs().toIValue())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/models/mobilebert/modeling_mobilebert.py:192: TracerWarning: Converting a tensor to a Python index might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  position_ids = self.position_ids[:, :seq_length]\n",
      "/Users/qqcao/.pyenv/versions/nlpnrg/lib/python3.8/site-packages/transformers/models/mobilebert/modeling_mobilebert.py:526: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  torch.tensor(1000),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tOpNode(id=attention_scores.1, op=aten::matmul, flops=10240000, mem_bytes=1049600, scope=encoder.layer.0.attention.self, \n",
      "\t\t\tinputs=[\n",
      "\t\t\t\tDataNode(id=query_layer.1, dtype=Float, shape=[4, 4, 100, 32])\n",
      "\t\t\t\tDataNode(id=236, dtype=Float, shape=[4, 4, 32, 100])],\n",
      "\t\t\toutputs=[\n",
      "\t\t\t\tDataNode(id=attention_scores.1, dtype=Float, shape=[4, 4, 100, 100])],)\n",
      "\t\tOpNode(id=input.10, op=aten::softmax, flops=800000, mem_bytes=1920000, scope=encoder.layer.0.attention.self, \n",
      "\t\t\tinputs=[\n",
      "\t\t\t\tDataNode(id=input.9, dtype=Float, shape=[4, 4, 100, 100])\n",
      "\t\t\t\tDataNode(id=105, dtype=int)\n",
      "\t\t\t\tDataNode(id=108, dtype=None)],\n",
      "\t\t\toutputs=[\n",
      "\t\t\t\tDataNode(id=input.10, dtype=Float, shape=[4, 4, 100, 100])],)\n"
     ]
    }
   ],
   "source": [
    "config = AutoConfig.from_pretrained(\"google/mobilebert-uncased\")\n",
    "config.torchscript = True\n",
    "model=AutoModel.from_config(config).eval()\n",
    "get_non_parametric_ml_ops(model, input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tOpNode(id=attention_scores.1, op=aten::matmul, flops=61440000, mem_bytes=4377600, scope=encoder.layer.0.attention.self, \n",
      "\t\t\tinputs=[\n",
      "\t\t\t\tDataNode(id=query_layer.1, dtype=Float, shape=[4, 12, 100, 64])\n",
      "\t\t\t\tDataNode(id=172, dtype=Float, shape=[4, 12, 64, 100])],\n",
      "\t\t\toutputs=[\n",
      "\t\t\t\tDataNode(id=attention_scores.1, dtype=Float, shape=[4, 12, 100, 100])],)\n",
      "\t\tOpNode(id=input.7, op=aten::softmax, flops=2400000, mem_bytes=5760000, scope=encoder.layer.0.attention.self, \n",
      "\t\t\tinputs=[\n",
      "\t\t\t\tDataNode(id=input.6, dtype=Float, shape=[4, 12, 100, 100])\n",
      "\t\t\t\tDataNode(id=100, dtype=int)\n",
      "\t\t\t\tDataNode(id=103, dtype=None)],\n",
      "\t\t\toutputs=[\n",
      "\t\t\t\tDataNode(id=input.7, dtype=Float, shape=[4, 12, 100, 100])],)\n"
     ]
    }
   ],
   "source": [
    "config = AutoConfig.from_pretrained(\"bert-base-uncased\")\n",
    "config.torchscript = True\n",
    "model=AutoModel.from_config(config).eval()\n",
    "get_non_parametric_ml_ops(model, input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tOpNode(id=scores.1, op=aten::matmul, flops=61440000, mem_bytes=4377600, scope=transformer.layer.0.attention, \n",
      "\t\t\tinputs=[\n",
      "\t\t\t\tDataNode(id=q.2, dtype=Float, shape=[4, 12, 100, 64])\n",
      "\t\t\t\tDataNode(id=110, dtype=Float, shape=[4, 12, 64, 100])],\n",
      "\t\t\toutputs=[\n",
      "\t\t\t\tDataNode(id=scores.1, dtype=Float, shape=[4, 12, 100, 100])],)\n",
      "\t\tOpNode(id=input.5, op=aten::softmax, flops=2400000, mem_bytes=5760000, scope=transformer.layer.0.attention, \n",
      "\t\t\tinputs=[\n",
      "\t\t\t\tDataNode(id=input.4, dtype=Float, shape=[4, 12, 100, 100])\n",
      "\t\t\t\tDataNode(id=58, dtype=int)\n",
      "\t\t\t\tDataNode(id=51, dtype=None)],\n",
      "\t\t\toutputs=[\n",
      "\t\t\t\tDataNode(id=input.5, dtype=Float, shape=[4, 12, 100, 100])],)\n"
     ]
    }
   ],
   "source": [
    "config = AutoConfig.from_pretrained(\"distilbert-base-uncased\")\n",
    "config.torchscript = True\n",
    "model=AutoModel.from_config(config).eval()\n",
    "get_non_parametric_ml_ops(model, input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tOpNode(id=attention_scores.1, op=aten::matmul, flops=61440000, mem_bytes=4377600, scope=encoder.layer.0.attention.self, \n",
      "\t\t\tinputs=[\n",
      "\t\t\t\tDataNode(id=query_layer.1, dtype=Float, shape=[4, 12, 100, 64])\n",
      "\t\t\t\tDataNode(id=178, dtype=Float, shape=[4, 12, 64, 100])],\n",
      "\t\t\toutputs=[\n",
      "\t\t\t\tDataNode(id=attention_scores.1, dtype=Float, shape=[4, 12, 100, 100])],)\n",
      "\t\tOpNode(id=input.7, op=aten::softmax, flops=2400000, mem_bytes=5760000, scope=encoder.layer.0.attention.self, \n",
      "\t\t\tinputs=[\n",
      "\t\t\t\tDataNode(id=input.6, dtype=Float, shape=[4, 12, 100, 100])\n",
      "\t\t\t\tDataNode(id=106, dtype=int)\n",
      "\t\t\t\tDataNode(id=109, dtype=None)],\n",
      "\t\t\toutputs=[\n",
      "\t\t\t\tDataNode(id=input.7, dtype=Float, shape=[4, 12, 100, 100])],)\n"
     ]
    }
   ],
   "source": [
    "config = AutoConfig.from_pretrained(\"roberta-base\")\n",
    "config.torchscript = True\n",
    "model=AutoModel.from_config(config).eval()\n",
    "get_non_parametric_ml_ops(model, input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bisect\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from common import sanitize\n",
    "\n",
    "pd.set_option('display.float_format', '{:.6f}'.format)\n",
    "def is_float(x):\n",
    "    try:\n",
    "        float(x)\n",
    "    except ValueError:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "class Namespace:\n",
    "    def __init__(self, **kwargs):\n",
    "        self.__dict__.update(kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_names = ['cpu', 'mem', 'gpu', 'gpu_mem']\n",
    "\n",
    "feature_names = ['batch_size', 'seq_len', 'flops',\n",
    "                 'mem_bytes'] + \\\n",
    "                ['times_mean', 'times_std',\n",
    "                 'gpu_power_mean', 'gpu_power_std',\n",
    "                 'energy_mean', 'energy_std',\n",
    "                 'level_name', 'model_name'] + res_names + [f'{k}_std' for k in res_names] \n",
    "\n",
    "def process_record(energy, prof_info, res, feature_values,\n",
    "                   model_name, bs, runs, seq_len):\n",
    "    res_np = res.to_numpy()\n",
    "    res_t = res_np[:, 0]\n",
    "    energy_np = energy.to_numpy()\n",
    "    energy_t = energy_np[:, 0]\n",
    "    for prof_item in prof_info:\n",
    "        gpu_power_runs = []\n",
    "        energy_runs = []\n",
    "        times_runs = []\n",
    "        res_runs = {k: [] for k in res_names}\n",
    "        repeats = prof_item['repeats']\n",
    "        for r in range(1, runs + 1):\n",
    "            start_r = prof_item[f'start_{r}']\n",
    "            end_r = prof_item[f'end_{r}']\n",
    "            times_runs.append(end_r - start_r)\n",
    "\n",
    "            res_s = bisect.bisect_right(res_t, start_r)\n",
    "            res_e = bisect.bisect_right(res_t, end_r)\n",
    "            res_r = res[res_s:res_e]\n",
    "            for rn in res_names:\n",
    "                res_runs[rn].append(res_r[rn].mean())\n",
    "            gpu_power_r = res_r['gpu_power'].sum()\n",
    "            gpu_power_runs.append(gpu_power_r)\n",
    "\n",
    "            e_s = bisect.bisect_right(energy_t, start_r)\n",
    "            e_e = bisect.bisect_right(energy_t, end_r)\n",
    "            energy_r = energy[e_s:e_e]['value'].div(repeats).sum()\n",
    "            energy_runs.append(energy_r)\n",
    "\n",
    "        times_mean = np.mean(times_runs)\n",
    "        times_std = np.std(times_runs) / times_mean * 100\n",
    "        gpu_power_mean = np.mean(gpu_power_runs)\n",
    "        gpu_power_std = np.std(gpu_power_runs) / gpu_power_mean * 100\n",
    "        energy_mean = np.mean(energy_runs)\n",
    "        energy_std = np.std(energy_runs) / energy_mean * 100\n",
    "        for rn in res_names:\n",
    "            feature_values[rn].append(np.mean(res_runs[rn]))\n",
    "            rn_std = np.std(res_runs[rn]) / np.mean(res_runs[rn]) * 100\n",
    "            feature_values[f'{rn}_std'].append(rn_std)\n",
    "\n",
    "        flops = prof_item['flops'] / 1e6\n",
    "        mem_bytes = prof_item['mem_bytes'] / 1024 / 1024\n",
    "        feature_values['batch_size'].append(bs)\n",
    "        feature_values['seq_len'].append(seq_len)\n",
    "        feature_values['energy_mean'].append(energy_mean)\n",
    "        feature_values['energy_std'].append(energy_std)\n",
    "        feature_values['gpu_power_mean'].append(gpu_power_mean)\n",
    "        feature_values['gpu_power_std'].append(gpu_power_std)\n",
    "        feature_values['flops'].append(flops)\n",
    "        feature_values['mem_bytes'].append(mem_bytes)\n",
    "        feature_values['times_mean'].append(times_mean)\n",
    "        feature_values['times_std'].append(times_std)\n",
    "        feature_values['level_name'].append(prof_item['name'])\n",
    "        feature_values['model_name'].append(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "args=Namespace(out_dir='data/small-exp', \n",
    "               models=[\"google/mobilebert-uncased\", \"bert-base-uncased\", \"distilbert-base-uncased\", \"roberta-base\"],\n",
    "               exp_type='ml', exp_name='mlexp2',\n",
    "               runs=10, batch_size=40, batch_step=8, seq_step=32, input_length=544)\n",
    "\n",
    "out_dir = Path(args.out_dir)\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "runs = args.runs\n",
    "input_length = args.input_length\n",
    "batch_size = args.batch_size\n",
    "batch_step = args.batch_step\n",
    "seq_step = args.seq_step\n",
    "exp_type = args.exp_type\n",
    "exp_name = args.exp_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-8fc70e7b2ad8>:48: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  rn_std = np.std(res_runs[rn]) / np.mean(res_runs[rn]) * 100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "google/mobilebert-uncased done.\n",
      "bert-base-uncased done.\n",
      "distilbert-base-uncased done.\n",
      "roberta-base done.\n"
     ]
    }
   ],
   "source": [
    "res = pd.read_csv(out_dir / f'{exp_name}-res.csv')\n",
    "energy = pd.read_csv(out_dir / f'{exp_name}-energy.csv',\n",
    "                     error_bad_lines=False, usecols=[0, 2])\n",
    "energy = energy[energy['value'].apply(lambda x: is_float(x))]\n",
    "energy = energy[energy['timestamp'].apply(lambda x: is_float(x))]\n",
    "\n",
    "energy['value'] = energy['value'].astype(float).div(100)\n",
    "energy['timestamp'] = energy['timestamp'].astype(float)\n",
    "infos = []\n",
    "for model_name in args.models:\n",
    "    name_s = sanitize(model_name)\n",
    "    feature_file = out_dir / f'{exp_name}_{name_s}_features.csv'\n",
    "    feature_values = {k: [] for k in feature_names}\n",
    "    for bs in list(range(8, batch_size, batch_step)):\n",
    "        for seq_len in range(32, input_length, seq_step):\n",
    "            \n",
    "            filename = f'{name_s}_{exp_type}_r{runs}_b{bs}_i{seq_len}.json'\n",
    "            prof_file = Path(out_dir) / exp_name / filename\n",
    "            if not prof_file.exists():\n",
    "                print(f'{prof_file} not exist!')\n",
    "                continue\n",
    "            with open(prof_file) as f:\n",
    "                prof_info = json.load(f)\n",
    "            process_record(energy, prof_info, res, feature_values,\n",
    "                           model_name, bs, runs, seq_len)\n",
    "    info = pd.DataFrame(data=feature_values)\n",
    "    infos.append(info)\n",
    "    info.to_csv(feature_file)\n",
    "    print(f'{model_name} done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>cpu</th>\n",
       "      <th>mem</th>\n",
       "      <th>n_gpu</th>\n",
       "      <th>gpu</th>\n",
       "      <th>gpu_mem</th>\n",
       "      <th>gpu_power</th>\n",
       "      <th>gpu.1</th>\n",
       "      <th>gpu_mem.1</th>\n",
       "      <th>gpu_power.1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1611101166.626549</td>\n",
       "      <td>1.100000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14720</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>34522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1611101166.796815</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14825</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>34473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1611101166.967060</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14630</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>34570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1611101167.137330</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14726</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>34473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1611101167.307614</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14825</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>34570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312716</th>\n",
       "      <td>1611154402.833895</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8396</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312717</th>\n",
       "      <td>1611154403.004164</td>\n",
       "      <td>1.100000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8494</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312718</th>\n",
       "      <td>1611154403.174428</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8490</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312719</th>\n",
       "      <td>1611154403.344674</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8492</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312720</th>\n",
       "      <td>1611154403.514936</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8394</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6058</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>312721 rows  10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               timestamp      cpu      mem  n_gpu  gpu  gpu_mem  gpu_power  \\\n",
       "0      1611101166.626549 1.100000 5.000000      2    0        0      14720   \n",
       "1      1611101166.796815 0.600000 5.000000      2    0        0      14825   \n",
       "2      1611101166.967060 0.100000 5.000000      2    0        0      14630   \n",
       "3      1611101167.137330 0.100000 5.000000      2    0        0      14726   \n",
       "4      1611101167.307614 0.100000 5.000000      2    0        0      14825   \n",
       "...                  ...      ...      ...    ...  ...      ...        ...   \n",
       "312716 1611154402.833895 0.100000 5.000000      2    0        0       8396   \n",
       "312717 1611154403.004164 1.100000 5.000000      2    0        0       8494   \n",
       "312718 1611154403.174428 0.100000 5.000000      2    0        0       8490   \n",
       "312719 1611154403.344674 0.600000 5.000000      2    0        0       8492   \n",
       "312720 1611154403.514936 0.600000 5.000000      2    0        0       8394   \n",
       "\n",
       "        gpu.1  gpu_mem.1  gpu_power.1  \n",
       "0           0          0        34522  \n",
       "1           0          0        34473  \n",
       "2           0          0        34570  \n",
       "3           0          0        34473  \n",
       "4           0          0        34570  \n",
       "...       ...        ...          ...  \n",
       "312716      0          0         6008  \n",
       "312717      0          0         6154  \n",
       "312718      0          0         6249  \n",
       "312719      0          0         6249  \n",
       "312720      0          0         6058  \n",
       "\n",
       "[312721 rows x 10 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(448, 20)\n",
      "(64, 20)\n",
      "(320, 20)\n",
      "(64, 20)\n"
     ]
    }
   ],
   "source": [
    "for info in infos:\n",
    "    print(info.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>batch_size</th>\n",
       "      <th>seq_len</th>\n",
       "      <th>flops</th>\n",
       "      <th>mem_bytes</th>\n",
       "      <th>times_mean</th>\n",
       "      <th>times_std</th>\n",
       "      <th>gpu_power_mean</th>\n",
       "      <th>gpu_power_std</th>\n",
       "      <th>energy_mean</th>\n",
       "      <th>energy_std</th>\n",
       "      <th>level_name</th>\n",
       "      <th>model_name</th>\n",
       "      <th>cpu</th>\n",
       "      <th>mem</th>\n",
       "      <th>gpu</th>\n",
       "      <th>gpu_mem</th>\n",
       "      <th>cpu_std</th>\n",
       "      <th>mem_std</th>\n",
       "      <th>gpu_std</th>\n",
       "      <th>gpu_mem_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>32</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.093994</td>\n",
       "      <td>0.390114</td>\n",
       "      <td>10.061582</td>\n",
       "      <td>99687.900000</td>\n",
       "      <td>19.070302</td>\n",
       "      <td>0.027893</td>\n",
       "      <td>3.804483</td>\n",
       "      <td>embeddings.position_embeddings:Embedding</td>\n",
       "      <td>bert-base-uncased</td>\n",
       "      <td>7.321667</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>7.483333</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>9.942225</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>108.604930</td>\n",
       "      <td>300.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>64</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.187988</td>\n",
       "      <td>0.396814</td>\n",
       "      <td>9.876111</td>\n",
       "      <td>113639.400000</td>\n",
       "      <td>19.212641</td>\n",
       "      <td>0.032223</td>\n",
       "      <td>16.578860</td>\n",
       "      <td>embeddings.position_embeddings:Embedding</td>\n",
       "      <td>bert-base-uncased</td>\n",
       "      <td>7.263333</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>9.233333</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>12.763097</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>109.999354</td>\n",
       "      <td>300.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>96</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.281982</td>\n",
       "      <td>0.408087</td>\n",
       "      <td>9.163128</td>\n",
       "      <td>127849.600000</td>\n",
       "      <td>18.979407</td>\n",
       "      <td>0.034123</td>\n",
       "      <td>18.758146</td>\n",
       "      <td>embeddings.position_embeddings:Embedding</td>\n",
       "      <td>bert-base-uncased</td>\n",
       "      <td>7.160000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>10.700000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>16.483077</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>106.428133</td>\n",
       "      <td>300.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>128</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.375977</td>\n",
       "      <td>0.380339</td>\n",
       "      <td>10.142732</td>\n",
       "      <td>125164.100000</td>\n",
       "      <td>18.273739</td>\n",
       "      <td>0.028487</td>\n",
       "      <td>26.565901</td>\n",
       "      <td>embeddings.position_embeddings:Embedding</td>\n",
       "      <td>bert-base-uncased</td>\n",
       "      <td>6.726667</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>11.900000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>14.436812</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>95.700804</td>\n",
       "      <td>300.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>160</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.469971</td>\n",
       "      <td>0.374682</td>\n",
       "      <td>14.391839</td>\n",
       "      <td>124632.900000</td>\n",
       "      <td>16.261459</td>\n",
       "      <td>0.030248</td>\n",
       "      <td>20.017994</td>\n",
       "      <td>embeddings.position_embeddings:Embedding</td>\n",
       "      <td>bert-base-uncased</td>\n",
       "      <td>7.030000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>15.616667</td>\n",
       "      <td>1.050000</td>\n",
       "      <td>15.833553</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>91.204718</td>\n",
       "      <td>300.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>32</td>\n",
       "      <td>384</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.127930</td>\n",
       "      <td>0.396759</td>\n",
       "      <td>10.551210</td>\n",
       "      <td>201587.300000</td>\n",
       "      <td>17.360342</td>\n",
       "      <td>0.038488</td>\n",
       "      <td>14.872588</td>\n",
       "      <td>embeddings.position_embeddings:Embedding</td>\n",
       "      <td>bert-base-uncased</td>\n",
       "      <td>7.225000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>24.200000</td>\n",
       "      <td>5.666667</td>\n",
       "      <td>14.714180</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>83.611652</td>\n",
       "      <td>182.148842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>32</td>\n",
       "      <td>416</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.221924</td>\n",
       "      <td>0.392991</td>\n",
       "      <td>9.387470</td>\n",
       "      <td>218736.100000</td>\n",
       "      <td>17.914643</td>\n",
       "      <td>0.040399</td>\n",
       "      <td>13.179194</td>\n",
       "      <td>embeddings.position_embeddings:Embedding</td>\n",
       "      <td>bert-base-uncased</td>\n",
       "      <td>7.230000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>29.433333</td>\n",
       "      <td>10.733333</td>\n",
       "      <td>14.858074</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>66.533553</td>\n",
       "      <td>104.426361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>32</td>\n",
       "      <td>448</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.315918</td>\n",
       "      <td>0.387889</td>\n",
       "      <td>10.084984</td>\n",
       "      <td>223854.500000</td>\n",
       "      <td>16.222069</td>\n",
       "      <td>0.045902</td>\n",
       "      <td>20.619385</td>\n",
       "      <td>embeddings.position_embeddings:Embedding</td>\n",
       "      <td>bert-base-uncased</td>\n",
       "      <td>7.148333</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>30.266667</td>\n",
       "      <td>12.216667</td>\n",
       "      <td>12.424077</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>58.677914</td>\n",
       "      <td>80.805108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>32</td>\n",
       "      <td>480</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.409912</td>\n",
       "      <td>0.402399</td>\n",
       "      <td>10.100118</td>\n",
       "      <td>234802.300000</td>\n",
       "      <td>17.635323</td>\n",
       "      <td>0.044473</td>\n",
       "      <td>25.428520</td>\n",
       "      <td>embeddings.position_embeddings:Embedding</td>\n",
       "      <td>bert-base-uncased</td>\n",
       "      <td>7.078333</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>31.416667</td>\n",
       "      <td>12.916667</td>\n",
       "      <td>13.102706</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>53.484826</td>\n",
       "      <td>69.763984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>32</td>\n",
       "      <td>512</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.503906</td>\n",
       "      <td>0.394121</td>\n",
       "      <td>11.973273</td>\n",
       "      <td>226505.300000</td>\n",
       "      <td>22.989529</td>\n",
       "      <td>0.042040</td>\n",
       "      <td>13.879941</td>\n",
       "      <td>embeddings.position_embeddings:Embedding</td>\n",
       "      <td>bert-base-uncased</td>\n",
       "      <td>7.271667</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>34.366667</td>\n",
       "      <td>14.433333</td>\n",
       "      <td>13.560842</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>52.931018</td>\n",
       "      <td>70.188857</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>64 rows  20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    batch_size  seq_len    flops  mem_bytes  times_mean  times_std  \\\n",
       "0            8       32 0.000000   0.093994    0.390114  10.061582   \n",
       "1            8       64 0.000000   0.187988    0.396814   9.876111   \n",
       "2            8       96 0.000000   0.281982    0.408087   9.163128   \n",
       "3            8      128 0.000000   0.375977    0.380339  10.142732   \n",
       "4            8      160 0.000000   0.469971    0.374682  14.391839   \n",
       "..         ...      ...      ...        ...         ...        ...   \n",
       "59          32      384 0.000000   1.127930    0.396759  10.551210   \n",
       "60          32      416 0.000000   1.221924    0.392991   9.387470   \n",
       "61          32      448 0.000000   1.315918    0.387889  10.084984   \n",
       "62          32      480 0.000000   1.409912    0.402399  10.100118   \n",
       "63          32      512 0.000000   1.503906    0.394121  11.973273   \n",
       "\n",
       "    gpu_power_mean  gpu_power_std  energy_mean  energy_std  \\\n",
       "0     99687.900000      19.070302     0.027893    3.804483   \n",
       "1    113639.400000      19.212641     0.032223   16.578860   \n",
       "2    127849.600000      18.979407     0.034123   18.758146   \n",
       "3    125164.100000      18.273739     0.028487   26.565901   \n",
       "4    124632.900000      16.261459     0.030248   20.017994   \n",
       "..             ...            ...          ...         ...   \n",
       "59   201587.300000      17.360342     0.038488   14.872588   \n",
       "60   218736.100000      17.914643     0.040399   13.179194   \n",
       "61   223854.500000      16.222069     0.045902   20.619385   \n",
       "62   234802.300000      17.635323     0.044473   25.428520   \n",
       "63   226505.300000      22.989529     0.042040   13.879941   \n",
       "\n",
       "                                  level_name         model_name      cpu  \\\n",
       "0   embeddings.position_embeddings:Embedding  bert-base-uncased 7.321667   \n",
       "1   embeddings.position_embeddings:Embedding  bert-base-uncased 7.263333   \n",
       "2   embeddings.position_embeddings:Embedding  bert-base-uncased 7.160000   \n",
       "3   embeddings.position_embeddings:Embedding  bert-base-uncased 6.726667   \n",
       "4   embeddings.position_embeddings:Embedding  bert-base-uncased 7.030000   \n",
       "..                                       ...                ...      ...   \n",
       "59  embeddings.position_embeddings:Embedding  bert-base-uncased 7.225000   \n",
       "60  embeddings.position_embeddings:Embedding  bert-base-uncased 7.230000   \n",
       "61  embeddings.position_embeddings:Embedding  bert-base-uncased 7.148333   \n",
       "62  embeddings.position_embeddings:Embedding  bert-base-uncased 7.078333   \n",
       "63  embeddings.position_embeddings:Embedding  bert-base-uncased 7.271667   \n",
       "\n",
       "         mem       gpu   gpu_mem   cpu_std  mem_std    gpu_std  gpu_mem_std  \n",
       "0  11.000000  7.483333  0.200000  9.942225 0.000000 108.604930   300.000000  \n",
       "1  11.000000  9.233333  0.450000 12.763097 0.000000 109.999354   300.000000  \n",
       "2  11.000000 10.700000  0.750000 16.483077 0.000000 106.428133   300.000000  \n",
       "3  11.000000 11.900000  0.900000 14.436812 0.000000  95.700804   300.000000  \n",
       "4  11.000000 15.616667  1.050000 15.833553 0.000000  91.204718   300.000000  \n",
       "..       ...       ...       ...       ...      ...        ...          ...  \n",
       "59 11.000000 24.200000  5.666667 14.714180 0.000000  83.611652   182.148842  \n",
       "60 11.000000 29.433333 10.733333 14.858074 0.000000  66.533553   104.426361  \n",
       "61 11.000000 30.266667 12.216667 12.424077 0.000000  58.677914    80.805108  \n",
       "62 11.000000 31.416667 12.916667 13.102706 0.000000  53.484826    69.763984  \n",
       "63 11.000000 34.366667 14.433333 13.560842 0.000000  52.931018    70.188857  \n",
       "\n",
       "[64 rows x 20 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infos[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>batch_size</th>\n",
       "      <th>seq_len</th>\n",
       "      <th>flops</th>\n",
       "      <th>mem_bytes</th>\n",
       "      <th>cpu</th>\n",
       "      <th>mem</th>\n",
       "      <th>gpu</th>\n",
       "      <th>gpu_mem</th>\n",
       "      <th>cpu_std</th>\n",
       "      <th>mem_std</th>\n",
       "      <th>gpu_std</th>\n",
       "      <th>gpu_mem_std</th>\n",
       "      <th>times_mean</th>\n",
       "      <th>times_std</th>\n",
       "      <th>gpu_power_mean</th>\n",
       "      <th>gpu_power_std</th>\n",
       "      <th>energy_mean</th>\n",
       "      <th>energy_std</th>\n",
       "      <th>level_name</th>\n",
       "      <th>model_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>32</td>\n",
       "      <td>96</td>\n",
       "      <td>267218.583552</td>\n",
       "      <td>9009.046875</td>\n",
       "      <td>8.508397</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>97.403333</td>\n",
       "      <td>47.084286</td>\n",
       "      <td>0.716761</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.834511</td>\n",
       "      <td>0.939147</td>\n",
       "      <td>6.105381</td>\n",
       "      <td>0.013890</td>\n",
       "      <td>7149497.000000</td>\n",
       "      <td>1.185902</td>\n",
       "      <td>110.030480</td>\n",
       "      <td>0.314705</td>\n",
       "      <td>distilbert-base-uncased</td>\n",
       "      <td>distilbert-base-uncased</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>16</td>\n",
       "      <td>224</td>\n",
       "      <td>320375.881728</td>\n",
       "      <td>7684.554688</td>\n",
       "      <td>8.452039</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>97.748019</td>\n",
       "      <td>47.471304</td>\n",
       "      <td>0.504157</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.115519</td>\n",
       "      <td>1.075000</td>\n",
       "      <td>7.692327</td>\n",
       "      <td>0.005014</td>\n",
       "      <td>8812926.600000</td>\n",
       "      <td>0.802637</td>\n",
       "      <td>135.957960</td>\n",
       "      <td>0.416252</td>\n",
       "      <td>distilbert-base-uncased</td>\n",
       "      <td>distilbert-base-uncased</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>256</td>\n",
       "      <td>552910.454784</td>\n",
       "      <td>12888.093750</td>\n",
       "      <td>8.606737</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>98.812987</td>\n",
       "      <td>49.563337</td>\n",
       "      <td>0.742626</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.779265</td>\n",
       "      <td>0.760980</td>\n",
       "      <td>13.130971</td>\n",
       "      <td>0.009864</td>\n",
       "      <td>15239726.800000</td>\n",
       "      <td>0.306798</td>\n",
       "      <td>233.934260</td>\n",
       "      <td>0.433762</td>\n",
       "      <td>distilbert-base-uncased</td>\n",
       "      <td>distilbert-base-uncased</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>32</td>\n",
       "      <td>256</td>\n",
       "      <td>737213.939712</td>\n",
       "      <td>17184.125000</td>\n",
       "      <td>8.544642</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>99.243914</td>\n",
       "      <td>50.349796</td>\n",
       "      <td>0.378460</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.340231</td>\n",
       "      <td>0.349927</td>\n",
       "      <td>17.350395</td>\n",
       "      <td>0.111740</td>\n",
       "      <td>20302028.800000</td>\n",
       "      <td>0.678733</td>\n",
       "      <td>311.653340</td>\n",
       "      <td>0.481888</td>\n",
       "      <td>distilbert-base-uncased</td>\n",
       "      <td>distilbert-base-uncased</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>32</td>\n",
       "      <td>192</td>\n",
       "      <td>545521.139712</td>\n",
       "      <td>13644.093750</td>\n",
       "      <td>9.037067</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>98.573333</td>\n",
       "      <td>49.106667</td>\n",
       "      <td>0.767233</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.705705</td>\n",
       "      <td>0.713012</td>\n",
       "      <td>12.735601</td>\n",
       "      <td>0.014315</td>\n",
       "      <td>14981904.600000</td>\n",
       "      <td>0.190907</td>\n",
       "      <td>230.381380</td>\n",
       "      <td>0.494689</td>\n",
       "      <td>distilbert-base-uncased</td>\n",
       "      <td>distilbert-base-uncased</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>32</td>\n",
       "      <td>224</td>\n",
       "      <td>640751.763456</td>\n",
       "      <td>15369.109375</td>\n",
       "      <td>8.886067</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>99.485393</td>\n",
       "      <td>48.773034</td>\n",
       "      <td>2.062584</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.157697</td>\n",
       "      <td>0.195151</td>\n",
       "      <td>15.209210</td>\n",
       "      <td>0.008151</td>\n",
       "      <td>17743795.200000</td>\n",
       "      <td>0.433082</td>\n",
       "      <td>273.227680</td>\n",
       "      <td>0.502665</td>\n",
       "      <td>distilbert-base-uncased</td>\n",
       "      <td>distilbert-base-uncased</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>32</td>\n",
       "      <td>128</td>\n",
       "      <td>358754.549760</td>\n",
       "      <td>10464.062500</td>\n",
       "      <td>8.464745</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>98.413095</td>\n",
       "      <td>48.198129</td>\n",
       "      <td>0.378008</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.730264</td>\n",
       "      <td>0.692230</td>\n",
       "      <td>8.268359</td>\n",
       "      <td>0.041995</td>\n",
       "      <td>9619756.000000</td>\n",
       "      <td>0.876812</td>\n",
       "      <td>146.896620</td>\n",
       "      <td>0.683801</td>\n",
       "      <td>distilbert-base-uncased</td>\n",
       "      <td>distilbert-base-uncased</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>16</td>\n",
       "      <td>128</td>\n",
       "      <td>179377.274880</td>\n",
       "      <td>5232.031250</td>\n",
       "      <td>8.446462</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>96.554769</td>\n",
       "      <td>44.796923</td>\n",
       "      <td>1.774962</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.795926</td>\n",
       "      <td>1.835574</td>\n",
       "      <td>4.342638</td>\n",
       "      <td>0.024293</td>\n",
       "      <td>4821553.200000</td>\n",
       "      <td>1.785010</td>\n",
       "      <td>74.930640</td>\n",
       "      <td>0.792158</td>\n",
       "      <td>distilbert-base-uncased</td>\n",
       "      <td>distilbert-base-uncased</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>24</td>\n",
       "      <td>96</td>\n",
       "      <td>200413.937664</td>\n",
       "      <td>6756.785156</td>\n",
       "      <td>8.520000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>97.184127</td>\n",
       "      <td>45.711111</td>\n",
       "      <td>1.362019</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.173821</td>\n",
       "      <td>1.208262</td>\n",
       "      <td>4.739821</td>\n",
       "      <td>0.017427</td>\n",
       "      <td>5361371.000000</td>\n",
       "      <td>1.757455</td>\n",
       "      <td>82.880320</td>\n",
       "      <td>0.877253</td>\n",
       "      <td>distilbert-base-uncased</td>\n",
       "      <td>distilbert-base-uncased</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>24</td>\n",
       "      <td>192</td>\n",
       "      <td>409140.854784</td>\n",
       "      <td>10233.070312</td>\n",
       "      <td>8.465064</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>98.553842</td>\n",
       "      <td>47.498911</td>\n",
       "      <td>0.558718</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.725364</td>\n",
       "      <td>0.773401</td>\n",
       "      <td>9.722713</td>\n",
       "      <td>0.008940</td>\n",
       "      <td>11198226.800000</td>\n",
       "      <td>0.517100</td>\n",
       "      <td>171.861860</td>\n",
       "      <td>0.890473</td>\n",
       "      <td>distilbert-base-uncased</td>\n",
       "      <td>distilbert-base-uncased</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>24</td>\n",
       "      <td>224</td>\n",
       "      <td>480563.822592</td>\n",
       "      <td>11526.832031</td>\n",
       "      <td>8.511940</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>98.934328</td>\n",
       "      <td>48.970149</td>\n",
       "      <td>0.505168</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.336336</td>\n",
       "      <td>0.344825</td>\n",
       "      <td>11.379723</td>\n",
       "      <td>0.006916</td>\n",
       "      <td>13273249.200000</td>\n",
       "      <td>0.134974</td>\n",
       "      <td>202.381520</td>\n",
       "      <td>0.958674</td>\n",
       "      <td>distilbert-base-uncased</td>\n",
       "      <td>distilbert-base-uncased</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>16</td>\n",
       "      <td>160</td>\n",
       "      <td>225761.034240</td>\n",
       "      <td>6004.539062</td>\n",
       "      <td>8.458516</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>97.043441</td>\n",
       "      <td>46.605806</td>\n",
       "      <td>1.086998</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.945466</td>\n",
       "      <td>1.948555</td>\n",
       "      <td>5.237405</td>\n",
       "      <td>0.181137</td>\n",
       "      <td>6080120.800000</td>\n",
       "      <td>1.244496</td>\n",
       "      <td>93.119820</td>\n",
       "      <td>0.970546</td>\n",
       "      <td>distilbert-base-uncased</td>\n",
       "      <td>distilbert-base-uncased</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>32</td>\n",
       "      <td>160</td>\n",
       "      <td>451522.068480</td>\n",
       "      <td>12009.078125</td>\n",
       "      <td>9.099413</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>99.123903</td>\n",
       "      <td>48.486832</td>\n",
       "      <td>0.490717</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.361352</td>\n",
       "      <td>0.343049</td>\n",
       "      <td>10.459841</td>\n",
       "      <td>0.022549</td>\n",
       "      <td>12303510.000000</td>\n",
       "      <td>0.398416</td>\n",
       "      <td>188.528500</td>\n",
       "      <td>0.998482</td>\n",
       "      <td>distilbert-base-uncased</td>\n",
       "      <td>distilbert-base-uncased</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>87841.308672</td>\n",
       "      <td>6369.015625</td>\n",
       "      <td>8.395385</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>93.261538</td>\n",
       "      <td>40.692308</td>\n",
       "      <td>1.944187</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.737670</td>\n",
       "      <td>1.953574</td>\n",
       "      <td>2.256446</td>\n",
       "      <td>0.030123</td>\n",
       "      <td>2352126.400000</td>\n",
       "      <td>0.201025</td>\n",
       "      <td>37.131620</td>\n",
       "      <td>1.033132</td>\n",
       "      <td>distilbert-base-uncased</td>\n",
       "      <td>distilbert-base-uncased</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>160</td>\n",
       "      <td>112880.517120</td>\n",
       "      <td>3002.269531</td>\n",
       "      <td>8.429500</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>94.982500</td>\n",
       "      <td>45.575000</td>\n",
       "      <td>1.915294</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.966219</td>\n",
       "      <td>2.000662</td>\n",
       "      <td>2.644552</td>\n",
       "      <td>0.441735</td>\n",
       "      <td>2977917.400000</td>\n",
       "      <td>3.816066</td>\n",
       "      <td>46.091020</td>\n",
       "      <td>1.110584</td>\n",
       "      <td>distilbert-base-uncased</td>\n",
       "      <td>distilbert-base-uncased</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>32</td>\n",
       "      <td>64</td>\n",
       "      <td>176914.169856</td>\n",
       "      <td>7644.031250</td>\n",
       "      <td>8.370067</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>96.439000</td>\n",
       "      <td>43.835667</td>\n",
       "      <td>1.157615</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.268276</td>\n",
       "      <td>1.219408</td>\n",
       "      <td>4.243408</td>\n",
       "      <td>0.008284</td>\n",
       "      <td>4732875.600000</td>\n",
       "      <td>1.621049</td>\n",
       "      <td>73.656100</td>\n",
       "      <td>1.268059</td>\n",
       "      <td>distilbert-base-uncased</td>\n",
       "      <td>distilbert-base-uncased</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>24</td>\n",
       "      <td>128</td>\n",
       "      <td>269065.912320</td>\n",
       "      <td>7848.046875</td>\n",
       "      <td>8.414459</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>96.710511</td>\n",
       "      <td>47.382432</td>\n",
       "      <td>1.190941</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.447758</td>\n",
       "      <td>1.495704</td>\n",
       "      <td>6.190069</td>\n",
       "      <td>0.173446</td>\n",
       "      <td>7181237.200000</td>\n",
       "      <td>0.746540</td>\n",
       "      <td>108.747440</td>\n",
       "      <td>1.373019</td>\n",
       "      <td>distilbert-base-uncased</td>\n",
       "      <td>distilbert-base-uncased</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>24</td>\n",
       "      <td>160</td>\n",
       "      <td>338641.551360</td>\n",
       "      <td>9006.808594</td>\n",
       "      <td>8.684000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>97.602126</td>\n",
       "      <td>47.829469</td>\n",
       "      <td>1.284403</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.946751</td>\n",
       "      <td>0.964129</td>\n",
       "      <td>7.793319</td>\n",
       "      <td>0.021567</td>\n",
       "      <td>9164151.800000</td>\n",
       "      <td>0.886953</td>\n",
       "      <td>139.449820</td>\n",
       "      <td>1.431923</td>\n",
       "      <td>distilbert-base-uncased</td>\n",
       "      <td>distilbert-base-uncased</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>256</td>\n",
       "      <td>368606.969856</td>\n",
       "      <td>8592.062500</td>\n",
       "      <td>8.485000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>98.465385</td>\n",
       "      <td>48.300000</td>\n",
       "      <td>0.783337</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.049091</td>\n",
       "      <td>1.026153</td>\n",
       "      <td>8.849152</td>\n",
       "      <td>0.009433</td>\n",
       "      <td>10138401.200000</td>\n",
       "      <td>0.093881</td>\n",
       "      <td>154.229700</td>\n",
       "      <td>1.456391</td>\n",
       "      <td>distilbert-base-uncased</td>\n",
       "      <td>distilbert-base-uncased</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>16</td>\n",
       "      <td>192</td>\n",
       "      <td>272760.569856</td>\n",
       "      <td>6822.046875</td>\n",
       "      <td>8.425804</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>97.583215</td>\n",
       "      <td>47.699289</td>\n",
       "      <td>1.304181</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.865276</td>\n",
       "      <td>0.883924</td>\n",
       "      <td>6.419232</td>\n",
       "      <td>0.154254</td>\n",
       "      <td>7387809.600000</td>\n",
       "      <td>0.836294</td>\n",
       "      <td>114.030360</td>\n",
       "      <td>1.457511</td>\n",
       "      <td>distilbert-base-uncased</td>\n",
       "      <td>distilbert-base-uncased</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8</td>\n",
       "      <td>192</td>\n",
       "      <td>136380.284928</td>\n",
       "      <td>3411.023438</td>\n",
       "      <td>8.342524</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>94.205238</td>\n",
       "      <td>43.373810</td>\n",
       "      <td>1.508406</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.689365</td>\n",
       "      <td>1.728065</td>\n",
       "      <td>3.398204</td>\n",
       "      <td>0.264102</td>\n",
       "      <td>3731381.400000</td>\n",
       "      <td>0.313544</td>\n",
       "      <td>57.428020</td>\n",
       "      <td>1.642813</td>\n",
       "      <td>distilbert-base-uncased</td>\n",
       "      <td>distilbert-base-uncased</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>8</td>\n",
       "      <td>224</td>\n",
       "      <td>160187.940864</td>\n",
       "      <td>3842.277344</td>\n",
       "      <td>8.465217</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>96.513043</td>\n",
       "      <td>46.304348</td>\n",
       "      <td>1.676292</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.183480</td>\n",
       "      <td>1.272292</td>\n",
       "      <td>3.899065</td>\n",
       "      <td>0.211298</td>\n",
       "      <td>4392376.200000</td>\n",
       "      <td>0.333699</td>\n",
       "      <td>67.546060</td>\n",
       "      <td>2.172611</td>\n",
       "      <td>distilbert-base-uncased</td>\n",
       "      <td>distilbert-base-uncased</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>256</td>\n",
       "      <td>184303.484928</td>\n",
       "      <td>4296.031250</td>\n",
       "      <td>8.485926</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>97.511111</td>\n",
       "      <td>44.874074</td>\n",
       "      <td>1.066374</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.262341</td>\n",
       "      <td>1.271797</td>\n",
       "      <td>4.674420</td>\n",
       "      <td>0.226137</td>\n",
       "      <td>5015780.600000</td>\n",
       "      <td>0.903326</td>\n",
       "      <td>78.488700</td>\n",
       "      <td>2.211093</td>\n",
       "      <td>distilbert-base-uncased</td>\n",
       "      <td>distilbert-base-uncased</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>24</td>\n",
       "      <td>64</td>\n",
       "      <td>132685.627392</td>\n",
       "      <td>5733.023438</td>\n",
       "      <td>8.413684</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>95.410526</td>\n",
       "      <td>42.989474</td>\n",
       "      <td>1.700108</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.247902</td>\n",
       "      <td>2.283096</td>\n",
       "      <td>3.215461</td>\n",
       "      <td>0.088619</td>\n",
       "      <td>3591900.000000</td>\n",
       "      <td>0.378004</td>\n",
       "      <td>55.505180</td>\n",
       "      <td>2.262959</td>\n",
       "      <td>distilbert-base-uncased</td>\n",
       "      <td>distilbert-base-uncased</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>16</td>\n",
       "      <td>96</td>\n",
       "      <td>133609.291776</td>\n",
       "      <td>4504.523438</td>\n",
       "      <td>8.314263</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>95.247895</td>\n",
       "      <td>43.810000</td>\n",
       "      <td>2.612124</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.554495</td>\n",
       "      <td>2.576934</td>\n",
       "      <td>3.255645</td>\n",
       "      <td>0.056882</td>\n",
       "      <td>3577320.800000</td>\n",
       "      <td>1.065213</td>\n",
       "      <td>55.363160</td>\n",
       "      <td>2.308336</td>\n",
       "      <td>distilbert-base-uncased</td>\n",
       "      <td>distilbert-base-uncased</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>128</td>\n",
       "      <td>89688.637440</td>\n",
       "      <td>2616.015625</td>\n",
       "      <td>8.307253</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>93.300000</td>\n",
       "      <td>41.934066</td>\n",
       "      <td>1.557604</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.578122</td>\n",
       "      <td>2.611659</td>\n",
       "      <td>2.321113</td>\n",
       "      <td>0.024921</td>\n",
       "      <td>2425176.000000</td>\n",
       "      <td>3.392617</td>\n",
       "      <td>38.224500</td>\n",
       "      <td>2.581707</td>\n",
       "      <td>distilbert-base-uncased</td>\n",
       "      <td>distilbert-base-uncased</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>64</td>\n",
       "      <td>44228.542464</td>\n",
       "      <td>1911.007812</td>\n",
       "      <td>8.098571</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>84.028571</td>\n",
       "      <td>37.607143</td>\n",
       "      <td>5.199522</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.002346</td>\n",
       "      <td>5.191130</td>\n",
       "      <td>1.211244</td>\n",
       "      <td>0.169637</td>\n",
       "      <td>1328822.800000</td>\n",
       "      <td>3.836343</td>\n",
       "      <td>19.563740</td>\n",
       "      <td>2.612894</td>\n",
       "      <td>distilbert-base-uncased</td>\n",
       "      <td>distilbert-base-uncased</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>16</td>\n",
       "      <td>32</td>\n",
       "      <td>43920.654336</td>\n",
       "      <td>3184.507812</td>\n",
       "      <td>8.164643</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>84.232143</td>\n",
       "      <td>37.257143</td>\n",
       "      <td>4.567375</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.262125</td>\n",
       "      <td>5.001740</td>\n",
       "      <td>1.215902</td>\n",
       "      <td>0.618864</td>\n",
       "      <td>1297079.400000</td>\n",
       "      <td>5.193029</td>\n",
       "      <td>19.967100</td>\n",
       "      <td>2.919606</td>\n",
       "      <td>distilbert-base-uncased</td>\n",
       "      <td>distilbert-base-uncased</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>96</td>\n",
       "      <td>66804.645888</td>\n",
       "      <td>2252.261719</td>\n",
       "      <td>8.282727</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>90.867273</td>\n",
       "      <td>45.443636</td>\n",
       "      <td>2.550891</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.785649</td>\n",
       "      <td>2.802997</td>\n",
       "      <td>1.775467</td>\n",
       "      <td>0.018684</td>\n",
       "      <td>1948890.400000</td>\n",
       "      <td>3.860630</td>\n",
       "      <td>30.101000</td>\n",
       "      <td>3.208866</td>\n",
       "      <td>distilbert-base-uncased</td>\n",
       "      <td>distilbert-base-uncased</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>24</td>\n",
       "      <td>32</td>\n",
       "      <td>65880.981504</td>\n",
       "      <td>4776.761719</td>\n",
       "      <td>8.284000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>87.820000</td>\n",
       "      <td>43.080000</td>\n",
       "      <td>2.852137</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.581997</td>\n",
       "      <td>2.548742</td>\n",
       "      <td>1.742802</td>\n",
       "      <td>0.550988</td>\n",
       "      <td>1911081.000000</td>\n",
       "      <td>2.409133</td>\n",
       "      <td>29.061040</td>\n",
       "      <td>3.535137</td>\n",
       "      <td>distilbert-base-uncased</td>\n",
       "      <td>distilbert-base-uncased</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>1</td>\n",
       "      <td>160</td>\n",
       "      <td>14110.064640</td>\n",
       "      <td>375.283691</td>\n",
       "      <td>7.793000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>64.330000</td>\n",
       "      <td>19.340000</td>\n",
       "      <td>6.361063</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>16.348243</td>\n",
       "      <td>17.171944</td>\n",
       "      <td>0.731378</td>\n",
       "      <td>8.683396</td>\n",
       "      <td>610320.800000</td>\n",
       "      <td>6.910692</td>\n",
       "      <td>9.247640</td>\n",
       "      <td>3.997410</td>\n",
       "      <td>distilbert-base-uncased</td>\n",
       "      <td>distilbert-base-uncased</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>16</td>\n",
       "      <td>64</td>\n",
       "      <td>88457.084928</td>\n",
       "      <td>3822.015625</td>\n",
       "      <td>8.266813</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>92.923077</td>\n",
       "      <td>41.428571</td>\n",
       "      <td>2.596559</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.975687</td>\n",
       "      <td>1.901766</td>\n",
       "      <td>2.253641</td>\n",
       "      <td>0.625066</td>\n",
       "      <td>2342947.400000</td>\n",
       "      <td>3.720508</td>\n",
       "      <td>35.150480</td>\n",
       "      <td>4.231981</td>\n",
       "      <td>distilbert-base-uncased</td>\n",
       "      <td>distilbert-base-uncased</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>5528.567808</td>\n",
       "      <td>238.875977</td>\n",
       "      <td>7.973333</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>38.260000</td>\n",
       "      <td>13.316667</td>\n",
       "      <td>8.792552</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>19.323928</td>\n",
       "      <td>20.081273</td>\n",
       "      <td>0.685335</td>\n",
       "      <td>10.762622</td>\n",
       "      <td>352486.400000</td>\n",
       "      <td>16.122614</td>\n",
       "      <td>7.208000</td>\n",
       "      <td>5.400891</td>\n",
       "      <td>distilbert-base-uncased</td>\n",
       "      <td>distilbert-base-uncased</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>1</td>\n",
       "      <td>256</td>\n",
       "      <td>23037.935616</td>\n",
       "      <td>537.003906</td>\n",
       "      <td>7.833000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>72.140000</td>\n",
       "      <td>26.230000</td>\n",
       "      <td>8.854247</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.802801</td>\n",
       "      <td>11.971026</td>\n",
       "      <td>0.775526</td>\n",
       "      <td>2.866193</td>\n",
       "      <td>728962.800000</td>\n",
       "      <td>8.684995</td>\n",
       "      <td>11.596580</td>\n",
       "      <td>7.993768</td>\n",
       "      <td>distilbert-base-uncased</td>\n",
       "      <td>distilbert-base-uncased</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>1</td>\n",
       "      <td>224</td>\n",
       "      <td>20023.492608</td>\n",
       "      <td>480.284668</td>\n",
       "      <td>7.770000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>67.870000</td>\n",
       "      <td>23.140000</td>\n",
       "      <td>10.483493</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.098134</td>\n",
       "      <td>11.257044</td>\n",
       "      <td>0.774679</td>\n",
       "      <td>3.920965</td>\n",
       "      <td>679458.600000</td>\n",
       "      <td>11.339360</td>\n",
       "      <td>10.751340</td>\n",
       "      <td>8.974828</td>\n",
       "      <td>distilbert-base-uncased</td>\n",
       "      <td>distilbert-base-uncased</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>1</td>\n",
       "      <td>192</td>\n",
       "      <td>17047.535616</td>\n",
       "      <td>426.377930</td>\n",
       "      <td>7.755000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>66.183333</td>\n",
       "      <td>21.366667</td>\n",
       "      <td>5.625934</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.011140</td>\n",
       "      <td>8.813317</td>\n",
       "      <td>0.725150</td>\n",
       "      <td>6.138047</td>\n",
       "      <td>520472.200000</td>\n",
       "      <td>9.237097</td>\n",
       "      <td>9.263460</td>\n",
       "      <td>9.372650</td>\n",
       "      <td>distilbert-base-uncased</td>\n",
       "      <td>distilbert-base-uncased</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>32</td>\n",
       "      <td>21960.327168</td>\n",
       "      <td>1592.253906</td>\n",
       "      <td>7.905000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>67.120000</td>\n",
       "      <td>22.200000</td>\n",
       "      <td>7.847829</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.848680</td>\n",
       "      <td>5.782633</td>\n",
       "      <td>0.758767</td>\n",
       "      <td>2.850558</td>\n",
       "      <td>697940.000000</td>\n",
       "      <td>12.757931</td>\n",
       "      <td>10.643360</td>\n",
       "      <td>10.956704</td>\n",
       "      <td>distilbert-base-uncased</td>\n",
       "      <td>distilbert-base-uncased</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>1</td>\n",
       "      <td>128</td>\n",
       "      <td>11211.079680</td>\n",
       "      <td>327.001953</td>\n",
       "      <td>7.690000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>50.100000</td>\n",
       "      <td>19.200000</td>\n",
       "      <td>3.629463</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12.355112</td>\n",
       "      <td>12.527097</td>\n",
       "      <td>0.680105</td>\n",
       "      <td>10.681813</td>\n",
       "      <td>475603.200000</td>\n",
       "      <td>10.133013</td>\n",
       "      <td>8.498900</td>\n",
       "      <td>11.442511</td>\n",
       "      <td>distilbert-base-uncased</td>\n",
       "      <td>distilbert-base-uncased</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>1</td>\n",
       "      <td>96</td>\n",
       "      <td>8350.580736</td>\n",
       "      <td>281.532715</td>\n",
       "      <td>7.923333</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>43.300000</td>\n",
       "      <td>16.326667</td>\n",
       "      <td>6.952160</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.334058</td>\n",
       "      <td>11.900664</td>\n",
       "      <td>0.677334</td>\n",
       "      <td>11.067201</td>\n",
       "      <td>429363.600000</td>\n",
       "      <td>10.241145</td>\n",
       "      <td>7.993180</td>\n",
       "      <td>12.793883</td>\n",
       "      <td>distilbert-base-uncased</td>\n",
       "      <td>distilbert-base-uncased</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>2745.040896</td>\n",
       "      <td>199.031738</td>\n",
       "      <td>7.208000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>21.986667</td>\n",
       "      <td>10.586667</td>\n",
       "      <td>8.177774</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>28.338693</td>\n",
       "      <td>27.386504</td>\n",
       "      <td>0.701955</td>\n",
       "      <td>11.375240</td>\n",
       "      <td>242469.600000</td>\n",
       "      <td>14.443852</td>\n",
       "      <td>6.028220</td>\n",
       "      <td>13.148183</td>\n",
       "      <td>distilbert-base-uncased</td>\n",
       "      <td>distilbert-base-uncased</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    batch_size  seq_len         flops    mem_bytes      cpu       mem  \\\n",
       "26          32       96 267218.583552  9009.046875 8.508397 11.000000   \n",
       "14          16      224 320375.881728  7684.554688 8.452039 13.000000   \n",
       "23          24      256 552910.454784 12888.093750 8.606737 13.000000   \n",
       "31          32      256 737213.939712 17184.125000 8.544642 12.000000   \n",
       "29          32      192 545521.139712 13644.093750 9.037067 11.000000   \n",
       "30          32      224 640751.763456 15369.109375 8.886067 13.000000   \n",
       "27          32      128 358754.549760 10464.062500 8.464745 13.000000   \n",
       "11          16      128 179377.274880  5232.031250 8.446462 13.000000   \n",
       "18          24       96 200413.937664  6756.785156 8.520000 13.000000   \n",
       "21          24      192 409140.854784 10233.070312 8.465064 13.000000   \n",
       "22          24      224 480563.822592 11526.832031 8.511940 13.000000   \n",
       "12          16      160 225761.034240  6004.539062 8.458516 11.000000   \n",
       "28          32      160 451522.068480 12009.078125 9.099413 13.000000   \n",
       "24          32       32  87841.308672  6369.015625 8.395385 13.000000   \n",
       "4            8      160 112880.517120  3002.269531 8.429500 13.000000   \n",
       "25          32       64 176914.169856  7644.031250 8.370067 12.000000   \n",
       "19          24      128 269065.912320  7848.046875 8.414459 13.000000   \n",
       "20          24      160 338641.551360  9006.808594 8.684000 13.000000   \n",
       "15          16      256 368606.969856  8592.062500 8.485000 13.000000   \n",
       "13          16      192 272760.569856  6822.046875 8.425804 13.000000   \n",
       "5            8      192 136380.284928  3411.023438 8.342524 13.000000   \n",
       "6            8      224 160187.940864  3842.277344 8.465217 13.000000   \n",
       "7            8      256 184303.484928  4296.031250 8.485926 13.000000   \n",
       "17          24       64 132685.627392  5733.023438 8.413684 13.000000   \n",
       "10          16       96 133609.291776  4504.523438 8.314263 13.000000   \n",
       "3            8      128  89688.637440  2616.015625 8.307253 13.000000   \n",
       "1            8       64  44228.542464  1911.007812 8.098571 13.000000   \n",
       "8           16       32  43920.654336  3184.507812 8.164643 13.000000   \n",
       "2            8       96  66804.645888  2252.261719 8.282727 13.000000   \n",
       "16          24       32  65880.981504  4776.761719 8.284000 13.000000   \n",
       "36           1      160  14110.064640   375.283691 7.793000 13.000000   \n",
       "9           16       64  88457.084928  3822.015625 8.266813 11.000000   \n",
       "33           1       64   5528.567808   238.875977 7.973333 13.000000   \n",
       "39           1      256  23037.935616   537.003906 7.833000 13.000000   \n",
       "38           1      224  20023.492608   480.284668 7.770000 13.000000   \n",
       "37           1      192  17047.535616   426.377930 7.755000 13.000000   \n",
       "0            8       32  21960.327168  1592.253906 7.905000 13.000000   \n",
       "35           1      128  11211.079680   327.001953 7.690000 13.000000   \n",
       "34           1       96   8350.580736   281.532715 7.923333 13.000000   \n",
       "32           1       32   2745.040896   199.031738 7.208000 13.000000   \n",
       "\n",
       "         gpu   gpu_mem   cpu_std  mem_std   gpu_std  gpu_mem_std  times_mean  \\\n",
       "26 97.403333 47.084286  0.716761 0.000000  0.834511     0.939147    6.105381   \n",
       "14 97.748019 47.471304  0.504157 0.000000  1.115519     1.075000    7.692327   \n",
       "23 98.812987 49.563337  0.742626 0.000000  0.779265     0.760980   13.130971   \n",
       "31 99.243914 50.349796  0.378460 0.000000  0.340231     0.349927   17.350395   \n",
       "29 98.573333 49.106667  0.767233 0.000000  0.705705     0.713012   12.735601   \n",
       "30 99.485393 48.773034  2.062584 0.000000  0.157697     0.195151   15.209210   \n",
       "27 98.413095 48.198129  0.378008 0.000000  0.730264     0.692230    8.268359   \n",
       "11 96.554769 44.796923  1.774962 0.000000  1.795926     1.835574    4.342638   \n",
       "18 97.184127 45.711111  1.362019 0.000000  1.173821     1.208262    4.739821   \n",
       "21 98.553842 47.498911  0.558718 0.000000  0.725364     0.773401    9.722713   \n",
       "22 98.934328 48.970149  0.505168 0.000000  0.336336     0.344825   11.379723   \n",
       "12 97.043441 46.605806  1.086998 0.000000  1.945466     1.948555    5.237405   \n",
       "28 99.123903 48.486832  0.490717 0.000000  0.361352     0.343049   10.459841   \n",
       "24 93.261538 40.692308  1.944187 0.000000  1.737670     1.953574    2.256446   \n",
       "4  94.982500 45.575000  1.915294 0.000000  1.966219     2.000662    2.644552   \n",
       "25 96.439000 43.835667  1.157615 0.000000  1.268276     1.219408    4.243408   \n",
       "19 96.710511 47.382432  1.190941 0.000000  1.447758     1.495704    6.190069   \n",
       "20 97.602126 47.829469  1.284403 0.000000  0.946751     0.964129    7.793319   \n",
       "15 98.465385 48.300000  0.783337 0.000000  1.049091     1.026153    8.849152   \n",
       "13 97.583215 47.699289  1.304181 0.000000  0.865276     0.883924    6.419232   \n",
       "5  94.205238 43.373810  1.508406 0.000000  1.689365     1.728065    3.398204   \n",
       "6  96.513043 46.304348  1.676292 0.000000  1.183480     1.272292    3.899065   \n",
       "7  97.511111 44.874074  1.066374 0.000000  1.262341     1.271797    4.674420   \n",
       "17 95.410526 42.989474  1.700108 0.000000  2.247902     2.283096    3.215461   \n",
       "10 95.247895 43.810000  2.612124 0.000000  2.554495     2.576934    3.255645   \n",
       "3  93.300000 41.934066  1.557604 0.000000  2.578122     2.611659    2.321113   \n",
       "1  84.028571 37.607143  5.199522 0.000000  5.002346     5.191130    1.211244   \n",
       "8  84.232143 37.257143  4.567375 0.000000  5.262125     5.001740    1.215902   \n",
       "2  90.867273 45.443636  2.550891 0.000000  2.785649     2.802997    1.775467   \n",
       "16 87.820000 43.080000  2.852137 0.000000  2.581997     2.548742    1.742802   \n",
       "36 64.330000 19.340000  6.361063 0.000000 16.348243    17.171944    0.731378   \n",
       "9  92.923077 41.428571  2.596559 0.000000  1.975687     1.901766    2.253641   \n",
       "33 38.260000 13.316667  8.792552 0.000000 19.323928    20.081273    0.685335   \n",
       "39 72.140000 26.230000  8.854247 0.000000 11.802801    11.971026    0.775526   \n",
       "38 67.870000 23.140000 10.483493 0.000000 11.098134    11.257044    0.774679   \n",
       "37 66.183333 21.366667  5.625934 0.000000  9.011140     8.813317    0.725150   \n",
       "0  67.120000 22.200000  7.847829 0.000000  5.848680     5.782633    0.758767   \n",
       "35 50.100000 19.200000  3.629463 0.000000 12.355112    12.527097    0.680105   \n",
       "34 43.300000 16.326667  6.952160 0.000000 11.334058    11.900664    0.677334   \n",
       "32 21.986667 10.586667  8.177774 0.000000 28.338693    27.386504    0.701955   \n",
       "\n",
       "    times_std  gpu_power_mean  gpu_power_std  energy_mean  energy_std  \\\n",
       "26   0.013890  7149497.000000       1.185902   110.030480    0.314705   \n",
       "14   0.005014  8812926.600000       0.802637   135.957960    0.416252   \n",
       "23   0.009864 15239726.800000       0.306798   233.934260    0.433762   \n",
       "31   0.111740 20302028.800000       0.678733   311.653340    0.481888   \n",
       "29   0.014315 14981904.600000       0.190907   230.381380    0.494689   \n",
       "30   0.008151 17743795.200000       0.433082   273.227680    0.502665   \n",
       "27   0.041995  9619756.000000       0.876812   146.896620    0.683801   \n",
       "11   0.024293  4821553.200000       1.785010    74.930640    0.792158   \n",
       "18   0.017427  5361371.000000       1.757455    82.880320    0.877253   \n",
       "21   0.008940 11198226.800000       0.517100   171.861860    0.890473   \n",
       "22   0.006916 13273249.200000       0.134974   202.381520    0.958674   \n",
       "12   0.181137  6080120.800000       1.244496    93.119820    0.970546   \n",
       "28   0.022549 12303510.000000       0.398416   188.528500    0.998482   \n",
       "24   0.030123  2352126.400000       0.201025    37.131620    1.033132   \n",
       "4    0.441735  2977917.400000       3.816066    46.091020    1.110584   \n",
       "25   0.008284  4732875.600000       1.621049    73.656100    1.268059   \n",
       "19   0.173446  7181237.200000       0.746540   108.747440    1.373019   \n",
       "20   0.021567  9164151.800000       0.886953   139.449820    1.431923   \n",
       "15   0.009433 10138401.200000       0.093881   154.229700    1.456391   \n",
       "13   0.154254  7387809.600000       0.836294   114.030360    1.457511   \n",
       "5    0.264102  3731381.400000       0.313544    57.428020    1.642813   \n",
       "6    0.211298  4392376.200000       0.333699    67.546060    2.172611   \n",
       "7    0.226137  5015780.600000       0.903326    78.488700    2.211093   \n",
       "17   0.088619  3591900.000000       0.378004    55.505180    2.262959   \n",
       "10   0.056882  3577320.800000       1.065213    55.363160    2.308336   \n",
       "3    0.024921  2425176.000000       3.392617    38.224500    2.581707   \n",
       "1    0.169637  1328822.800000       3.836343    19.563740    2.612894   \n",
       "8    0.618864  1297079.400000       5.193029    19.967100    2.919606   \n",
       "2    0.018684  1948890.400000       3.860630    30.101000    3.208866   \n",
       "16   0.550988  1911081.000000       2.409133    29.061040    3.535137   \n",
       "36   8.683396   610320.800000       6.910692     9.247640    3.997410   \n",
       "9    0.625066  2342947.400000       3.720508    35.150480    4.231981   \n",
       "33  10.762622   352486.400000      16.122614     7.208000    5.400891   \n",
       "39   2.866193   728962.800000       8.684995    11.596580    7.993768   \n",
       "38   3.920965   679458.600000      11.339360    10.751340    8.974828   \n",
       "37   6.138047   520472.200000       9.237097     9.263460    9.372650   \n",
       "0    2.850558   697940.000000      12.757931    10.643360   10.956704   \n",
       "35  10.681813   475603.200000      10.133013     8.498900   11.442511   \n",
       "34  11.067201   429363.600000      10.241145     7.993180   12.793883   \n",
       "32  11.375240   242469.600000      14.443852     6.028220   13.148183   \n",
       "\n",
       "                 level_name               model_name  \n",
       "26  distilbert-base-uncased  distilbert-base-uncased  \n",
       "14  distilbert-base-uncased  distilbert-base-uncased  \n",
       "23  distilbert-base-uncased  distilbert-base-uncased  \n",
       "31  distilbert-base-uncased  distilbert-base-uncased  \n",
       "29  distilbert-base-uncased  distilbert-base-uncased  \n",
       "30  distilbert-base-uncased  distilbert-base-uncased  \n",
       "27  distilbert-base-uncased  distilbert-base-uncased  \n",
       "11  distilbert-base-uncased  distilbert-base-uncased  \n",
       "18  distilbert-base-uncased  distilbert-base-uncased  \n",
       "21  distilbert-base-uncased  distilbert-base-uncased  \n",
       "22  distilbert-base-uncased  distilbert-base-uncased  \n",
       "12  distilbert-base-uncased  distilbert-base-uncased  \n",
       "28  distilbert-base-uncased  distilbert-base-uncased  \n",
       "24  distilbert-base-uncased  distilbert-base-uncased  \n",
       "4   distilbert-base-uncased  distilbert-base-uncased  \n",
       "25  distilbert-base-uncased  distilbert-base-uncased  \n",
       "19  distilbert-base-uncased  distilbert-base-uncased  \n",
       "20  distilbert-base-uncased  distilbert-base-uncased  \n",
       "15  distilbert-base-uncased  distilbert-base-uncased  \n",
       "13  distilbert-base-uncased  distilbert-base-uncased  \n",
       "5   distilbert-base-uncased  distilbert-base-uncased  \n",
       "6   distilbert-base-uncased  distilbert-base-uncased  \n",
       "7   distilbert-base-uncased  distilbert-base-uncased  \n",
       "17  distilbert-base-uncased  distilbert-base-uncased  \n",
       "10  distilbert-base-uncased  distilbert-base-uncased  \n",
       "3   distilbert-base-uncased  distilbert-base-uncased  \n",
       "1   distilbert-base-uncased  distilbert-base-uncased  \n",
       "8   distilbert-base-uncased  distilbert-base-uncased  \n",
       "2   distilbert-base-uncased  distilbert-base-uncased  \n",
       "16  distilbert-base-uncased  distilbert-base-uncased  \n",
       "36  distilbert-base-uncased  distilbert-base-uncased  \n",
       "9   distilbert-base-uncased  distilbert-base-uncased  \n",
       "33  distilbert-base-uncased  distilbert-base-uncased  \n",
       "39  distilbert-base-uncased  distilbert-base-uncased  \n",
       "38  distilbert-base-uncased  distilbert-base-uncased  \n",
       "37  distilbert-base-uncased  distilbert-base-uncased  \n",
       "0   distilbert-base-uncased  distilbert-base-uncased  \n",
       "35  distilbert-base-uncased  distilbert-base-uncased  \n",
       "34  distilbert-base-uncased  distilbert-base-uncased  \n",
       "32  distilbert-base-uncased  distilbert-base-uncased  "
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infos[2].dropna().sort_values('energy_std')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>batch_size</th>\n",
       "      <th>seq_len</th>\n",
       "      <th>flops</th>\n",
       "      <th>mem_bytes</th>\n",
       "      <th>cpu</th>\n",
       "      <th>mem</th>\n",
       "      <th>gpu</th>\n",
       "      <th>gpu_mem</th>\n",
       "      <th>cpu_std</th>\n",
       "      <th>mem_std</th>\n",
       "      <th>gpu_std</th>\n",
       "      <th>gpu_mem_std</th>\n",
       "      <th>times_mean</th>\n",
       "      <th>times_std</th>\n",
       "      <th>gpu_power_mean</th>\n",
       "      <th>gpu_power_std</th>\n",
       "      <th>energy_mean</th>\n",
       "      <th>energy_std</th>\n",
       "      <th>level_name</th>\n",
       "      <th>model_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>32</td>\n",
       "      <td>256</td>\n",
       "      <td>1474943.909888</td>\n",
       "      <td>38283.125000</td>\n",
       "      <td>8.508525</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>99.541730</td>\n",
       "      <td>55.186016</td>\n",
       "      <td>0.348381</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.168512</td>\n",
       "      <td>0.257035</td>\n",
       "      <td>36.452941</td>\n",
       "      <td>0.074229</td>\n",
       "      <td>42826452.600000</td>\n",
       "      <td>0.136187</td>\n",
       "      <td>659.394280</td>\n",
       "      <td>0.176440</td>\n",
       "      <td>roberta-base</td>\n",
       "      <td>roberta-base</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>16</td>\n",
       "      <td>224</td>\n",
       "      <td>640946.862080</td>\n",
       "      <td>16831.150391</td>\n",
       "      <td>8.491075</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>99.160896</td>\n",
       "      <td>52.015162</td>\n",
       "      <td>0.468395</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.296474</td>\n",
       "      <td>0.388250</td>\n",
       "      <td>16.102221</td>\n",
       "      <td>0.021939</td>\n",
       "      <td>18519307.400000</td>\n",
       "      <td>0.488594</td>\n",
       "      <td>286.260440</td>\n",
       "      <td>0.239087</td>\n",
       "      <td>roberta-base</td>\n",
       "      <td>roberta-base</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>32</td>\n",
       "      <td>224</td>\n",
       "      <td>1281893.724160</td>\n",
       "      <td>33660.050781</td>\n",
       "      <td>9.029523</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>99.527074</td>\n",
       "      <td>52.851741</td>\n",
       "      <td>1.815107</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.279158</td>\n",
       "      <td>0.351909</td>\n",
       "      <td>31.849768</td>\n",
       "      <td>0.063903</td>\n",
       "      <td>37259910.200000</td>\n",
       "      <td>0.296302</td>\n",
       "      <td>574.288680</td>\n",
       "      <td>0.249235</td>\n",
       "      <td>roberta-base</td>\n",
       "      <td>roberta-base</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>32</td>\n",
       "      <td>128</td>\n",
       "      <td>717635.043328</td>\n",
       "      <td>21734.828125</td>\n",
       "      <td>8.517996</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>99.205743</td>\n",
       "      <td>50.748020</td>\n",
       "      <td>0.346976</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.220200</td>\n",
       "      <td>0.265863</td>\n",
       "      <td>17.132208</td>\n",
       "      <td>0.144713</td>\n",
       "      <td>19914179.000000</td>\n",
       "      <td>0.490498</td>\n",
       "      <td>307.390720</td>\n",
       "      <td>0.287794</td>\n",
       "      <td>roberta-base</td>\n",
       "      <td>roberta-base</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>32</td>\n",
       "      <td>192</td>\n",
       "      <td>1091325.517824</td>\n",
       "      <td>29360.976562</td>\n",
       "      <td>9.179919</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>99.297950</td>\n",
       "      <td>52.945688</td>\n",
       "      <td>2.702689</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.436990</td>\n",
       "      <td>0.448791</td>\n",
       "      <td>26.582488</td>\n",
       "      <td>0.073688</td>\n",
       "      <td>31182663.800000</td>\n",
       "      <td>0.225302</td>\n",
       "      <td>481.171700</td>\n",
       "      <td>0.302256</td>\n",
       "      <td>roberta-base</td>\n",
       "      <td>roberta-base</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>32</td>\n",
       "      <td>160</td>\n",
       "      <td>903239.290880</td>\n",
       "      <td>25385.902344</td>\n",
       "      <td>8.965966</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>99.338792</td>\n",
       "      <td>51.254306</td>\n",
       "      <td>1.464205</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.289965</td>\n",
       "      <td>0.406308</td>\n",
       "      <td>21.762289</td>\n",
       "      <td>0.147602</td>\n",
       "      <td>25667877.400000</td>\n",
       "      <td>0.288475</td>\n",
       "      <td>394.860140</td>\n",
       "      <td>0.315285</td>\n",
       "      <td>roberta-base</td>\n",
       "      <td>roberta-base</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>256</td>\n",
       "      <td>737471.954944</td>\n",
       "      <td>19142.687500</td>\n",
       "      <td>8.511865</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>99.085371</td>\n",
       "      <td>53.058215</td>\n",
       "      <td>0.267594</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.381918</td>\n",
       "      <td>0.403914</td>\n",
       "      <td>18.593617</td>\n",
       "      <td>0.040337</td>\n",
       "      <td>21241280.400000</td>\n",
       "      <td>0.196262</td>\n",
       "      <td>330.010140</td>\n",
       "      <td>0.322031</td>\n",
       "      <td>roberta-base</td>\n",
       "      <td>roberta-base</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>24</td>\n",
       "      <td>192</td>\n",
       "      <td>818494.138368</td>\n",
       "      <td>22021.294922</td>\n",
       "      <td>8.505210</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>99.267227</td>\n",
       "      <td>51.275630</td>\n",
       "      <td>0.252548</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.310596</td>\n",
       "      <td>0.301194</td>\n",
       "      <td>20.275331</td>\n",
       "      <td>0.038197</td>\n",
       "      <td>23352421.200000</td>\n",
       "      <td>0.179585</td>\n",
       "      <td>360.348560</td>\n",
       "      <td>0.356916</td>\n",
       "      <td>roberta-base</td>\n",
       "      <td>roberta-base</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>16</td>\n",
       "      <td>96</td>\n",
       "      <td>267256.387584</td>\n",
       "      <td>9205.001953</td>\n",
       "      <td>8.521000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>98.691667</td>\n",
       "      <td>46.388718</td>\n",
       "      <td>1.069904</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.625956</td>\n",
       "      <td>0.619118</td>\n",
       "      <td>6.719930</td>\n",
       "      <td>0.162062</td>\n",
       "      <td>7381716.200000</td>\n",
       "      <td>0.804158</td>\n",
       "      <td>115.028300</td>\n",
       "      <td>0.382395</td>\n",
       "      <td>roberta-base</td>\n",
       "      <td>roberta-base</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>32</td>\n",
       "      <td>96</td>\n",
       "      <td>534512.775168</td>\n",
       "      <td>18407.753906</td>\n",
       "      <td>8.521351</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>98.735135</td>\n",
       "      <td>49.356757</td>\n",
       "      <td>0.862931</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.583160</td>\n",
       "      <td>0.595233</td>\n",
       "      <td>12.603253</td>\n",
       "      <td>0.155676</td>\n",
       "      <td>14743108.800000</td>\n",
       "      <td>0.408438</td>\n",
       "      <td>226.760560</td>\n",
       "      <td>0.382748</td>\n",
       "      <td>roberta-base</td>\n",
       "      <td>roberta-base</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>32</td>\n",
       "      <td>64</td>\n",
       "      <td>353872.486400</td>\n",
       "      <td>15404.679688</td>\n",
       "      <td>8.489065</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>98.399925</td>\n",
       "      <td>45.317044</td>\n",
       "      <td>0.688402</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.238113</td>\n",
       "      <td>0.234880</td>\n",
       "      <td>8.728349</td>\n",
       "      <td>0.175672</td>\n",
       "      <td>9775531.600000</td>\n",
       "      <td>0.326428</td>\n",
       "      <td>152.564900</td>\n",
       "      <td>0.391980</td>\n",
       "      <td>roberta-base</td>\n",
       "      <td>roberta-base</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>24</td>\n",
       "      <td>160</td>\n",
       "      <td>677429.468160</td>\n",
       "      <td>19039.989258</td>\n",
       "      <td>8.953178</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>98.872829</td>\n",
       "      <td>51.418575</td>\n",
       "      <td>2.096752</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.329929</td>\n",
       "      <td>0.336000</td>\n",
       "      <td>16.199111</td>\n",
       "      <td>0.037344</td>\n",
       "      <td>19134636.200000</td>\n",
       "      <td>0.315819</td>\n",
       "      <td>292.682840</td>\n",
       "      <td>0.415903</td>\n",
       "      <td>roberta-base</td>\n",
       "      <td>roberta-base</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>16</td>\n",
       "      <td>160</td>\n",
       "      <td>451619.645440</td>\n",
       "      <td>12694.076172</td>\n",
       "      <td>8.528854</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>98.723462</td>\n",
       "      <td>50.347619</td>\n",
       "      <td>0.293697</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.205944</td>\n",
       "      <td>0.210217</td>\n",
       "      <td>10.883185</td>\n",
       "      <td>0.126905</td>\n",
       "      <td>12685587.400000</td>\n",
       "      <td>0.752431</td>\n",
       "      <td>195.803400</td>\n",
       "      <td>0.428557</td>\n",
       "      <td>roberta-base</td>\n",
       "      <td>roberta-base</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>24</td>\n",
       "      <td>96</td>\n",
       "      <td>400884.581376</td>\n",
       "      <td>13806.377930</td>\n",
       "      <td>8.531730</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>98.847671</td>\n",
       "      <td>48.347126</td>\n",
       "      <td>0.807534</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.481078</td>\n",
       "      <td>0.569371</td>\n",
       "      <td>9.786200</td>\n",
       "      <td>0.114821</td>\n",
       "      <td>11103926.800000</td>\n",
       "      <td>0.745454</td>\n",
       "      <td>172.488520</td>\n",
       "      <td>0.439903</td>\n",
       "      <td>roberta-base</td>\n",
       "      <td>roberta-base</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>256</td>\n",
       "      <td>368735.977472</td>\n",
       "      <td>9572.468750</td>\n",
       "      <td>8.477931</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>98.317241</td>\n",
       "      <td>49.579310</td>\n",
       "      <td>0.807900</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.390808</td>\n",
       "      <td>0.649322</td>\n",
       "      <td>9.814744</td>\n",
       "      <td>0.192458</td>\n",
       "      <td>10785826.200000</td>\n",
       "      <td>0.358000</td>\n",
       "      <td>167.766880</td>\n",
       "      <td>0.442895</td>\n",
       "      <td>roberta-base</td>\n",
       "      <td>roberta-base</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>16</td>\n",
       "      <td>128</td>\n",
       "      <td>358817.521664</td>\n",
       "      <td>10868.539062</td>\n",
       "      <td>8.465318</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>97.956813</td>\n",
       "      <td>47.424109</td>\n",
       "      <td>0.913876</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.949569</td>\n",
       "      <td>1.180707</td>\n",
       "      <td>9.011779</td>\n",
       "      <td>0.167357</td>\n",
       "      <td>10004725.000000</td>\n",
       "      <td>0.394581</td>\n",
       "      <td>155.442640</td>\n",
       "      <td>0.541899</td>\n",
       "      <td>roberta-base</td>\n",
       "      <td>roberta-base</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>16</td>\n",
       "      <td>192</td>\n",
       "      <td>545662.758912</td>\n",
       "      <td>14681.613281</td>\n",
       "      <td>8.471753</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>99.007335</td>\n",
       "      <td>51.516521</td>\n",
       "      <td>0.388394</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.503457</td>\n",
       "      <td>0.512649</td>\n",
       "      <td>13.381442</td>\n",
       "      <td>0.030944</td>\n",
       "      <td>15427470.000000</td>\n",
       "      <td>0.574570</td>\n",
       "      <td>237.896060</td>\n",
       "      <td>0.600342</td>\n",
       "      <td>roberta-base</td>\n",
       "      <td>roberta-base</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>8</td>\n",
       "      <td>224</td>\n",
       "      <td>320473.431040</td>\n",
       "      <td>8416.700195</td>\n",
       "      <td>8.443741</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>97.857908</td>\n",
       "      <td>50.395748</td>\n",
       "      <td>1.261989</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.697187</td>\n",
       "      <td>1.226708</td>\n",
       "      <td>8.168771</td>\n",
       "      <td>0.134643</td>\n",
       "      <td>9193970.600000</td>\n",
       "      <td>0.247904</td>\n",
       "      <td>142.402820</td>\n",
       "      <td>0.758372</td>\n",
       "      <td>roberta-base</td>\n",
       "      <td>roberta-base</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>24</td>\n",
       "      <td>224</td>\n",
       "      <td>961420.293120</td>\n",
       "      <td>25245.600586</td>\n",
       "      <td>8.578429</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>99.334286</td>\n",
       "      <td>53.127143</td>\n",
       "      <td>0.382745</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.361841</td>\n",
       "      <td>0.396693</td>\n",
       "      <td>23.829337</td>\n",
       "      <td>0.027209</td>\n",
       "      <td>27731352.000000</td>\n",
       "      <td>0.274491</td>\n",
       "      <td>425.862240</td>\n",
       "      <td>0.769007</td>\n",
       "      <td>roberta-base</td>\n",
       "      <td>roberta-base</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>256</td>\n",
       "      <td>1106207.932416</td>\n",
       "      <td>28712.906250</td>\n",
       "      <td>8.628148</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>99.462963</td>\n",
       "      <td>54.543210</td>\n",
       "      <td>0.726880</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.243516</td>\n",
       "      <td>0.221080</td>\n",
       "      <td>27.592415</td>\n",
       "      <td>0.027985</td>\n",
       "      <td>32022689.600000</td>\n",
       "      <td>0.121626</td>\n",
       "      <td>490.985700</td>\n",
       "      <td>0.817151</td>\n",
       "      <td>roberta-base</td>\n",
       "      <td>roberta-base</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>24</td>\n",
       "      <td>64</td>\n",
       "      <td>265404.364800</td>\n",
       "      <td>11554.072266</td>\n",
       "      <td>8.455385</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>97.328205</td>\n",
       "      <td>44.769231</td>\n",
       "      <td>1.242159</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.323490</td>\n",
       "      <td>1.320033</td>\n",
       "      <td>6.612784</td>\n",
       "      <td>0.135322</td>\n",
       "      <td>7323652.800000</td>\n",
       "      <td>0.656237</td>\n",
       "      <td>114.307480</td>\n",
       "      <td>0.885676</td>\n",
       "      <td>roberta-base</td>\n",
       "      <td>roberta-base</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>160</td>\n",
       "      <td>225809.822720</td>\n",
       "      <td>6348.163086</td>\n",
       "      <td>8.458750</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>97.837500</td>\n",
       "      <td>49.200000</td>\n",
       "      <td>1.084770</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.810917</td>\n",
       "      <td>0.858386</td>\n",
       "      <td>5.494987</td>\n",
       "      <td>0.223691</td>\n",
       "      <td>6249514.800000</td>\n",
       "      <td>0.563747</td>\n",
       "      <td>96.556540</td>\n",
       "      <td>1.008864</td>\n",
       "      <td>roberta-base</td>\n",
       "      <td>roberta-base</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>24</td>\n",
       "      <td>128</td>\n",
       "      <td>538226.282496</td>\n",
       "      <td>16301.683594</td>\n",
       "      <td>8.497442</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>98.781684</td>\n",
       "      <td>50.377649</td>\n",
       "      <td>0.815810</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.769154</td>\n",
       "      <td>0.775152</td>\n",
       "      <td>12.803643</td>\n",
       "      <td>0.110809</td>\n",
       "      <td>14917603.000000</td>\n",
       "      <td>0.480207</td>\n",
       "      <td>227.986660</td>\n",
       "      <td>1.032331</td>\n",
       "      <td>roberta-base</td>\n",
       "      <td>roberta-base</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8</td>\n",
       "      <td>192</td>\n",
       "      <td>272831.379456</td>\n",
       "      <td>7341.931641</td>\n",
       "      <td>8.482625</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>97.037979</td>\n",
       "      <td>48.057724</td>\n",
       "      <td>0.871524</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.985959</td>\n",
       "      <td>0.999646</td>\n",
       "      <td>7.094035</td>\n",
       "      <td>0.093773</td>\n",
       "      <td>7714217.000000</td>\n",
       "      <td>1.059562</td>\n",
       "      <td>122.433600</td>\n",
       "      <td>1.169741</td>\n",
       "      <td>roberta-base</td>\n",
       "      <td>roberta-base</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>16</td>\n",
       "      <td>64</td>\n",
       "      <td>176936.243200</td>\n",
       "      <td>7703.464844</td>\n",
       "      <td>8.425185</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>96.748148</td>\n",
       "      <td>43.562963</td>\n",
       "      <td>0.731057</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.766174</td>\n",
       "      <td>0.745850</td>\n",
       "      <td>4.621815</td>\n",
       "      <td>0.200636</td>\n",
       "      <td>4837074.200000</td>\n",
       "      <td>0.735027</td>\n",
       "      <td>76.482680</td>\n",
       "      <td>1.171999</td>\n",
       "      <td>roberta-base</td>\n",
       "      <td>roberta-base</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>16</td>\n",
       "      <td>32</td>\n",
       "      <td>87857.088512</td>\n",
       "      <td>6363.927734</td>\n",
       "      <td>8.295810</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>91.516190</td>\n",
       "      <td>40.186667</td>\n",
       "      <td>2.626594</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.886067</td>\n",
       "      <td>1.891442</td>\n",
       "      <td>2.478176</td>\n",
       "      <td>0.138510</td>\n",
       "      <td>2709297.000000</td>\n",
       "      <td>2.969825</td>\n",
       "      <td>41.310240</td>\n",
       "      <td>1.253009</td>\n",
       "      <td>roberta-base</td>\n",
       "      <td>roberta-base</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>175714.177024</td>\n",
       "      <td>12725.605469</td>\n",
       "      <td>8.426693</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>96.801323</td>\n",
       "      <td>42.153968</td>\n",
       "      <td>1.033441</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.070447</td>\n",
       "      <td>1.415433</td>\n",
       "      <td>4.627549</td>\n",
       "      <td>0.293691</td>\n",
       "      <td>4899640.600000</td>\n",
       "      <td>1.170984</td>\n",
       "      <td>76.800280</td>\n",
       "      <td>1.452981</td>\n",
       "      <td>roberta-base</td>\n",
       "      <td>roberta-base</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>24</td>\n",
       "      <td>32</td>\n",
       "      <td>131785.632768</td>\n",
       "      <td>9544.766602</td>\n",
       "      <td>8.406000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>94.305714</td>\n",
       "      <td>46.182381</td>\n",
       "      <td>0.862654</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.309267</td>\n",
       "      <td>1.312855</td>\n",
       "      <td>3.551594</td>\n",
       "      <td>0.091201</td>\n",
       "      <td>3973063.200000</td>\n",
       "      <td>2.465094</td>\n",
       "      <td>61.958260</td>\n",
       "      <td>1.556611</td>\n",
       "      <td>roberta-base</td>\n",
       "      <td>roberta-base</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>128</td>\n",
       "      <td>179408.760832</td>\n",
       "      <td>5435.394531</td>\n",
       "      <td>8.490961</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>97.595320</td>\n",
       "      <td>45.729803</td>\n",
       "      <td>1.068542</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.127699</td>\n",
       "      <td>2.175239</td>\n",
       "      <td>4.812476</td>\n",
       "      <td>0.196022</td>\n",
       "      <td>5041446.600000</td>\n",
       "      <td>1.363442</td>\n",
       "      <td>79.976120</td>\n",
       "      <td>1.750464</td>\n",
       "      <td>roberta-base</td>\n",
       "      <td>roberta-base</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>64</td>\n",
       "      <td>88468.121600</td>\n",
       "      <td>3852.857422</td>\n",
       "      <td>8.413048</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>91.684762</td>\n",
       "      <td>41.307619</td>\n",
       "      <td>2.166430</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.372498</td>\n",
       "      <td>3.606372</td>\n",
       "      <td>2.489375</td>\n",
       "      <td>0.181095</td>\n",
       "      <td>2658856.600000</td>\n",
       "      <td>2.333770</td>\n",
       "      <td>41.514640</td>\n",
       "      <td>1.786423</td>\n",
       "      <td>roberta-base</td>\n",
       "      <td>roberta-base</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>96</td>\n",
       "      <td>133628.193792</td>\n",
       "      <td>4603.625977</td>\n",
       "      <td>8.405628</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>94.444589</td>\n",
       "      <td>48.208225</td>\n",
       "      <td>1.653915</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.328765</td>\n",
       "      <td>0.323403</td>\n",
       "      <td>3.669163</td>\n",
       "      <td>0.274743</td>\n",
       "      <td>4079224.600000</td>\n",
       "      <td>1.595311</td>\n",
       "      <td>62.940540</td>\n",
       "      <td>1.900760</td>\n",
       "      <td>roberta-base</td>\n",
       "      <td>roberta-base</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>1</td>\n",
       "      <td>160</td>\n",
       "      <td>28226.227840</td>\n",
       "      <td>795.489136</td>\n",
       "      <td>8.102500</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>74.460714</td>\n",
       "      <td>23.146429</td>\n",
       "      <td>4.076322</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.873199</td>\n",
       "      <td>2.331062</td>\n",
       "      <td>1.336745</td>\n",
       "      <td>3.452474</td>\n",
       "      <td>1115465.600000</td>\n",
       "      <td>2.535629</td>\n",
       "      <td>19.212880</td>\n",
       "      <td>2.548215</td>\n",
       "      <td>roberta-base</td>\n",
       "      <td>roberta-base</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>1</td>\n",
       "      <td>224</td>\n",
       "      <td>40059.178880</td>\n",
       "      <td>1054.056274</td>\n",
       "      <td>8.142222</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>81.911111</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>3.923302</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.006437</td>\n",
       "      <td>5.078263</td>\n",
       "      <td>1.537060</td>\n",
       "      <td>1.667605</td>\n",
       "      <td>1414276.800000</td>\n",
       "      <td>4.838443</td>\n",
       "      <td>23.153880</td>\n",
       "      <td>2.600703</td>\n",
       "      <td>roberta-base</td>\n",
       "      <td>roberta-base</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>1</td>\n",
       "      <td>192</td>\n",
       "      <td>34103.922432</td>\n",
       "      <td>919.710205</td>\n",
       "      <td>8.166667</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>80.805556</td>\n",
       "      <td>27.847222</td>\n",
       "      <td>2.932142</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.557218</td>\n",
       "      <td>3.523612</td>\n",
       "      <td>1.387758</td>\n",
       "      <td>1.972328</td>\n",
       "      <td>1212794.800000</td>\n",
       "      <td>2.414529</td>\n",
       "      <td>19.823240</td>\n",
       "      <td>3.857791</td>\n",
       "      <td>roberta-base</td>\n",
       "      <td>roberta-base</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>1</td>\n",
       "      <td>256</td>\n",
       "      <td>46091.997184</td>\n",
       "      <td>1198.527344</td>\n",
       "      <td>8.241556</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>86.902222</td>\n",
       "      <td>34.306667</td>\n",
       "      <td>1.780571</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.913483</td>\n",
       "      <td>3.592404</td>\n",
       "      <td>1.610236</td>\n",
       "      <td>1.241279</td>\n",
       "      <td>1479737.600000</td>\n",
       "      <td>4.885859</td>\n",
       "      <td>24.172660</td>\n",
       "      <td>4.615878</td>\n",
       "      <td>roberta-base</td>\n",
       "      <td>roberta-base</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>32</td>\n",
       "      <td>43928.544256</td>\n",
       "      <td>3183.088867</td>\n",
       "      <td>8.178333</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>80.508333</td>\n",
       "      <td>26.016667</td>\n",
       "      <td>5.290378</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.553197</td>\n",
       "      <td>5.564958</td>\n",
       "      <td>1.485244</td>\n",
       "      <td>1.827383</td>\n",
       "      <td>1412678.200000</td>\n",
       "      <td>5.046847</td>\n",
       "      <td>22.589040</td>\n",
       "      <td>5.635955</td>\n",
       "      <td>roberta-base</td>\n",
       "      <td>roberta-base</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>1</td>\n",
       "      <td>96</td>\n",
       "      <td>16703.524224</td>\n",
       "      <td>577.421997</td>\n",
       "      <td>8.085714</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>56.033333</td>\n",
       "      <td>21.261905</td>\n",
       "      <td>5.539933</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12.282902</td>\n",
       "      <td>12.064998</td>\n",
       "      <td>1.190791</td>\n",
       "      <td>7.796169</td>\n",
       "      <td>766917.200000</td>\n",
       "      <td>2.770989</td>\n",
       "      <td>14.214180</td>\n",
       "      <td>5.742394</td>\n",
       "      <td>roberta-base</td>\n",
       "      <td>roberta-base</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>1</td>\n",
       "      <td>128</td>\n",
       "      <td>22426.095104</td>\n",
       "      <td>681.393066</td>\n",
       "      <td>8.366429</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>68.692857</td>\n",
       "      <td>26.832143</td>\n",
       "      <td>1.682477</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.480237</td>\n",
       "      <td>12.028796</td>\n",
       "      <td>1.228377</td>\n",
       "      <td>7.866152</td>\n",
       "      <td>928006.200000</td>\n",
       "      <td>6.421930</td>\n",
       "      <td>16.154420</td>\n",
       "      <td>6.456240</td>\n",
       "      <td>roberta-base</td>\n",
       "      <td>roberta-base</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>11058.515200</td>\n",
       "      <td>483.575928</td>\n",
       "      <td>7.958333</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>46.047619</td>\n",
       "      <td>16.213095</td>\n",
       "      <td>5.197769</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.538809</td>\n",
       "      <td>6.708874</td>\n",
       "      <td>1.217900</td>\n",
       "      <td>7.664883</td>\n",
       "      <td>680242.800000</td>\n",
       "      <td>8.453346</td>\n",
       "      <td>13.676660</td>\n",
       "      <td>8.045772</td>\n",
       "      <td>roberta-base</td>\n",
       "      <td>roberta-base</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>5491.068032</td>\n",
       "      <td>399.854858</td>\n",
       "      <td>8.167262</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>27.988095</td>\n",
       "      <td>13.826190</td>\n",
       "      <td>3.532622</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.217780</td>\n",
       "      <td>6.421135</td>\n",
       "      <td>1.250377</td>\n",
       "      <td>7.549703</td>\n",
       "      <td>398799.200000</td>\n",
       "      <td>8.344308</td>\n",
       "      <td>11.181740</td>\n",
       "      <td>8.432107</td>\n",
       "      <td>roberta-base</td>\n",
       "      <td>roberta-base</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    batch_size  seq_len          flops    mem_bytes      cpu       mem  \\\n",
       "31          32      256 1474943.909888 38283.125000 8.508525 13.000000   \n",
       "14          16      224  640946.862080 16831.150391 8.491075 13.000000   \n",
       "30          32      224 1281893.724160 33660.050781 9.029523 13.000000   \n",
       "27          32      128  717635.043328 21734.828125 8.517996 13.000000   \n",
       "29          32      192 1091325.517824 29360.976562 9.179919 12.000000   \n",
       "28          32      160  903239.290880 25385.902344 8.965966 13.000000   \n",
       "15          16      256  737471.954944 19142.687500 8.511865 13.000000   \n",
       "21          24      192  818494.138368 22021.294922 8.505210 13.000000   \n",
       "10          16       96  267256.387584  9205.001953 8.521000 13.000000   \n",
       "26          32       96  534512.775168 18407.753906 8.521351 12.000000   \n",
       "25          32       64  353872.486400 15404.679688 8.489065 13.000000   \n",
       "20          24      160  677429.468160 19039.989258 8.953178 13.000000   \n",
       "12          16      160  451619.645440 12694.076172 8.528854 12.000000   \n",
       "18          24       96  400884.581376 13806.377930 8.531730 13.000000   \n",
       "7            8      256  368735.977472  9572.468750 8.477931 13.000000   \n",
       "11          16      128  358817.521664 10868.539062 8.465318 13.000000   \n",
       "13          16      192  545662.758912 14681.613281 8.471753 13.000000   \n",
       "6            8      224  320473.431040  8416.700195 8.443741 13.000000   \n",
       "22          24      224  961420.293120 25245.600586 8.578429 13.000000   \n",
       "23          24      256 1106207.932416 28712.906250 8.628148 13.000000   \n",
       "17          24       64  265404.364800 11554.072266 8.455385 13.000000   \n",
       "4            8      160  225809.822720  6348.163086 8.458750 13.000000   \n",
       "19          24      128  538226.282496 16301.683594 8.497442 13.000000   \n",
       "5            8      192  272831.379456  7341.931641 8.482625 13.000000   \n",
       "9           16       64  176936.243200  7703.464844 8.425185 12.000000   \n",
       "8           16       32   87857.088512  6363.927734 8.295810 13.000000   \n",
       "24          32       32  175714.177024 12725.605469 8.426693 13.000000   \n",
       "16          24       32  131785.632768  9544.766602 8.406000 13.000000   \n",
       "3            8      128  179408.760832  5435.394531 8.490961 13.000000   \n",
       "1            8       64   88468.121600  3852.857422 8.413048 13.000000   \n",
       "2            8       96  133628.193792  4603.625977 8.405628 13.000000   \n",
       "36           1      160   28226.227840   795.489136 8.102500 13.000000   \n",
       "38           1      224   40059.178880  1054.056274 8.142222 13.000000   \n",
       "37           1      192   34103.922432   919.710205 8.166667 13.000000   \n",
       "39           1      256   46091.997184  1198.527344 8.241556 13.000000   \n",
       "0            8       32   43928.544256  3183.088867 8.178333 13.000000   \n",
       "34           1       96   16703.524224   577.421997 8.085714 13.000000   \n",
       "35           1      128   22426.095104   681.393066 8.366429 13.000000   \n",
       "33           1       64   11058.515200   483.575928 7.958333 13.000000   \n",
       "32           1       32    5491.068032   399.854858 8.167262 13.000000   \n",
       "\n",
       "         gpu   gpu_mem  cpu_std  mem_std   gpu_std  gpu_mem_std  times_mean  \\\n",
       "31 99.541730 55.186016 0.348381 0.000000  0.168512     0.257035   36.452941   \n",
       "14 99.160896 52.015162 0.468395 0.000000  0.296474     0.388250   16.102221   \n",
       "30 99.527074 52.851741 1.815107 0.000000  0.279158     0.351909   31.849768   \n",
       "27 99.205743 50.748020 0.346976 0.000000  0.220200     0.265863   17.132208   \n",
       "29 99.297950 52.945688 2.702689 0.000000  0.436990     0.448791   26.582488   \n",
       "28 99.338792 51.254306 1.464205 0.000000  0.289965     0.406308   21.762289   \n",
       "15 99.085371 53.058215 0.267594 0.000000  0.381918     0.403914   18.593617   \n",
       "21 99.267227 51.275630 0.252548 0.000000  0.310596     0.301194   20.275331   \n",
       "10 98.691667 46.388718 1.069904 0.000000  0.625956     0.619118    6.719930   \n",
       "26 98.735135 49.356757 0.862931 0.000000  0.583160     0.595233   12.603253   \n",
       "25 98.399925 45.317044 0.688402 0.000000  0.238113     0.234880    8.728349   \n",
       "20 98.872829 51.418575 2.096752 0.000000  0.329929     0.336000   16.199111   \n",
       "12 98.723462 50.347619 0.293697 0.000000  0.205944     0.210217   10.883185   \n",
       "18 98.847671 48.347126 0.807534 0.000000  0.481078     0.569371    9.786200   \n",
       "7  98.317241 49.579310 0.807900 0.000000  0.390808     0.649322    9.814744   \n",
       "11 97.956813 47.424109 0.913876 0.000000  0.949569     1.180707    9.011779   \n",
       "13 99.007335 51.516521 0.388394 0.000000  0.503457     0.512649   13.381442   \n",
       "6  97.857908 50.395748 1.261989 0.000000  0.697187     1.226708    8.168771   \n",
       "22 99.334286 53.127143 0.382745 0.000000  0.361841     0.396693   23.829337   \n",
       "23 99.462963 54.543210 0.726880 0.000000  0.243516     0.221080   27.592415   \n",
       "17 97.328205 44.769231 1.242159 0.000000  1.323490     1.320033    6.612784   \n",
       "4  97.837500 49.200000 1.084770 0.000000  0.810917     0.858386    5.494987   \n",
       "19 98.781684 50.377649 0.815810 0.000000  0.769154     0.775152   12.803643   \n",
       "5  97.037979 48.057724 0.871524 0.000000  0.985959     0.999646    7.094035   \n",
       "9  96.748148 43.562963 0.731057 0.000000  0.766174     0.745850    4.621815   \n",
       "8  91.516190 40.186667 2.626594 0.000000  1.886067     1.891442    2.478176   \n",
       "24 96.801323 42.153968 1.033441 0.000000  1.070447     1.415433    4.627549   \n",
       "16 94.305714 46.182381 0.862654 0.000000  1.309267     1.312855    3.551594   \n",
       "3  97.595320 45.729803 1.068542 0.000000  2.127699     2.175239    4.812476   \n",
       "1  91.684762 41.307619 2.166430 0.000000  3.372498     3.606372    2.489375   \n",
       "2  94.444589 48.208225 1.653915 0.000000  0.328765     0.323403    3.669163   \n",
       "36 74.460714 23.146429 4.076322 0.000000  1.873199     2.331062    1.336745   \n",
       "38 81.911111 30.000000 3.923302 0.000000  5.006437     5.078263    1.537060   \n",
       "37 80.805556 27.847222 2.932142 0.000000  3.557218     3.523612    1.387758   \n",
       "39 86.902222 34.306667 1.780571 0.000000  3.913483     3.592404    1.610236   \n",
       "0  80.508333 26.016667 5.290378 0.000000  5.553197     5.564958    1.485244   \n",
       "34 56.033333 21.261905 5.539933 0.000000 12.282902    12.064998    1.190791   \n",
       "35 68.692857 26.832143 1.682477 0.000000 11.480237    12.028796    1.228377   \n",
       "33 46.047619 16.213095 5.197769 0.000000  6.538809     6.708874    1.217900   \n",
       "32 27.988095 13.826190 3.532622 0.000000  7.217780     6.421135    1.250377   \n",
       "\n",
       "    times_std  gpu_power_mean  gpu_power_std  energy_mean  energy_std  \\\n",
       "31   0.074229 42826452.600000       0.136187   659.394280    0.176440   \n",
       "14   0.021939 18519307.400000       0.488594   286.260440    0.239087   \n",
       "30   0.063903 37259910.200000       0.296302   574.288680    0.249235   \n",
       "27   0.144713 19914179.000000       0.490498   307.390720    0.287794   \n",
       "29   0.073688 31182663.800000       0.225302   481.171700    0.302256   \n",
       "28   0.147602 25667877.400000       0.288475   394.860140    0.315285   \n",
       "15   0.040337 21241280.400000       0.196262   330.010140    0.322031   \n",
       "21   0.038197 23352421.200000       0.179585   360.348560    0.356916   \n",
       "10   0.162062  7381716.200000       0.804158   115.028300    0.382395   \n",
       "26   0.155676 14743108.800000       0.408438   226.760560    0.382748   \n",
       "25   0.175672  9775531.600000       0.326428   152.564900    0.391980   \n",
       "20   0.037344 19134636.200000       0.315819   292.682840    0.415903   \n",
       "12   0.126905 12685587.400000       0.752431   195.803400    0.428557   \n",
       "18   0.114821 11103926.800000       0.745454   172.488520    0.439903   \n",
       "7    0.192458 10785826.200000       0.358000   167.766880    0.442895   \n",
       "11   0.167357 10004725.000000       0.394581   155.442640    0.541899   \n",
       "13   0.030944 15427470.000000       0.574570   237.896060    0.600342   \n",
       "6    0.134643  9193970.600000       0.247904   142.402820    0.758372   \n",
       "22   0.027209 27731352.000000       0.274491   425.862240    0.769007   \n",
       "23   0.027985 32022689.600000       0.121626   490.985700    0.817151   \n",
       "17   0.135322  7323652.800000       0.656237   114.307480    0.885676   \n",
       "4    0.223691  6249514.800000       0.563747    96.556540    1.008864   \n",
       "19   0.110809 14917603.000000       0.480207   227.986660    1.032331   \n",
       "5    0.093773  7714217.000000       1.059562   122.433600    1.169741   \n",
       "9    0.200636  4837074.200000       0.735027    76.482680    1.171999   \n",
       "8    0.138510  2709297.000000       2.969825    41.310240    1.253009   \n",
       "24   0.293691  4899640.600000       1.170984    76.800280    1.452981   \n",
       "16   0.091201  3973063.200000       2.465094    61.958260    1.556611   \n",
       "3    0.196022  5041446.600000       1.363442    79.976120    1.750464   \n",
       "1    0.181095  2658856.600000       2.333770    41.514640    1.786423   \n",
       "2    0.274743  4079224.600000       1.595311    62.940540    1.900760   \n",
       "36   3.452474  1115465.600000       2.535629    19.212880    2.548215   \n",
       "38   1.667605  1414276.800000       4.838443    23.153880    2.600703   \n",
       "37   1.972328  1212794.800000       2.414529    19.823240    3.857791   \n",
       "39   1.241279  1479737.600000       4.885859    24.172660    4.615878   \n",
       "0    1.827383  1412678.200000       5.046847    22.589040    5.635955   \n",
       "34   7.796169   766917.200000       2.770989    14.214180    5.742394   \n",
       "35   7.866152   928006.200000       6.421930    16.154420    6.456240   \n",
       "33   7.664883   680242.800000       8.453346    13.676660    8.045772   \n",
       "32   7.549703   398799.200000       8.344308    11.181740    8.432107   \n",
       "\n",
       "      level_name    model_name  \n",
       "31  roberta-base  roberta-base  \n",
       "14  roberta-base  roberta-base  \n",
       "30  roberta-base  roberta-base  \n",
       "27  roberta-base  roberta-base  \n",
       "29  roberta-base  roberta-base  \n",
       "28  roberta-base  roberta-base  \n",
       "15  roberta-base  roberta-base  \n",
       "21  roberta-base  roberta-base  \n",
       "10  roberta-base  roberta-base  \n",
       "26  roberta-base  roberta-base  \n",
       "25  roberta-base  roberta-base  \n",
       "20  roberta-base  roberta-base  \n",
       "12  roberta-base  roberta-base  \n",
       "18  roberta-base  roberta-base  \n",
       "7   roberta-base  roberta-base  \n",
       "11  roberta-base  roberta-base  \n",
       "13  roberta-base  roberta-base  \n",
       "6   roberta-base  roberta-base  \n",
       "22  roberta-base  roberta-base  \n",
       "23  roberta-base  roberta-base  \n",
       "17  roberta-base  roberta-base  \n",
       "4   roberta-base  roberta-base  \n",
       "19  roberta-base  roberta-base  \n",
       "5   roberta-base  roberta-base  \n",
       "9   roberta-base  roberta-base  \n",
       "8   roberta-base  roberta-base  \n",
       "24  roberta-base  roberta-base  \n",
       "16  roberta-base  roberta-base  \n",
       "3   roberta-base  roberta-base  \n",
       "1   roberta-base  roberta-base  \n",
       "2   roberta-base  roberta-base  \n",
       "36  roberta-base  roberta-base  \n",
       "38  roberta-base  roberta-base  \n",
       "37  roberta-base  roberta-base  \n",
       "39  roberta-base  roberta-base  \n",
       "0   roberta-base  roberta-base  \n",
       "34  roberta-base  roberta-base  \n",
       "35  roberta-base  roberta-base  \n",
       "33  roberta-base  roberta-base  \n",
       "32  roberta-base  roberta-base  "
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infos[3].dropna().sort_values('energy_std')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
